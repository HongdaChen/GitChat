Elasticsearch 的一个分片对应 Lucene 的一个索引，Elasticsearch 的核心就是将这些 Lucene
索引分布式化，提供索引和检索服务。可见，如何设计分片是至关重要的。

一个索引到底该设置几个主分片呢？由于单个分片只能处于 Elasticsearch
集群中的单个节点，分片太少，影响索引入库的并发度，以及以后的横向扩展性，如果分片过大会引发查询、更新、迁移、恢复、平衡等性能问题。

### 1 主分片数量确定

我们建议综合考虑分片物理大小因素、查询压力因素、索引压力因素，来设计分片数量。

#### 1.1 物理大小因素

建议单个分片的物理大小不大于 50GB，之所以这样建议，基于如下几个因素：

  * **更快的恢复速度**

集群故障后，更小的分片相对大分片来讲，更容易使集群恢复到 Green 状态。

  * **merge 过程中需要的资源更少**

Lucene 的 segment merge 过程需要两倍的磁盘空间，如果分片过大，势必需要更大的临时磁盘空间用于 merge，同时，分片过大 merge
过程持续时间更长，将对 IO 产生持续的压力。

  * **集群分片分布更容易均衡**

分片过大，Elasticsearch 内部的平衡机制需要更多的时间。

  * **提高 update 操作的性能**

对 Elasticsearch 索引进行 update 操作，底层 Lucene 采用的是先查找，再删除，最后 index
的过程。如果在分片比较大的索引上有比较多的 update 操作，将会对性能产生很大的影响。

  * **影响缓存**

节点的物理内存是有限的，如果分片过大，节点不能缓存分片必要的数据，对一些数据的访问将从物理磁盘加载，可想而知，对性能会产生多大的影响。

#### 1.2 查询压力因素

单个 shard 位于一个节点，如果索引只有一个 shard，则只有一个节点执行查询操作。如果有多个 shard
分部在不同的节点，多个节点可以并行执行，最后归并。

但是过多的分片会增加归并的执行时间，所以考虑这个因素，需要根据业务的数据特点，以贴近真实业务的查询去测试，不断加大分片数量，直到查询性能开始降低。

  * **索引压力因素**

单个 shard 只能位于一块单个节点上，索引过程是 CPU
密集型操作，单个节点的入库性能是有限的，所以需要把入库的压力分散到多个节点来满足写入性能。单纯考虑索引性能，可以根据单个节点的索引性能和需要索引的总性能来估算分片数量。

### 2 副本数量

副本是主分片的拷贝，可以响应查询请求、防止数据丢失、提高集群可用性等，但是副本不是“免费”的，需要占用与主分片一样的资源，包括
CPU、内存、磁盘，副本数量的确定等涉及多方面的因素。

#### 2.1 数据可靠性

明确自己的业务需要多高的可靠性和可用性。依据 Elasticsearch 的内部分片分布规则，同一索引相同编号的分片不会处于同一个
node，多一份副本就多一份数据安全性保障。

#### 2.2 索引性能

副本和主分片在索引过程中执行和主分片一样的操作，如果副本过多，有多少副本就会有几倍的 CPU
资源消耗在索引上，会拖累整个集群的索引吞吐量，对于索引密集型的业务场景影响巨大。所以要在数据安全型和索引性能上做权衡处理来确定副本的数量。

#### 2.3 查询性能

副本可以减轻对主分片的查询压力，这里可能说查询次数更为合理。节点加载副本以提供查询服务和加载主分片消耗的内存资源是完全相同的，增加副本的数量势必增加每个
node 所管理的分片数，因此会消耗更多的内存资源，Elasticsearch 的高速运行严重依赖于操作系统的 Cache。

如果节点本身内存不充足，副本数量的增加会导致节点对内存的需求的增加，从而降低 Lucene 索引文件的缓存效率，使 OS
产生大量的换页，最终影响到查询性能。当然，在资源充足的情况下，扩大副本数是肯定可以提高集群整体的 QPS。

### 3 分片分布

ELasticsearch 提供的关于 shard 平衡的两个参数是 `cluster.routing.allocation.balance.shard`
和 `cluster.routing.allocation.balance.index`。

第一个参数的意思是让每个节点维护的分片总数尽量平衡，第二个参数的意思是让每个索引的的分片尽量平均的分散到不同的节点。

如果集群中有不同类型的索引，而且每个类型的索引的索引方式、物理大小不一致，很容易造成节点间磁盘占用不均衡、不同节点间堆内存占用差异大的问题，从而导致集群不稳定。

所以我们建议尽量保证不同索引的 shard 大小尽量相近，以获得实质意义上的均衡分片分布。

### 4 集群分片总数控制

由于 Elasticsearch 的集群管理方式还是中心化的，分片元信息的维护在选举出来的 master
节点上，分片过多会增加查询结果合并的时间，同时增加集群管理的负担。

根据我们的经验，单个集群的分片超过 10 万时，集群维护相关操作例如创建索引、删除索引等就会出现缓慢的情况。所以在实践中，尽量控制单个集群的分片总数在 10
万以内。

### 5 【案例分析】大分片数据更新引发的 IO 100% 异常

分片大小对某些业务类型来讲会产生致命的影响，这里介绍一个我们遇到的一个案例，由于分片不合理导致了很严重的性能问题。

#### 5.1 问题背景

有一个集群由很多业务部门公用，该集群为 10 个节点，单节点 128G 内存，CPU 为 40 线程，硬盘为 4T*12 的配置，存储使用总量在
20%。业务 A 反应他的 bulk 操作很慢，需要分钟级才能完成。

经过与业务沟通后，了解到他们单次 bulk 1000 条数据，单条数据大小为 1k，这种 bulk 并不大，因此速度慢肯定是不正常的现象。

#### 5.2 问题分析

有了以上的背景信息，开始问题排查。该索引大小为 1.2T，5 个主分片。登录到集群服务器后台，在业务运行过程中，通过 top 查看，CPU 利用率在
30%，在正常范围内。但是 iowait 一直持续在 20% 左右，问题初步原因应该在 IO 上。

随即通过 iostat 观察磁盘的状况，发现有一块盘的 IO 持续 100%。登录其他几个服务器，发现每个服务器都有一块盘的 IO 处于 100% 状态。

之后通过分析该索引的分布情况，发现 IO 利用率高的磁盘都有这个索引对应的 shard，难道是这个索引导致的？

因为这些磁盘上还有其他的索引，我们现在也只是推测。打开 iotop，发现有些线程的 io 持续有大量读取。

将线程 tid 转换成 16 进制，通过 jtack 查询对应的线程，发现是 Lucene 的 refresh
操作触发的。但是只通过线程堆栈扔无法确认是由该索引的 bulk 操作引起。之后通过跟踪系统调用：

    
    
    strace -t -T -y -p $tid
    

发现每秒有数百次的 pread 系统调用，而且读取的目录全部为该索引所在目录，使得磁盘 IO 一直处于满负荷状态。bulk 是写操作，不会引起大量的读。

有一种情况是，如果待索引的数据的 id 是应用程序自己生成的，底层 Lucene 在索引时，要去查找对应的文档是否存在。

跟业务人员沟通后，id 确实是由应用程序自己控制。这样问题就清晰了，在这个索引的单个分片上 bulk 200 条数据，底层 Lucene 要进行 200
次查找以确定对应的数据是否存在。

#### 5.3 问题解决

通过以上分析，在目前的情况下，紧急缩小单个分片的物理大小，增加该索引的 shard 数到 200，使数据均衡到更多的磁盘，对旧索引的数据进行
reindex。迁移完成后，同样的 bulk 操作在 1s 左右执行完成。

### 总结

没有不好的技术，只有不合理的使用。通过减少单个分片的物理大小，将数据分散到更多的 shard，从而将 IO 压力分散到了集群内的所有磁盘上，暂时可以解决目前
bulk 慢的问题。

但是考虑到 Lucene
的技术特点，并不适用于有大量更新的业务场景，还是需要重构业务，以追加的方式写入新数据，通过数据的版本或者时间戳来查询最新的数据，并定期对数据进行整理，以达到最优的性能。

* * *

### 交流与答疑

> **为了方便与作者交流与学习，GitChat 编辑团队组织了一个《高可用 Elasticsearch 集群 21 讲》读者交流群，添加小助手-
> 伽利略微信：「GitChatty6」，回复关键字「244」给小助手-伽利略获取入群资格。**
>
> 阅读文章过程中有任何疑问随时可以跟其他小伙伴讨论，或者直接向作者提问（作者看到后抽空回复）。你的分享不仅帮助他人，更会提升自己。
> ![R2Y8ju](https://images.gitbook.cn/R2Y8ju.jpg)

