上一篇，我们主要介绍了优化神经网络中的一些常用技巧，包括输入标准化、权重 W 初始化、批归一化（Batch
Normalization）、超参数调试等。这些技巧和方法在实际应用中非常有用，能够大大提高训练神经网络的效率。本文将介绍一些构建神经网络模型的实用建议。

### 如何评估模型

神经网络模型训练完成之后需要对其进行评估，正确地评估有助于了解模型的性能，能够帮助我们继续优化模型。如何评估模型呢？这里先介绍两个名词：精确率（Precision）和召回率（Recall）。此节内容只针对二分类问题。

精确率（Precision）表示被分为正例的示例中实际为正例的比例。

召回率（Recall）是覆盖面的度量，表示样本中的正例有多少被预测正确。

下面用图示来说明：

![enter image description
here](https://images.gitbook.cn/dd10a9f0-ad17-11e8-afe5-6ba901a27e1b)

其中，TP（True Positive）表示将正类预测为正类数；TN（True Negative）表示将负类预测为负类数；FP（False
Positive）表示将负类预测为正类数；FN（False Negative）表示将正类预测为负类数。

精确率的计算公式为：

$$P=\frac{TP}{TP+FP}$$

召回率的计算公式为：

$$R=\frac{TP}{TP+FN}$$

实际应用时，只看精确率或者召回率都不太科学。通常我们结合 P 和 R，使用单一指标 F1 分数来评估模型的性能。F1 分数的计算公式如下：

$$F1=\frac{2PR}{P+R}$$

例如已经知道了 A 和 B 模型的精确率 P 和召回率 R，就可以计算它们各自的 F1 分数。

![enter image description
here](https://images.gitbook.cn/a85e9db0-ad1d-11e8-8a14-814e96ebc1f4)

通过比较，A 模型的 F1 Score 比 B 模型的 F1 Score 大，所以我们判定 A 模型性能优于 B 模型。

### 训练&验证&测试集

在构建神经网络模型的时候，我们通常会把总的样本数据划分成训练、验证 、测试集。划分的比例一般可以根据总的样本数量决定。如果总的样本数量少于 1
万，通常将训练、验证、测试集的比例设为60%、20%、20%；如果总的样本数量比较大（百万级别），通常将相应的比例设为98%、1%、1%即可。

我们知道，验证集是用来进行模型选择的，选出最优的模型之后再在测试集上进行最后的测试。模型在测试集上的表现应该与验证集上表现相似，能够反映出了模型处理真实样本的能力。因此，我们在划分数据集的时候，务必要让验证集和测试集来自于同一分布。这样才能保证模型在验证集上的性能近似就表现出它在测试集乃至真实样本集中的性能。如果验证集和测试集不是来自同一分布，很可能我们通过交叉验证、超参数调试，根据在验证集上的结果，选择得到的“最优”模型，在测试集上却表现得很差。这就好比我们在验证集上找到最接近靶心的箭，但是测试集的靶心却远远偏离验证集靶心。这样得到的“Good
Model”可能并不好。

![enter image description
here](https://images.gitbook.cn/b7314ed0-ad24-11e8-afe5-6ba901a27e1b)

一般除了要让验证集和测试集来自同一分布之外，验证集最好能够检测不同算法或模型的区别，以便选择出更好的模型，验证集和测试集尽量与真实样本类似。

但是实际应用中往往可能出现这样的情况，就是训练集和验证集/测试集不是来自同一分布且数量上也有差异。以猫类图片识别为例，训练集可能来自于网络下载（Webpages），图片清晰度较高；验证集/测试集来自用户手机拍摄（Mobile），图片清晰度较低。假如训练集数量为20万，而验证集/测试集数量为1万。这时候应该如何准确划分数据集呢？

![enter image description
here](https://images.gitbook.cn/9fd63540-adbd-11e8-b483-e79b276a9d5b)

第一种方法是将训练集和验证集/测试集完全混合，然后随机选择一部分作为训练集，另一部分作为验证集/测试集。针对上面的例子，混合21万例样本，然后随机选择20.5万例样本作为训练集，2500例作为验证集，2500例作为测试集。这种做法使训练集和验证集/测试集分布一致，但缺点是验证集/测试集中网络图片所占的比重比手机拍摄的图片大得多。这样，验证集/测试集中算法模型对比验证，仍然主要由网络图片决定，实际应用的手机拍摄图片所占比重很小，达不到验证效果。因此，这种方法并不是很好。

第二种方法是将原来的训练集和一部分验证集/测试集组合当成训练集，剩下的验证集/测试集分别划分为验证集和测试集。例如，20万例网络图片和5000例手机拍摄图片组合成训练集，剩下的2500例手机拍摄图片作为验证集，2500例手机拍摄图片作为测试集。这样，验证集/测试集全部来自于手机拍摄，保证了验证集最接近实际应用场合。这种方法较为常用，而且性能表现比较好。

为了让训练集和验证集/测试集类似，我们可以使用人工数据合成的方法。例如说话人识别问题，实际应用场合往往包含背景噪声，而训练样本很可能没有背景噪声。为了让训练集和验证集/测试集分布一致，我们可以在训练集上人工添加背景噪声，合成类似实际场景的声音。这样会让模型训练的效果更准确。

### 动态调整算法评估标准

算法模型的评价标准有时候需要根据实际情况进行动态调整，目的是让算法模型在实际应用中有更好的效果。

比如说，在损失函数中，我们一般是对正类样本预测为负类和负类样本预测为正类这两种错误类型采用同样的惩罚度，从损失函数的表达式就能看得出来：

$$J=-\frac{1}{m}\sum_{i=1}^my^{(i)}log\hat y^{(i)}+(1-y^{(i)})log(1-\hat
y^{(i)})$$

但是实际应用中，可能这两类错误造成的影响不同，容忍度不同。例如安保单位的门禁系统，当然希望标准更加严格，绝不允许把非工作人员放进来。也就是说对正类样本预测为负类和负类样本预测为正类这两种错误类型采用的惩罚度是不一样的，后者的惩罚力度应该更大一些，相比之下更不希望误放入非工作人员。

基于这样的实际应用场合，我们就可以动态调整算法的评估标准，例如引入一个权重因子 $w$，根据这两类错误的不同影响程度，调整权重因子 $w$
的大小值。如上文提到的门禁系统，我们可以设置正类样本预测为负类的权重因子为 1，负类样本预测为正类的权重因子为 10，这样就能得到调整之后的损失函数：

$$J=-\frac{1}{m}\cdot w_i\sum_{i=1}^my^{(i)}log\hat
y^{(i)}+(1-y^{(i)})log(1-\hat y^{(i)})$$

![](https://images.gitbook.cn/1fe56710-b020-11e8-a327-c9f52d5b5a77)

上面的两个公式可以合并到一起：

$$J=-\frac{1}{m}\sum_{i=1}^m1\cdot y^{(i)}log\hat y^{(i)}+10\cdot
(1-y^{(i)})log(1-\hat y^{(i)})$$

以上是一种动态调整惩罚力度的方法，还有另外一种方法是动态改变预测正负的分界概率值。我们通常设定分界概率值为0.5，即当 $\hat
y\geqslant0.5$ 时，预测为正类，当 $\hat y<0.5$
时，预测为负类。还是门禁系统的例子，如果我们希望预测为正类的标准更加严格，那么可以把分界概率设置得高于 0.5，例如设置为 0.6、0.7
等。相反，如果我们希望预测为正类的标准更加宽松，那么可以把分界概率设置得低于 0.5，例如设置为 0.3、0.4等。

### 贝叶斯最优误差和人类表现水平

构建一个神经网络模型之后，我们通常使用误差（Error）来衡量模型的性能。模型的误差是有下界的，这个下界就是贝叶斯最优误差（Bayes Optimal
Error，BOE）。BOE 被称为理论上的最优误差，任何一个机器学习模型都无法超越贝叶斯最优，其 Error 都不会低于 BOE，只可能接近 BOE。

人类水平表现（Human-Level
Performance，HLP），顾名思义，就是人类在处理预测、分类等问题上的能力。例如人类通过观察，识别图片中的数字、识别语音，等等。神经网络模型乃至所有机器学习模型的表现通常会跟人类水平表现作比较，如下图所示：

![enter image description
here](https://images.gitbook.cn/39002d60-ad97-11e8-a2e3-f9c87df3ca9a)

上图中，横坐标是模型训练时间，纵坐标是模型准确度，代表模型性能。一般来说，随着训练次数增加，机器学习模型准确度会逐渐增加，不断接近
HLP，甚至超过它。之后，准确度会上升得比较缓慢，向 BOE 逼近，但不可能超过 BOE。

当然，上面这张图也不是绝对的，实际上，HLP
在某些方面表现得比机器学习模型更好。例如图像识别、语音识别等领域，人类相比机器来说还是更擅长一些。所以，我们的目标是让机器学习模型的性能能够不断接近
HLP。为了达到这一目标，我们做以下几个方向的努力：

  * 人类准确标注更多的样本数据给模型；
  * 从手动错误分析中获得洞察：为什么人类能做到这一点；
  * 更好地分析偏差和方差。

虽然机器学习模型超过 HLP 是比较困难的。但是只要提供足够多的样本数据，训练复杂的神经网络，模型预测准确性会大大提高，很有可能接近甚至超过
HLP。值得一提的是当算法模型的表现超过 HLP 时，很难再通过人的直觉来继续提高算法模型性能。

通常，我们把 Training Error 与 Human-Level Error 之间的差值称为偏差（Bias），把 Dev Error 与
Training Error
之间的差值称为方差（Variance）。根据偏差和方差的相对大小，就可以知道是否发生了欠拟合（Underfitting）或者过拟合（Overfitting）。选择不同的
Human-Level Error，会直接影响偏差和方差的相对大小。例如，猫类图片识别问题，如果 Human-Level Error 为
1%，Training Error 为 8%，Dev Error 为 10%。由于 Training Error 与 Human-Level Error
相差 7%，Dev Error 与 Training Error 只相差 2%，所以偏差是主要问题。如果图片很模糊，肉眼也看不太清，Human-Level
Error 上升到 7%。这时，由于 Training Error 与 Human-Level Error 只相差 1%，Dev Error 与
Training Error 相差 2%，所以方差又变成了主要问题。

通常在构建神经网络模型时，可以采用以下方法来减小偏差问题：

  * 增加模型复杂度；
  * 使用更好的优化算法：Monmentum、RMSprop、Adam。

还可以采用以下方法来减小方差问题：

  * 增加训练数据；
  * 使用正则化技巧：L1/L2 正则化、Dropout。

我们再来看一下 Human-Level Error 与 Bayes Optimal Error 的关系。对于图像识别、语音识别这类问题，Human-
Level Error 是很低的，很接近理想情况下的 Bayes Optimal Error。因此，上面例子中的1%和7%都可以近似看成是两种情况下对应的
Bayes Optimal Error。实际应用中，我们一般会用 Human-Level Error 来代表 Bayes Optimal Error。

### 错误分析

对已经建立的神经网络模型进行错误分析（Error
Analysis）十分必要，而且有针对性地、正确地进行错误分析更加重要，能够有效帮助我们改善模型结构，提高模型性能。

**1\. 从误分类样本中定位问题所在。**

举个例子，猫类图片识别，已知的模型错误率为
10%。我们分析误分类样本，发现模型会将一些狗类图片错误分类成猫。一种常规的解决办法是增加狗类样本，加强模型对狗类（负样本）的训练。但是，这一过程可能会花费几个月的时间，耗费这么大的时间成本到底是否值得呢？也就是说扩大狗类样本，重新训练模型，对提高模型准确率到底有多大作用？这时候我们就需要进行正确的错误分析，帮助我们做出判断。

方法很简单，我们可以从误分类样本中统计出狗类的样本数量。根据其所占的比重，判断这一问题的重要性。假如狗类样本所占比重仅为
5%，即使我们花费几个月的时间扩大狗类样本，提升模型对狗类的识别率，改进后的模型错误率最多只会降低到9.5%。相比之前的
10%，并没有显著改善。我们把这种性能限制称为性能上限（Ceiling on Performance）。相反，如果错误样本中狗类所占比重为
50%，那么改进后的模型错误率有望降低到 5%，性能改善就很大了。因此，值得去花费更多的时间扩大狗类样本。

这种方法就是分析误分类样本产生的原因，统计每个因素所占比例，若所占比例很大，说明该因素是造成误分类的主要成分。那么就可以花费时间和精力来重新优化模型，消除这一因素的影响。

这种方法简单，且能够避免花费大量的时间精力去做一些对提高模型性能收效甚微的工作，让我们更专注解决影响模型正确率的主要问题，十分必要。

**2\. 正确处理样本标签错误的情况。**

在监督式学习中，训练样本有时候会出现输出标签标注错误的情况。出现这种情况如何处理呢？如果标签出错的情况是随机性的，且次数不多，则可以把它看成是系统噪声，一般可以直接忽略，无需纠正。然而，如果是系统标注错误，这将对机器学习算法造成影响，降低模型性能，需要进行纠正。

如果在验证集/测试集中出现了输出标签标注错误的情况，又该如何处理呢？还是上面第一点提出的方法，统计验证集/测试集中所有误分类样本中标签标注错误所占的比例。根据该比例的大小，决定是否需要修正所有的错误标签。

举个简单的例子，如果验证集/测试集总的错误率为10%，其中由于标签标注错误造成的错误率为0.5%，其它因素造成的错误率为9.5%。可见，标签标注错误率仅占总的错误率的5%，因此可以忽略。如果模型经过优化之后，性能准确度得到提升，验证集/测试集总的错误率为2%，其中由于标签标注错误造成的错误率为0.5%，其它因素造成的错误率为1.5%。可见，标签标注错误率占总的错误率的25%，因此不可以忽略，需要人工修正错误标签。

**3\. 根据错误类型判断是否出现偏差和方差。**

我们之前介绍过，根据 Human-Level Error、Training Error 和 Dev Error
的相对值就可以判断是否出现了偏差或者方差。但是，需要注意的一点是，如果训练集和验证集来源于不同分布，则无法直接根据相对值大小来判断是否出现了偏差或者方差。例如某个模型
Human-Level Error 为 0%，Training Error 为 1%，Dev Error 为
10%。根据我们之前的理解，显然该模型出现的方差较大。但是，Training Error 与 Dev Error 之间的 9%
差值可能来自算法本身（方差），也可能是样本分布不同的影响。比如验证集都是很模糊的图片样本，本身就难以识别，跟算法模型关系不大，因此不能简单认为主要是方差的影响。

在可能出现训练集和验证集来源于不同分布的情况下，准确定位偏差和方差相对大小的方法是设置 Train-Dev Set。Ng 给它的定义是：“Same
distribution as training set, but not used for training.”
也就是说，从原来的训练集中分割出一部分作为 Train-Dev Set，Train-Dev Set 不作为训练模型使用，而是与验证集一样用于验证。

这样，我们就有 Training Error、Training-Dev Error 和 Dev Error 三种错误类型。其中，Training Error
与 Human-Level Error 的差值反映了偏差；Training Error 与 Training-Dev Error
的差值反映了方差；Training-Dev Error 与 Dev Error 的差值反映了数据匹配问题，即训练集和验证集的样本分布不一致。

举例来说明，默认 Human-Level Error 为 0%，如果 Training Error 为 1%，Training-Dev Error 为
9%，Dev Error 为 10%：

  * 偏差：1% - 0% = 1%
  * 方差：9% - 1% = 8%
  * 数据匹配误差：10% - 9% = 1%

很明显，方差的问题比较突出。如果 Training Error 为 1%，Training-Dev Error 为 2%，Dev Error 为 10%：

  * 偏差：1% - 0% = 1%
  * 方差：2% - 1% = 1%
  * 数据匹配误差：10% - 2% = 8%

则数据匹配问题比较突出。

下面用一张图来总结一下 Human-Level Error、Training Error、Training-Dev Error、Dev Error 以及
Test Error 之间的关系：

![enter image description
here](https://images.gitbook.cn/8a787190-adc2-11e8-b483-e79b276a9d5b)

### 总结

本文从模型评估方法、训练/验证/测试集划分原则、贝叶斯最优误差和人类表现水平及错误分析等不同方面，给出了构建神经网络模型时的一些实用建议。

总的来说，快速构建一个神经网络模型是比较简单的，我们应该花费更多的精力来优化模型，使用各种分析方法和技巧不断提高模型的性能。

