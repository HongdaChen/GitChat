上一篇，我们一步步搭建了一个深层的神经网络，来实现图片的分类。结果显示，随着网络层数加深，隐藏层数增加，网络性能会有所提升。但是，单纯地通过增加网络层数也不一定能取得很好的效果，且模型容易发生过拟合。本文将主要来谈谈神经网络中的过拟合问题以及如何避免过拟合。

### 什么是过拟合

任何机器学习模型，包括神经网络都可能存在过拟合（Overfit）问题。下面用一张图来说明：

![enter image description
here](https://images.gitbook.cn/e8870de0-9ebe-11e8-b6f3-454e1d4b65e0)

上图中，分别用三个模型来拟合实际的样本点（红色 x 表示样本）。第一个模型是一条直线，模型简单，但是预测值与样本 Label
差别较大，这种情况称之为欠拟合（Underfit）。第三个模型是一个高阶多项式，模型过于复杂，虽然预测值与样本 Label
完全吻合，但是该模型在训练样本之外的数据上拟合效果可能很差，该模型可能把噪声也学习了。这种情况称之为过拟合（Overfit），即模型过于拟合训练样本的数据而泛化能力很差。第二个模型是二次曲线，模型复杂度中等，既能对训练样本有较好的拟合效果，也能保证有不错的泛化能力。这是我们构建神经网络模型希望得到的模型。

欠拟合和过拟合分别对应着高偏差（High Bias）和高方差（High
Variance）。偏差度量了学习算法的期望预测与真实结果的偏离程度，刻画描述了算法本身对数据的拟合能力，也就是训练数据的样本与训练出来的模型的匹配程度；方差度量了训练集的变化导致学习性能的变化，描述了数据扰动造成的影响；噪声则表示任何学习算法在泛化能力的下界，描述了学习问题本身的难度。所以，任何一个机器学习算法的误差可以拆分成偏差、方差、噪声三个方面，如下列公式所示：

$$E=bias+variance+noise$$

用一张图来表示方差和偏差的关系：

![enter image description
here](https://images.gitbook.cn/66d80530-9ec1-11e8-b6f3-454e1d4b65e0)

那么在神经网络模型中，如何判断模型是否出现了过拟合呢？一般地，我们会将所有的样本数据分成三个部分：训练集（Train Set）、验证集（Dev
Set）、测试集（Test
Set）。训练集用来训练你的算法模型，验证集用来验证不同算法的表现情况，从中选择最好的算法模型，测试集用来测试最好算法的实际表现，作为该算法的无偏估计。训练集、验证集、测试集各自占的比例一般分别为60%、20%、20%，但如果训练样本很多的时候，可相应减小验证集和测试集的比例。

根据训练集和验证集的错误率常常可以判别神经网络模型是否发生了过拟合。如果 Train Set Error 为3%，而 Dev Set Error 为
17%，即该算法模型对训练样本的识别很好，但是对验证集的识别却不太好。这说明了该模型对训练样本可能存在过拟合，模型泛化能力不强，导致验证集识别率低。如果
Train Set Error 为18%，而 Dev Set Error 为 19%，虽然二者 Error
接近，即该算法模型对训练样本和验证集的识别都不是太好。这说明了该模型对训练样本存在欠拟合。如果 Train Set Error 为18%，而 Dev Set
Error 为35%，说明了该模型既存在高偏差也存在高方差，这是最坏的情况。如果 Train Set Error 为 3%，而 Dev Set Error
为 5%，两者数值相近，且较小，说明了该模型不存在欠拟合和过拟合，是个不错的模型。

Train Set Error | Dev Set Error | 模型性能  
---|---|---  
3% | 17% | 低偏差、高方差  
18% | 19% | 高偏差、低方差  
18% | 35% | 高偏差、高方差  
3% | 5% | 低偏差、低方差  
  
神经网络模型中，一般可以通过增加神经网络的隐藏层个数、神经元个数，延长训练时间，选择其它更复杂的 NN
模型等措施来提高模型复杂度。但是为了避免发生过拟合，通常需要一些正则化技巧提高模型的泛化能力。

### L1、L2 正则化

神经网络 L2 正则化是指在损失函数中增加 $||w||^2$ 项，表达式如下：

$$J(w^{[1]},b^{[1]},\cdots,w^{[L]},b^{[L]})=\frac1m\sum_{i=1}^mL(\hat
y^{(i)},y^{(i)})+\frac{\lambda}{2m}\sum_{l=1}^L||w^{[l]}||^2$$

$$||w^{[l]}||^2=\sum_{i=1}^{n^{[l]}}\sum_{j=1}^{n^{[l-1]}}(w_{ij}^{[l]})^2$$

其中，$l$ 表示当前神经网络层数，$n^{[l]}$ 为当前神经网络层包含的神经元个数。通常，我们把 $||w^{[l]}||^2$ 称为
Frobenius 范数，记为 $||w^{[l]}||_F^2$。一个矩阵的 Frobenius 范数就是计算所有元素平方和再开方，如下所示：

$$||A||_F=\sqrt {\sum_{i=1}^m\sum_{j=1}^n|a_{ij}|^2}$$

值得注意的是，一般只对网络参数 W 进行正则化而不对常数项 b 进行正则化。原因是一般 W 的维度很大，而 b 只是一个常数。相比较来说，参数很大程度上由
W 决定，改变 b 值对整体模型影响较小。所以，一般为了简便，就忽略对 b 的正则化了。但是，对 b 进行正则化也没有什么不妥。

加入 L2 正则化项之后，梯度下降算法中的 $dW^{[l]}$ 表达式需要做如下修改：

$$dW^{[l]}=dW^{[l]}_{before}+\frac{\lambda}{m}W^{[l]}$$

$$W^{[l]}:=W^{[l]}-\alpha\cdot dW^{[l]}$$

对于 L1 正则化，是指在损失函数中增加 $|w|$ 项，表达式如下：

$$J(w^{[1]},b^{[1]},\cdots,w^{[L]},b^{[L]})=\frac1m\sum_{i=1}^mL(\hat
y^{(i)},y^{(i)})+\frac{\lambda}{2m}\sum_{l=1}^L|w^{[l]}|$$

$$|w^{[l]}|=\sum_{i=1}^{n^{[l]}}\sum_{j=1}^{n^{[l-1]}}|w_{ij}^{[l]}|$$

与 L2 正则化相比，L1 正则化得到的解 w 更加稀疏，即很多 w 为零值。其优点是节约存储空间，因为大部分 w 为0。然而，L1
正则化的缺点也很明显，它包含了绝对值项，微分求导比较复杂。所以，一般 L2 正则化更加常用。

L1、L2 正则化中的 $\lambda$ 就是正则化参数，是可调超参数。训练模型时，可以设置 $\lambda$
为不同的值，在验证集中进行验证，选择最佳的 $\lambda$。

### L2 正则化的物理解释

我们知道，正则化的目的是限制参数过多或者过大，避免模型更加复杂。例如，如果使用非常深层的神经网络，模型可能过于复杂，容易发生过拟合。为了达到这一目的，最直观的方法就是限制
w 的个数，但是这类条件属于 NP-hard 问题，求解非常困难。所以，一般的做法是寻找更宽松的限定条件：

$$||w^{[l]}||^2\leqslant C$$

上式是对 $w$ 的平方和做数值上界限定，即所有 $w$ 的平方和不超过参数 $C$。这时候，我们的目标就转换为：最小化训练样本误差，但是要遵循 $w$
平方和小于 $C$ 的条件。

下面，我用一张图来说明如何在限定条件下，对训练样本误差 $E_{in}$ 进行最小化的优化。

![enter image description
here](https://images.gitbook.cn/007bd580-9ecd-11e8-b6f3-454e1d4b65e0)

如上图所示，蓝色椭圆区域是最小化 $Ein$ 区域，红色圆圈是 $w$
的限定条件区域。在没有限定条件的情况下，一般使用梯度下降算法，在蓝色椭圆区域内会一直沿着 $w$ 梯度的反方向前进，直到找到全局最优值
$w_{lin}$。例如空间中有一点 $w$（图中紫色点），此时 $w$ 会沿着 $-\nabla E_{in}$
的方向移动，如图中蓝色箭头所示。但是，由于存在限定条件，$w$ 不能离开红色圆形区域，最多只能位于圆上边缘位置，沿着切线方向。$w$
的方向如图中红色箭头所示。

那么问题来了，存在限定条件，$w$ 最终会在什么位置取得最优解呢？也就是说在满足限定条件的基础上，尽量让 $E_{in}$ 最小。

我们来看，$w$ 是沿着圆的切线方向运动，如上图绿色箭头所示。运动方向与 $w$ 的方向（红色箭头方向）垂直。运动过程中，根据向量知识，只要
$-\nabla E_{in}$ 与运行方向有夹角，不垂直，则表明 $-\nabla E_{in}$ 仍会在 $w$ 切线方向上产生分量，那么 $w$
就会继续运动，寻找下一步最优解。只有当 $-\nabla E_{in}$ 与 $w$ 的切线方向垂直时，$-\nabla E_{in}$ 在 $w$
的切线方向才没有分量，这时候 $w$ 才会停止更新，到达最接近 $w_{lin}$ 的位置，且同时满足限定条件。

![enter image description
here](https://images.gitbook.cn/a16b05b0-9ecd-11e8-991f-1fa5582600fd)

$-\nabla E_{in}$ 与 $w$ 的切线方向垂直，即 $-\nabla E_{in}$ 与 $w$
的方向平行。如上图所示，蓝色箭头和红色箭头互相平行。这样，根据平行关系得到：

$$-\nabla E_{in}+\lambda w=0$$

移项，得：

$$\nabla E_{in}+\lambda w=0$$

这样，我们就把优化目标和限定条件整合在一个式子中了。也就是说只要在优化 $E_{in}$ 的过程中满足上式，就能实现正则化目标。

接下来，重点来了！根据最优化算法的思想：梯度为 0 的时候，函数取得最优值。已知 $\nabla E_{in}$ 是 $E_{in}$
的梯度，观察上式，$\lambda w$ 是否也能看成是某个表达式的梯度呢？

当然可以！$\lambda w$ 可以看成是 $\frac12\lambda w^2$ 的梯度：

$$\frac{\partial}{\partial w}(\frac12\lambda w^2)=\lambda w$$

这样，我们根据平行关系求得的公式，构造一个新的损失函数：

$$E_{aug}=E_{in}+\frac{\lambda}{2}w^2$$

之所以这样定义，是因为对 $E_{aug}$ 求导，正好得到上面所求的平行关系式。上式中等式右边第二项就是 L2 正则化项。这样，
我们从图像化的角度，分析了 L2 正则化的物理意义，解释了带 L2 正则化项的损失函数是如何推导而来的。

### Dropout 正则化

Dropout
正则化，顾名思义，是指在深度学习神经网络的训练过程中，对于每层的神经元，按照一定的概率将其暂时从网络中丢弃。也就是说，每次训练时，每一层都有部分神经元不工作，将它们丢弃起到简化复杂网络模型的效果，从而避免发生过拟合，提高模型的泛化能力。

Dropout 示意图如下所示：

![enter image description
here](https://images.gitbook.cn/f1a44160-9ed0-11e8-8e5d-ef0460a9dd5a)

Dropout 的一般做法是：在训练阶段，每层的所有神经元将以概率 p 被保留（Dropout 丢弃率为
1-p）。在测试阶段，保留所有神经元，但是每层神经元权重参数 w 要乘以 p。之所以乘上 p
是因为测试阶段保留了所有神经元，没有“丢弃”神经元，为了保证测试阶段和训练阶段具有同样的输出期望值。但这需要对测试的代码进行更改并增加测试时的计算量，非常影响测试性能。因此，下面介绍一种常用的方法：Inverted
Dropout。

Inverted Dropout 在训练的时候，原神经元的输出直接乘以 $\frac1p$，以获得同样的期望值。举个例子来说明，假设对于第 $l$
层神经元的输出是 $al$，保留神经元比例概率 keep_prob=0.8，即该层有 20% 的神经元停止工作。经过 Dropout
正则化作用，随机删减20%的神经元，只保留80%的神经元。最后，还要对 $al$ 进行 Scale Up 处理，即乘以 $\frac1p$。相应的
Python 示例代码如下：

    
    
    dl = np.random.rand(al.shape[0],al.shape[1])<keep_prob
    al = np.multiply(al,dl)
    al /= keep_prob
    

迭代训练过程，每次迭代时，都会随机删除掉隐藏层一定数量的神经元；然后，在剩下的神经元上进行正向传播和反向传播过程，更新权重 w 和常数项
b。一般每次迭代训练都会随机选取到不同的神经元，这样最大限度地保证了 Dropout 的效果。

训练完成后，在测试过程中，则不需要进行 Dropout 和随机删减神经元的操作，此时所有的神经元都在工作。

为什么 Dropout 有防止过拟合的效果呢？

从权重 $w$ 的角度来看，对于某个神经元来说，某次训练时，它的某些输入在 Dropout
的作用被过滤了。而在下一次训练时，又有某些不同的输入被过滤。经过多次训练后，某些输入被过滤，某些输入被保留。这样，该神经元就不会因某个输入而受到非常大的影响，影响被各个输入均匀化了。也就是说，一般不会出现某个输入权重
$w$ 很大的情况。从效果上来说，与 L2 正则化效果是类似的，都是对权重 $w$ 进行“惩罚”，限制 $w$ 过大。示意图如下所示：

![enter image description
here](https://images.gitbook.cn/e9f66870-9ed6-11e8-991f-1fa5582600fd)

从神经元角度来看，每次舍弃一定数量的隐藏层神经元，相当于在不同的神经网络上进行训练，这样就减少了各神经元之间的依赖性，即每个神经元不能依赖于某几个其他的神经元（指层与层之间相连接的神经元），使神经网络更加能学习到与其他神经元之间的更加健壮的特征，效果能够避免发生过拟合。

使用 Dropout 有几点实用的建议：

  * 不同隐藏层的 Dropout 系数 `keep_prob` 可以不同。一般来说，神经元越多的隐藏层，`keep_out` 可以设置得小一些，例如 0.5；神经元越少的隐藏层，`keep_out` 可以设置的大一些，例如 0.8。

  * 实际应用中，不建议对输入层进行 Dropout，如果输入层维度很大，例如图片，那么可以设置 Dropout，但是 `keep_out` 应设置的大一些，例如 0.8，0.9。

  * 原则上越容易出现过拟合的隐藏层，其 `keep_prob` 就设置的相对小一些。通常可以使用交叉验证来选择 `keep_prob` 值的大小。

  * 对绘制的损失函数进行调试时，一般做法是将所有层的 `keep_prob` 全设置为 1，再绘制，即涵盖所有神经元，看损失函数是否单调下降。下一次迭代训练时，再将 `keep_prob` 设置为其它值。也就是说，绘制损失函数时使用的是所有神经元。

### 其它正则化技巧

除了 L1、L2 正则化、Dropout 正则化技巧之外，还有其它正则化技巧防止过拟合。

一种最直接也是最有效的方法就是增加训练样本，但直接获取更多样本的成本通常比较高，我们可以对已有的训练样本进行一些处理来“制造”出更多的样本，这种方法被称为
Data
Augmentation。例如在图片识别问题中，我们可以对已有的图片进行轻微扭曲、水平翻转、垂直翻转、任意角度旋转、缩放或扩大等等。这样就能有效地在原有样本的基础上创造出更多有效的样本数据，而且操作简单，不需要增加额外的成本。

![enter image description
here](https://images.gitbook.cn/0746ae60-9efc-11e8-991f-1fa5582600fd)

另外一种防止过拟合的方法就是选取合适的迭代训练次数。一般来说，神经网络模型随着迭代训练次数增加，Train Set Error 一般是逐渐减小的，而 Dev
Set Error
会先减小，后增大。也就是说训练次数过多时，模型会对训练样本拟合的越来越好，但是对验证集拟合效果逐渐变差，即发生了过拟合。因此，迭代训练次数不是越多越好，可以根据
Train Set Error 和 Dev Set Error 随着迭代次数的变化趋势，通过交叉验证选择合适的迭代次数，这种方法也成为 Early
Stopping。Train Set Error 和 Dev Set Error 随迭代训练次数变化而变化的趋势图如下所示：

![enter image description
here](https://images.gitbook.cn/b68bc2c0-9efc-11e8-8e5d-ef0460a9dd5a)

### 总结

本文主要介绍了几种常用的正则化方法，总的来说，无论是 L1、L2 正则化还是 Dropout 正则化，还是 Data Augmentation、Early
Stopping 都是比较常用的正则化技巧，对防止过拟合有着不错的效果。

