上一篇是实战内容，我们使用 Python
一步步搭建了一个最简单的神经网络模型，只包含单层隐藏层。并使用这个简单模型对非线性可分的样本集进行分类，最终得到了不错的分类效果。本文将继续从深度神经网络入手，介绍深层神经网络的数学原理和推导过程。

### 神经网络为什么要深？

我们都知道神经网络能处理很多问题，而且效果显著。其强大能力主要源自神经网络足够“深”，也就是说网络层数越多，神经网络就更加复杂，处理数据的能力越强，模型就越具有强大的学习能力。这符合我们直观的理解。下面通过两个例子来解释其中内在的原因。

首先来看神经网络在图像处理、人脸识别领域的应用。神经网络的输入是一张图片，从计算机角度来看，接收的是一个一个像素值。神经网络的第一层主要是从原始图片中提取一些边缘信息，例如面部的水平、垂直边缘。这样每个神经元做的事是边缘检测。然后，神经网络的第二层是将前一层得到的边缘信息进行整合，提取出面部的一些局部特征，例如眼睛、鼻子、嘴巴等等。之后，神经网络更深，提取的特征更加复杂，从模糊到细节、从局部到整体。可见，如果隐藏层足够多，那么能够提取的特征就越丰富、越复杂，模型的准确率就会越高。

下图是一个人类识别的例子，随着网络层数加深，隐藏层提取的特征也逐渐变得复杂。

![enter image description
here](https://images.gitbook.cn/98ef5690-985e-11e8-911c-dd974a01956f)

语音识别神经网络模型也是一样，浅层的神经元能检测出简单的音调，随着网络层数加深，神经元还能检测出基本的音素、单词信息，甚至对短语和句子进行检测。提取的特征由简单到复杂，功能也越来越强大。

值得注意的是，虽然深层神经网络学习能力更强大、性能更好。但是在实际应用中，我们还是要尽量首先选择层数较少的网络层数，这符合奥卡姆剃刀定律（Occam’s
Razor），能够有效避免发生过拟合。对于比较复杂的问题，再考虑使用深层神经网络模型。

### 深层神经网络标记符号

我们通常说的 L-layer 神经网络，L 等于隐藏层和输出层数之和。例如，简单的逻辑回归没有隐藏层，可称为 1-layer 神经网络；有 1
个隐藏层可称为 2-layers 神经网络；有 2 个隐藏层可称为 3-layers 神经网络；以此类推，有 L-1 个隐藏层可称为 L-layers
神经网络，网络最后的 L 层即为输出层。

![enter image description
here](https://images.gitbook.cn/c91f78a0-9862-11e8-b78f-09922e3c574f)

以 4-layer 神经网络为例，如上图所示，L = 5。在命名习惯上，我们把输入层称之为第 0 层，输出层称之为第 L 层。$l$
表示第几层，$l=0,1,\cdots,L$。$n^{[l]}$ 表示第 $l$ 层神经元个数。上图中，$n^{[0]}=2$，即
$x_1$，$x_2$，$n^{[1]}=3$，$n^{[2]}=5$，$n^{[3]}=4$，$n^{[4]}=2$，$n^{[5]}=1$。第 $l$
层的线性输出用 $z^{[l]}$ 表示，第 $l$ 层神经元的输出（非线性输出）用 $a^{[l]}$ 表示。一般把输入 $x$ 记为
$a^{[0]}$，把输出 $\hat y$ 记为 $a^{[L]}$。神经网络第 $l$ 层的参数分别用 $W^{[l]}$ 和 $b^{[l]}$
来表示。第 $l$ 层的激活函数用 $g^{[l]}(\cdot)$ 表示。

### 正向传播

深层神经网络的正向传播比较简单，仍以上一节中的 5 层神经网络为例，推导网络的正向传播过程。

当 $l=1$ 时：

$$z^{[1]}=W^{[1]}x+b^{[1]}=W^{[1]}a^{[0]}+b^{[1]}$$

$$a^{[1]}=g^{[1]}(z^{[1]})$$

当 $l=2$ 时：

$$z^{[2]}=W^{[2]}a^{[1]}+b^{[2]}$$

$$a^{[2]}=g^{[2]}(z^{[2]})$$

当 $l=3$ 时：

$$z^{[3]}=W^{[3]}a^{[2]}+b^{[3]}$$

$$a^{[3]}=g^{[3]}(z^{[3]})$$

当 $l=4$ 时：

$$z^{[4]}=W^{[4]}a^{[3]}+b^{[4]}$$

$$a^{[4]}=g^{[4]}(z^{[4]})$$

当 $l=5$ 时：

$$z^{[5]}=W^{[5]}a^{[4]}+b^{[5]}$$

$$a^{[5]}=g^{[5]}(z^{[5]})$$

如果有 m 个训练样本，则正向传播的矩阵形式为：

当 $l=1$ 时：

$$Z^{[1]}=W^{[1]}X+b^{[1]}=W^{[1]}A^{[0]}+b^{[1]}$$

$$A^{[1]}=g^{[1]}(Z^{[1]})$$

当 $l=2$ 时：

$$Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}$$

$$A^{[2]}=g^{[2]}(Z^{[2]})$$

当 $l=3$ 时：

$$Z^{[3]}=W^{[3]}A^{[2]}+b^{[3]}$$

$$A^{[3]}=g^{[3]}(Z^{[3]})$$

当 $l=4$ 时：

$$Z^{[4]}=W^{[4]}A^{[3]}+b^{[4]}$$

$$A^{[4]}=g^{[4]}(Z^{[4]})$$

当 $l=5$ 时：

$$Z^{[5]}=W^{[5]}A^{[4]}+b^{[5]}$$

$$A^{[5]}=g^{[5]}(Z^{[5]})$$

综上所述，第 $l$ 层的正向传输过程表示如下：

$$Z^{[l]}=W^{[l]}A^{[l-1]}+b^{[l]}$$

$$A^{[l]}=g^{[l]}(Z^{[l]})$$

各个变量和参数的维度如下所示：

$$Z^{[l]}:(n^{[l]},m)$$

$$A^{[l]}:(n^{[l]},m)$$

$$W^{[l]}:(n^{[l]},n^{[l-1]})$$

$$b^{[l]}:(n^{[l]},1)$$

值得注意的是，$dZ^{[l]}、dA^{[l]}、dW^{[l]}、db^{[l]}$ 的维度分别与
$Z^{[l]}、A^{[l]}、W^{[l]}、b^{[l]}$ 相同。

### 反向传播

L 层神经网络反向传播过程的推导与单隐藏层神经网络的推导过程非常类似。接下来我们就来进行详细的推导。

令损失函数为 $J$，$J$ 对 $A^{[L]}$ 的偏导数为 $dA^{[L]}$。

$J$ 对 $Z^{[L]}$ 的偏导数为：

$$dZ^{[L]}=dA^{[L]}*g^{[L]'}(Z^{[L]})$$

其中，$g^{[L]'}$ 表示 L 层激活函数的导数，$*$ 表示点乘而非矩阵相乘。这点需要特别注意。

这里，先把正向传播的表达式写下来：

$$Z^{[L]}=W^{[L]}A^{[L-1]}+b^{[L]}$$

$$A^{[L]}=g^{[L]}(Z^{[L]})$$

则 $J$ 对 $W^{[L]}$ 的偏导数为：

$$dW^{[L]}=\frac1mdZ^{[L]}\cdot A^{[L-1]T}$$

$J$ 对 $b^{[L]}$ 的偏导数为：

$$db^{[L]}=\frac1mnp.sum(dZ^{L]},axis=1)$$

$J$ 对 $A^{[L-1]}$ 的偏导数为：

$$dA^{[L-1]}=W^{[L]T}\cdot dZ^{[L]}$$

显然，知道了 $dA^{[L]}$ 之后，就可以推导出 $dZ^{[L]}、dW^{[L]}、db^{[L]}、dA^{[L-1]}$
的表达式。那么，对于第 $l$ 层，已知 $dA^{[l]}$ 之后，就可以推导：

$$dZ^{[l]}=dA^{[l]}*g^{[l]'}(Z^{[l]})$$

$$dW^{[l]}=\frac1mdZ^{[l]}\cdot A^{[l-1]T}$$

$$db^{[l]}=\frac1mnp.sum(dZ^{l]},axis=1)$$

$$dA^{[l-1]}=W^{[l]T}\cdot dZ^{[l]}$$

这样，知道了反向传播每一层的递推公式之后，就可以从 L 层逐步反推至 0
层，得到各层的变量和参数的偏导数。我们把正向传播和反向传播涉及的公式全都整理出来，以供查阅：

![enter image description
here](https://images.gitbook.cn/10b00b60-9892-11e8-a5a1-17130ea31e3a)

### 多分类 Softmax

目前我们介绍的都是二分类问题，神经网络输出层只有一个神经元。若输出 $A^{[L]}\geqslant0.5$，则判断为正类（1）；若输出
$A^{[L]}<0.5$，则判断为负类（0） 。

对于多分类问题，用 C 表示种类个数，神经网络中输出层就有 C 个神经元，即 n[L]=C。其中，每个神经元的输出依次对应属于该类的概率，即
P(y=c|x)。为了处理多分类问题，我们一般使用 Softmax 回归模型。Softmax 回归模型输出层的激活函数如下所示。

如果是多分类问题，例如手写数字识别（0～9），用 $C$ 表示类别个数，则神经网络输出层就有 $C$
个神经元，$n^{[L]}=C$。每个神经元的输出分别对应相应类别的概率。为了处理多分类问题，一般在输出层使用 Softmax 函数而不是 Sigmoid
函数。Softmax 对输出层的 $Z^{[L]}$ 作如下处理：

$$a^{[L]}_i=\frac{e^{z^{[L]}_i}}{\sum_{i=1}^Ce^{z^{[L]}_i}}$$

其中，$a^{[L]}_i$ 满足：

$$\sum_{i=1}^C a^{[L]}_i=1$$

举个简单的例子，$C = 4$，$a^{[L]}$ 的输出为：

![](https://images.gitbook.cn/76462ea0-9c39-11e8-963d-2304deb492e0)

则经过 Softmax 处理后，$a^{[L]}$ 的输出为：

![](https://images.gitbook.cn/b1862e70-9c39-11e8-963d-2304deb492e0)

值得一提的是，$z^{[L]}$ 和 $a^{[L]}$ 在各类别的数值大小顺序上并没有不同。即无论是 $z^{[L]}$ 还是
$a^{[L]}$，第二类别的输出总是最大的。

接下来，我们来看一下如何定义引入 Softmax 之后的损失函数。简单来说，我们希望正确类别对应的概率越大越好，即 $a^{[L]}_c$ 越大越好，$c$
为正确类别。那么就可以定义损失函数为：

$$J=-\frac1m\sum_{i=1}^m\sum_{j=1}^C y_j^{(i)}log\ a_j^{[L](i)}$$

其中，$y_j^{(i)}$ 中正确类别对应的元素为 1，其它为 0。

损失函数 $J$ 对 $a^{[L]}$ 的偏导数为：

$$da^{[L]}=-\frac{y}{a^{[L]}}$$

$a^{[L]}$ 对 $z^{[L]}$ 的偏导数为：

$$\frac{\partial a^{[L]}}{\partial z^{[L]}}=\frac{\partial}{\partial
z^{[L]}}\cdot (\frac{e^{z^{[L]}_i}}{\sum_{i=1}^Ce^{z^{[L]}_i}})=a^{[L]}\cdot
(1-a^{[L]})$$

则 $J$ 对 $z^{[L]}$ 的偏导数为：

$$dz^{[L]}=da^{[L]}\cdot \frac{\partial a^{[L]}}{\partial
z^{[L]}}==a^{[L]}-y$$

若对于 m 个样本：

$$dZ^{[L]}=A^{[L]}-Y$$

这个结果我们非常熟悉，其与二分类推导得到的 $dZ^{[L]}$
是完全一致的。接下来，就可以按照上文中介绍深层神经网络的反向传播过程进行递归推导各个变量和参数。

### 总结

本文主要介绍了深层神经网络模型的结构和常用的标记方法，详细推导了深层神经网络模型的正向传播和反向传播过程。最后，介绍了多分类 Softmax 模型，推导出
Softmax 的 $dZ^{[L]}$ 表达式与二分类的 $dZ^{[L]}$ 完全一致。

