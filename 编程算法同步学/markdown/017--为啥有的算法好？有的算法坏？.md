上一节课讲了顺序查找的原理、数据结构和控制流程，按说接下来就该讲编程实现了。不过在第一次讲算法落实到程序之前，我们先要讲明一个问题：
**如何衡量一个算法的性能** 。

### 性能指标

每个算法都有一个目标任务，能够完成这个任务，是算法功能的体现。但同样是完成这个任务，有的算法快有的算法慢，有的几乎不需要额外的存储空间，有的则非要占一大块……这就是算法的
**性能** 不同了。

我们看机械设备的性能会考虑耗油/耗电量，使用寿命，单位时间损耗等诸多指标，那么看一个算法的性能，要考虑哪些指标呢？

最主要的是两个： **时间复杂度** 和 **空间复杂度** 。

前者指算法需要消耗的时间资源，后者则是算法需要消耗的存储空间资源。

### 时间复杂度

#### 运算速度的表达

算法时间复杂度的形式化表达是一个 **函数** ，这个函数描述了该算法的运行时间。

既然有函数就有自变量和因标量，因变量是算法运行时间的话，自变量又是什么呢？

答：自变量是问题的规模。而问题规模的直接体现就是输入数据的数量。

比如之前讲过的顺序查找，同样是找一个目标数，在10个数字里找和在100个数字里找所需时间肯定不一样。

当然了，算法是通过程序来是实现的，程序是通过计算机运行的，由于计算机的硬件不同，同样的程序和输入数据到了不同的计算机上，运行起来很可能时间也不同，难道是算法性能变了？

这当然是不可能的，我们定义的算法性能，与具体的程序执行脱节，仅讨论理论上的时间消耗。

这种理论上的时间消耗可以直观理解为算法中 **基本操作的个数（或者叫做“步数”）** 。

也就是说，我们可以通过计算：在输入数据规模为n的情况下，某算法从开始到结束总共执行了多少个基本操作，来确定该算法的时间复杂度。

这个时间复杂度写出来就是 $f(n)$——其中$n$是问题规模，用输入数据量来表示。

#### 顺序查找的步数

我们来看看顺序查找，其中有哪些基本操作呢？

对于顺序查找而言，它的基本操作就是下图中红圈圈出来的部分，即每个元素值与目标数进行比较的操作。

![enter image description
here](https://images.gitbook.cn/e0b3c050-8b4d-11e9-abd4-3359f30b3591)

假设输入数据（待查数列）长度为n（有n个元素），红圈中的操作就会执行n次，所以这个算法的总步数就是n。

可能有同学要说了，不对呀：

**_疑问-1_** ：红圈处上下还各有操作呢，上面是比较i和len（arr）的大小，下面是i递增1，这些明明都是操作嘛，怎么没计算进去？

**_疑问-2_**
：算法是一碰到和目标数一样的元素就停止，可能这次第一个元素就碰到，下次第二个元素就碰到，并不是每次都要把n个元素比较完啊，怎么能就算成n呢？

  * 对于疑问-1，红圈上下的操作是分别是对循环的进入条件的判断和修改，只要有循环存在，就一定存在这两者，否则要么没有循环，要么就形成死循环，都不是正常的循环结构。因此，我们不把这种构造循环的操作归属于任何算法。

既然所有包含循环的算法就一定包含对循环条件的检查和增/减操作，那干脆我们统一忽视它们就是了。

  * 对于疑问-2，本身这种提法是对的。

假设我们的待查数列有n个元素，那么其中任何一个是第一个与目标数一致的可能性都是1/(n+1)
——之所以不是1/n是因为还有一种可能就是n个元素都不等于目标数。

那么我们来精细地计算一下n个待查数分别与目标数相等及都不相等这（n+1）情况的平均运行步数。这个平均步数应该是：（1 + 2 + 3 + …… +
n）/（n+1）= n/2 。

顺序查找的平均步数只有n的1/2哦，并不是n啊！

如果我们要用一个精确的多项式来表述顺序查找的时间复杂度的话，$f(n) = n$ 和 $f(n) = n/2$
的确会相差很多。不过在考虑时间复杂度的时候，我们其实真正关心的是 **运算量的量级** ，而非精确数字！n/2和n的量级是一样的。

如何才能够准确描述“量级”呢？我们需要引入：大O记号。

#### 大O记号

我们选用一个特殊的字符来表达函数的量级，这个符号是：O，读作大O（英语：Big O）。

**顺序查找的时间复杂度是O(n).**

大O是一个数学记号，它描述了一个函数在其参数达到某一特定值或者无穷大时的极限行为。这个记号体现了函数的增长率。

假设有两个函数$f(x)$和$g(x)$，如果存在$c>0$，和$x _0$, 当$x \geqslant x_ 0$时，总有$f(x) \leqslant
cg(x)$，则我们说$f(x) = O(g(x))$。

比如下面这个例子：

![enter image description
here](https://images.gitbook.cn/03c6f030-8b4e-11e9-b38f-03c8201e19f7)

上图中的红线表示$f(x)$，而蓝线表示$g(x)$。

存在$x _0 = 4.4$,以及$c=1$，当$x \geqslant x_ 0$时，总有$f(x) \leqslant 1·g(x）$。因此我们就可以说
$f(x)=O(g(x))$.

#### 常见算法的时间复杂度

回头看顺序查找的时间复杂度$f(n) = n/2$, 设$g(n) = n$, 则存在
$c=1$(其实$c$只要不小于1/2就可以，我们为了方便就选个1)，使得$x \geqslant 0$时，总是有$f(n) \leqslant
g(n)$，因此$f(n)=O(g(n))$, 又因为$g(n) = n$，$所以f(n)=O(n)$。因此，我们说顺序查找的时间复杂度为$O(n)$.

那我们再来看几个其他的例子，假设某算法时间复杂度是$f(n) = n^2 + 2n + 3$, 则我们设 $g(n) = n^2$，存在$c=2$，当$x
\geqslant 3$时，总有$f(n) \leqslant 2g(n)$，于是$f(n)=O(n^2)$.

其他的——

$f(n) = n^3 + 3n^2 + 5n +12 => f(n) = O(n^3)$. $f(n) = log(n/2) => f(n) =
O(log(n))$ ……

通过几个直观的例子不难看出，用了大O记号之后，函数的表达式变得比以前简单了。因此，大O记号的作用可以简单理解为： **聚焦主要因素，忽略次要因素** 。

一般的基础算法，大致有如下几种时间复杂度（按量级由小到大排序）：

![enter image description
here](https://images.gitbook.cn/238094d0-8b4e-11e9-b38f-03c8201e19f7)

其中$O(1)$表示一个常数，这个常数可以是任意正数，可以很大（比如1000， 10000，
1000000等等），但却与输入数据量无关。这样的算法是非常难得的。

这些复杂度写成式子没什么感觉，画出图来就直观多了。大家看看下图，当$n$达到了一定的大小以后，$n$越是增加，不同时间复杂度差别也就越大！

![enter image description
here](https://images.gitbook.cn/3d7812a0-8b4e-11e9-b38f-03c8201e19f7)

除了极少数特别简单的算法，大部分我们常用的算法的时间复杂度都是$O(nlog(n))$量级 **起** 的！

在以后的日子里，大家会发现，$log(n)$是一个很可爱的式子，我们特别希望它出现，就算它出现不了，它的亲戚$nlog(n)$来也行，一旦出现$O(n^2)$，这个算法就有点艰难了。

>
> 告诉大家一个小tricky：如果你去面试程序员，被考了一个算法题。你给出的答案虽然功能对了，但是时间复杂度在$O(n^2)$或者更高，那这道题基本就算没做出来。

### 空间复杂度

空间复杂就是算法需要占据存储空间的多少。

当然了，任何程序只要在计算机里面执行就要占据存储，就算一点数据没有，光指令也要消耗存储。但是在绝大多数情况下，算法要处理的数据所需存储空间远大于非数据部分。因此数据之外的部分，我们直接就忽略不计了。

对任何一个算法而言，只要它处理n个输入数据，总要把这些数据读入到存储空间里面去，所以，对于任何问题规模为n的算法，它所需要消耗的存储空间都至少是O(n)。所有算法都要占的部分，我们就干脆一起不去管他就好了。

对算法而言，我们关心的实际上是它所消耗的额外存储空间——在存储输入数据之外还需要消耗的存储空间。

既不是程序体控制流程占据的，也不是输入数据占据的，那还有什么需要消耗存储空间呢？

答：用于在算法过程中临时性存储数据的缓存空间。

在顺序查找里，到底哪部分是缓存空间？不好意思，顺序查找里没有缓存空间。

不过缓存空间并不难理解，等我们到了后面排序算法的时候，就经常要用到缓存空间了。到时候再解释不迟。

我们现在需要知道两件事情：

  * 其一，空间复杂度也用大O记号来表达。

  * 其二，在计算机刚被发明出来的时候，存储部件特别地贵，那个时候节约空间特别重要，因此早期的算法会非常在意空间复杂度，其重要性几乎不下于时间复杂度。

不过随着后来硬件技术的发展，存储器件用量越来越大，I/O速度越来越快，价钱越来越便宜。加之数据库、分布式存储等软件技术的发展，早期被认为是极限的存储量一次次被突破，空间复杂度逐渐变得没有时间复杂度重要了。

> **小贴士** ：时至今日，当我们衡量一个算法性能的时候，一般主要关注的是它的时间复杂度。

