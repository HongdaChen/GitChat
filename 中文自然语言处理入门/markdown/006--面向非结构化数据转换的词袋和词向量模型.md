通过前面几个小节的学习，我们现在已经学会了如何获取文本预料，然后分词，在分词之后的结果上，我们可以提取文本的关键词查看文本核心思想，进而可以通过可视化技术把文档从视觉的角度表达出来。

下面，我们来看看，文本数据如何转换成计算机能够计算的数据。这里介绍两种常用的模型：词袋和词向量模型。

### 词袋模型（Bag of Words Model）

#### 词袋模型的概念

先来看张图，从视觉上感受一下词袋模型的样子。

![enter image description
here](http://images.gitbook.cn/bcd2fa60-62eb-11e8-8a60-1bdde4cc4659)

词袋模型看起来好像一个口袋把所有词都装进去，但却不完全如此。在自然语言处理和信息检索中作为一种简单假设，词袋模型把文本（段落或者文档）被看作是无序的词汇集合，忽略语法甚至是单词的顺序，把每一个单词都进行统计，同时计算每个单词出现的次数，常常被用在文本分类中，如贝叶斯算法、LDA
和 LSA 等。

#### 动手实战词袋模型

（1）词袋模型

本例中，我们自己动手写代码看看词袋模型是如何操作的。

首先，引入 jieba 分词器、语料和停用词（标点符号集合，自己可以手动添加或者用一个文本字典代替）。

    
    
        import jieba
        #定义停用词、标点符号
        punctuation = ["，","。", "：", "；", "？"]
        #定义语料
        content = ["机器学习带动人工智能飞速的发展。",
                   "深度学习带动人工智能飞速的发展。",
                   "机器学习和深度学习带动人工智能飞速的发展。"
                  ]
    

接下来，我们先对语料进行分词操作，这里用到 lcut() 方法：

    
    
        #分词
        segs_1 = [jieba.lcut(con) for con in content]
        print(segs_1)
    

得到分词后的结果如下：

> [['机器', '学习', '带动', '人工智能', '飞速', '的', '发展', '。'], ['深度', '学习', '带动',
> '人工智能', '飞速', '的', '发展', '。'], ['机器', '学习', '和', '深度', '学习', '带动', '人工智能',
> '飞速', '的', '发展', '。']]

因为中文语料带有停用词和标点符号，所以需要去停用词和标点符号，这里语料很小，我们直接去标点符号：

    
    
        tokenized = []
        for sentence in segs_1:
            words = []
            for word in sentence:
                if word not in punctuation:          
                    words.append(word)
            tokenized.append(words)
        print(tokenized)
    

去标点符号后，我们得到结果如下：

> [['机器', '学习', '带动', '人工智能', '飞速', '的', '发展'], ['深度', '学习', '带动', '人工智能',
> '飞速', '的', '发展'], ['机器', '学习', '和', '深度', '学习', '带动', '人工智能', '飞速', '的',
> '发展']]

下面操作就是把所有的分词结果放到一个袋子（List）里面，也就是取并集，再去重，获取对应的特征词。

    
    
        #求并集
        bag_of_words = [ x for item in segs_1 for x in item if x not in punctuation]
        #去重
        bag_of_words = list(set(bag_of_words))
        print(bag_of_words)
    

得到的特征词结果如下：

> ['飞速', '的', '深度', '人工智能', '发展', '和', '机器', '学习', '带动']

我们以上面特征词的顺序，完成词袋化：

    
    
        bag_of_word2vec = []
        for sentence in tokenized:
            tokens = [1 if token in sentence else 0 for token in bag_of_words ]
            bag_of_word2vec.append(tokens)
    

最后得到词袋向量：

> [[1, 1, 0, 1, 1, 0, 1, 1, 1], [1, 1, 1, 1, 1, 0, 0, 1, 1], [1, 1, 1, 1, 1,
> 1, 1, 1, 1]]

上面的例子在编码时，对于 for 循环多次直接用到列表推导式。在 Python 中，列表推导式的效率比 for
快很多，尤其在数据量大的时候效果更明显，建议多使用列表推导式。

（2）Gensim 构建词袋模型

下面我们介绍 Gensim 库的使用，继续沿用上面的例子：

    
    
        from gensim import corpora
        import gensim
        #tokenized是去标点之后的
        dictionary = corpora.Dictionary(tokenized)
        #保存词典
        dictionary.save('deerwester.dict') 
        print(dictionary)
    

这时我们得到的结果不全，但通过提示信息可知道共9个独立的词：

> Dictionary(9 unique tokens: ['人工智能', '发展', '学习', '带动', '机器']...)

那我们如何查看所有词呢？通过下面方法，可以查看到所有词和对应的下标：

    
    
        #查看词典和下标 id 的映射
        print(dictionary.token2id)
    

最后结果如下：

> {'人工智能': 0, '发展': 1, '学习': 2, '带动': 3, '机器': 4, '的': 5, '飞速': 6, '深度': 7,
> '和': 8}

根据得到的结果，我们同样可以得到词袋模型的特征向量。这里顺带提一下函数 doc2bow()，作用只是计算每个不同单词的出现次数，将单词转换为其整数单词 id
并将结果作为稀疏向量返回。

    
    
    corpus = [dictionary.doc2bow(sentence) for sentence in segs_1]
        print(corpus )
    

得到的稀疏向量结果如下：

> [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1)], [(0, 1), (1, 1),
> (2, 1), (3, 1), (5, 1), (6, 1), (7, 1)], [(0, 1), (1, 1), (2, 2), (3, 1),
> (4, 1), (5, 1), (6, 1), (7, 1), (8, 1)]]

### 词向量 （Word Embedding）

深度学习带给自然语言处理最令人兴奋的突破是词向量（Word
Embedding）技术。词向量技术是将词语转化成为稠密向量。在自然语言处理应用中，词向量作为机器学习、深度学习模型的特征进行输入。因此，最终模型的效果很大程度上取决于词向量的效果。

### 词向量的概念

在 Word2Vec 出现之前，自然语言处理经常把字词进行独热编码，也就是 One-Hot Encoder。

> 大数据 [0,0,0,0,0,0,0,1,0,……，0,0,0,0,0,0,0]
>
> 云计算[0,0,0,0,1,0,0,0,0,……，0,0,0,0,0,0,0]
>
> 机器学习[0,0,0,1,0,0,0,0,0,……，0,0,0,0,0,0,0]
>
> 人工智能[0,0,0,0,0,0,0,0,0,……，1,0,0,0,0,0,0]

比如上面的例子中，大数据 、云计算、机器学习和人工智能各对应一个向量，向量中只有一个值为1，其余都为0。所以使用 One-Hot Encoder有以下问题：

  * 第一，词语编码是随机的，向量之间相互独立，看不出词语之间可能存在的关联关系。
  * 第二，向量维度的大小取决于语料库中词语的多少，如果语料包含的所有词语对应的向量合为一个矩阵的话，那这个矩阵过于稀疏，并且会造成维度灾难。 

而解决这个问题的手段，就是使用向量表示（Vector Representations）。比如 Word2Vec 可以将 One-Hot Encoder
转化为低维度的连续值，也就是稠密向量，并且其中意思相近的词也将被映射到向量空间中相近的位置。经过降维，在二维空间中，相似的单词在空间中的距离也很接近。

这里简单给词向量一个定义，词向量就是要用某个固定维度的向量去表示单词。也就是说要把单词变成固定维度的向量，作为机器学习（Machine
Learning）或深度学习模型的特征向量输入。

#### 动手实战词向量

（1）Word2Vec

Word2Vec 是 Google
团队2013年推出的，自提出后被广泛应用在自然语言处理任务中，并且受到它的启发，后续出现了更多形式的词向量模型。Word2Vec
主要包含两种模型：Skip-Gram 和 CBOW，值得一提的是，Word2Vec 词向量可以较好地表达不同词之间的相似和类比关系。

下面我们通过代码实战来体验一下 Word2Vec。通过 `pip install gensim` 安装好库后，即可导入使用。

![enter image description
here](http://images.gitbook.cn/9e0a13c0-6b01-11e8-8431-a75f9cd1b0ae)

先导入 Gensim 中的 Word2Vec 和 jieba 分词器，再引入从百度百科抓取的黄河和长江的语料：

    
    
        from gensim.models import Word2Vec  
        import jieba
        #定义停用词、标点符号
        punctuation = [",","。", ":", ";", ".", "'", '"', "’", "?", "/", "-", "+", "&", "(", ")"]
        sentences = [
        "长江是中国第一大河，干流全长6397公里（以沱沱河为源），一般称6300公里。流域总面积一百八十余万平方公里，年平均入海水量约九千六百余亿立方米。以干流长度和入海水量论，长江均居世界第三位。",
        "黄河，中国古代也称河，发源于中华人民共和国青海省巴颜喀拉山脉，流经青海、四川、甘肃、宁夏、内蒙古、陕西、山西、河南、山东9个省区，最后于山东省东营垦利县注入渤海。干流河道全长5464千米，仅次于长江，为中国第二长河。黄河还是世界第五长河。",
        "黄河,是中华民族的母亲河。作为中华文明的发祥地,维系炎黄子孙的血脉.是中华民族民族精神与民族情感的象征。",
        "黄河被称为中华文明的母亲河。公元前2000多年华夏族在黄河领域的中原地区形成、繁衍。",
        "在兰州的“黄河第一桥”内蒙古托克托县河口镇以上的黄河河段为黄河上游。",
        "黄河上游根据河道特性的不同，又可分为河源段、峡谷段和冲积平原三部分。 ",
        "黄河,是中华民族的母亲河。"
        ]
    

上面定义好语料，接下来进行分词，去标点符号操作 ：

    
    
        sentences = [jieba.lcut(sen) for sen in sentences]
        tokenized = []
        for sentence in sentences:
            words = []
            for word in sentence:
                if word not in punctuation:          
                    words.append(word)
            tokenized.append(words)
    

这样我们获取的语料在分词之后，去掉了标点符号，如果做得更严谨，大家可以去停用词，然后进行模型训练：

    
    
        model = Word2Vec(tokenized, sg=1, size=100,  window=5,  min_count=2,  negative=1, sample=0.001, hs=1, workers=4)
    

参数解释如下：

  * sg=1 是 `skip-gram` 算法，对低频词敏感；默认 sg=0 为 CBOW 算法。
  * size 是输出词向量的维数，值太小会导致词映射因为冲突而影响结果，值太大则会耗内存并使算法计算变慢，一般值取为100到200之间。
  * window 是句子中当前词与目标词之间的最大距离，3表示在目标词前看3-b 个词，后面看 b 个词（b 在0-3之间随机）。
  * `min_count` 是对词进行过滤，频率小于 `min-count` 的单词则会被忽视，默认值为5。
  * negative 和 sample 可根据训练结果进行微调，sample 表示更高频率的词被随机下采样到所设置的阈值，默认值为 1e-3。
  * hs=1 表示层级 softmax 将会被使用，默认 hs=0 且 negative 不为0，则负采样将会被选择使用。
  * 详细参数说明可查看 Word2Vec 源代码。

训练后的模型可以保存与加载，如下代码所示：

    
    
        model.save('model')  #保存模型
        model = Word2Vec.load('model')   #加载模型
    

模型训练好之后，接下来就可以使用模型，可以用来计算句子或者词的相似性、最大匹配程度等。

例如，我们判断一下黄河和黄河自己的相似度：

    
    
        print(model.similarity('黄河', '黄河'))
    

结果输出为：

> 1.0000000000000002

例如，当输入黄河和长江来计算相似度的时候，结果就比较小，因为我们的语料实在太小了。

    
    
        print(model.similarity('黄河', '长江'))
    

结果输出为：

> -0.036808977457324699

下面我们预测最接近的词，预测与黄河和母亲河最接近，而与长江不接近的词：

    
    
    print(model.most_similar(positive=['黄河', '母亲河'], negative=['长江']))
    

得到结果如下，可以根据相似度大小找到与黄河和母亲河最接近的词（实际处理建议增大数据量和去停用词）。

> [('是', 0.14632007479667664), ('以', 0.14630728960037231), ('长河',
> 0.13878652453422546), ('河道', 0.13716217875480652), ('在',
> 0.11577725410461426), ('全长', 0.10969121754169464), ('内蒙古',
> 0.07590540498495102), ('入海', 0.06970417499542236), ('民族',
> 0.06064444035291672), ('中华文明', 0.057667165994644165)]

上面通过小数据量的语料实战，加强了对 Word2Vec 的理解，总之 Word2Vec
是一种将词变成词向量的工具。通俗点说，只有这样文本预料才转化为计算机能够计算的矩阵向量。

（2）Doc2Vec

Doc2Vec 是 Mikolov 在 Word2Vec 基础上提出的另一个用于计算长文本向量的工具。在 Gensim 库中，Doc2Vec 与
Word2Vec 都极为相似。但两者在对输入数据的预处理上稍有不同，Doc2vec 接收一个由 LabeledSentence
对象组成的迭代器作为其构造函数的输入参数。其中，LabeledSentence 是 Gensim 内建的一个类，它接收两个 List
作为其初始化的参数：word list 和 label list。

Doc2Vec 也包括两种实现方式：DBOW（Distributed Bag of Words）和 DM （Distributed Memory）。DBOW
和 DM 的实现，二者在 gensim 库中的实现用的是同一个方法，该方法中参数 dm = 0 或者 dm=1 决定调用 DBOW 还是
DM。Doc2Vec 将文档语料通过一个固定长度的向量表达。

下面是 Gensim 中 Doc2Vec
模型的实战，我们把上述语料每一句话当做一个文本，添加上对应的标签。接下来，定义数据预处理类，作用是给每个文章添加对应的标签：

    
    
        #定义数据预处理类，作用是给每个文章添加对应的标签
        from gensim.models.doc2vec import Doc2Vec,LabeledSentence
        doc_labels = ["长江","黄河","黄河","黄河","黄河","黄河","黄河"]
        class LabeledLineSentence(object):
            def __init__(self, doc_list, labels_list):
               self.labels_list = labels_list
               self.doc_list = doc_list
            def __iter__(self):
                for idx, doc in enumerate(self.doc_list):
                    yield LabeledSentence(words=doc,tags=[self.labels_list[idx]])
    
            model = Doc2Vec(documents,dm=1, size=100, window=8, min_count=5, workers=4)
            model.save('model')
            model = Doc2Vec.load('model')  
    

上面定义好了数据预处理函数，我们将 Word2Vec 中分词去标点后的数据，进行转换：

    
    
        iter_data = LabeledLineSentence(tokenized, doc_labels)
    

得到一个数据集，我开始定义模型参数，这里 dm=1，采用了 Gensim 中的 DM 实现。

    
    
        model = Doc2Vec(dm=1, size=100, window=8, min_count=5, workers=4)
        model.build_vocab(iter_data)
    

接下来训练模型， 设置迭代次数1000次，`start_alpha` 为开始学习率，`end_alpha` 与 `start_alpha` 线性递减。

    
    
        model.train(iter_data,total_examples=model.corpus_count,epochs=1000,start_alpha=0.01,end_alpha =0.001)
    

最后我们对模型进行一些预测：

    
    
        #根据标签找最相似的，这里只有黄河和长江，所以结果为长江，并计算出了相似度
        print(model.docvecs.most_similar('黄河'))
    

得到的结果：

> [('长江', 0.25543850660324097)]

然后对黄河和长江标签做相似性计算：

    
    
    print(model.docvecs.similarity('黄河','长江'))
    

得到的结果：

> 0.25543848271351405

上面只是在小数据量进行的小练习，而最终影响模型准确率的因素有：文档的数量越多，文档的相似性越好，也就是基于大数据量的模型训练。在工业界，Word2Vec 和
Doc2Vec 常见的应用有：做相似词计算；相关词挖掘，在推荐系统中用在品牌、用户、商品挖掘中；上下文预测句子；机器翻译；作为特征输入其他模型等。

总结，本文只是简单的介绍了词袋和词向量模型的典型应用，对于两者的理论和其他词向量模型，比如 TextRank 、FastText 和 GloVe
等，阅读文末给出参考文献将了解更多。

**参考文献：**

  1. <https://radimrehurek.com/gensim/tut1.html>
  2. <https://radimrehurek.com/gensim/models/word2vec.html>
  3. <https://radimrehurek.com/gensim/summarization/summariser.html>
  4. <https://radimrehurek.com/gensim/models/fasttext.html>
  5. <https://nlp.stanford.edu/projects/glove/>

### [示例数据下载](https://github.com/sujeek/chinese_nlp)

