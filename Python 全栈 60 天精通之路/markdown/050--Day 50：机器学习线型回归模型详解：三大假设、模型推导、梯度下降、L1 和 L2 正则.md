正式进入机器学习的理论和实践阶段，后面开展的基本思路：

![](https://images.gitbook.cn/1a894a00-db90-11ea-bd81-030d631eb386)

示意图如上所示，先扼要总结每个任务的基本假设，以假设为前提，逐步建立机器学习算法模型，将算法模型应用到实际问题中，输出算法结果，通过评价准则进行算法调优、迭代模型，并从中选择解决问题的最佳算法模型。

下面开始第一类问题：回归问题的求解，为了简化我们的模型，使用线性的回归模型。

### 算法模型的三个假定

为了保证使用的线性数学模型能够取得较好的拟合效果，那么有三个前提假定就非常重要。

  1. 假设真实值与预测值的误差项 $\epsilon $ 服从正态分布；
  2. 假定每个样本之间都是相互独立的；
  3. 预测的数据分布和训练时用到的数据分布是相同的（至于为什么在 Day 49 有解释）

### 建立线性回归模型

因为第 $j$ 个样本的误差项 $\epsilon^{(j)}$ 服从高斯分布，因此可得：

![](https://images.gitbook.cn/2d6c0950-db90-11ea-8d16-77936a705f77)

因为建立的是线性数学模型，因此第 $j$ 个样本根据模型预测值为：

![](https://images.gitbook.cn/378c7580-715d-11e9-88d9-19b18010fa5c)

损失函数为：

![](https://images.gitbook.cn/3f87cf20-db90-11ea-8d16-77936a705f77)

以上式子中：$x^{(j)}$、$y^{(j)}$ 分别表示第 $j$ 个样本的实际取值，$n$ 表示特征的个数。

综上可得：

![](https://images.gitbook.cn/edcd3790-715c-11e9-b0cd-95416de514a5)

至此，我们得到一个含有 $n+1$ 个特征参数的等式，$f$ 表示事件 $\epsilon^j$，也就是第 $j$ 个样本的误差项的概率密度值。

参数估计中，使用最大似然估计（Maximum Likelihood Estimation，简称为 MLE）求权重参数，接下来介绍。

### 最大似然估计求参数

最大似然估计会使得已经发生的所有事件联合概率取值最大，上面说到的第 3 个假定样本每个特征间是相互独立的，所以 $m$ 个样本误差概率密度
$f(\epsilon^{j})$ 同时都发生的概率转化为累乘积：

![](https://images.gitbook.cn/54f18770-db90-11ea-bd81-030d631eb386)

样本个数 $m$ 通常会很很大，所以相乘的结果会很小。

通常做法转化为求对数，因此又称最大对数似然估计，可得如下公式：

![](https://images.gitbook.cn/64d78180-db90-11ea-9369-b5210af59199)

结合已经得出的公式：

![](https://images.gitbook.cn/e19b44d0-715c-11e9-88d9-19b18010fa5c)

最终得到：

![](https://images.gitbook.cn/79fb0910-db90-11ea-b89a-ffa3ff92b2c1)

上式含有 $n$ 个未知权重参数，如何求解当上式取得最大值时各个参数的取值，使用梯度下降方法。

### 梯度下降求解

梯度下降法是一种经常使用的找最小值的优化算法（梯度下降的详细实施步骤大家参考 Day
47）。在这里，我们使用梯度下降法来找最小值，因此需要对上节式子取反后求最小值，故：

![enter image description
here](https://images.gitbook.cn/8edc24c0-715d-11e9-bf59-b3bc33a9bfb4)

接下来，求出 $J(\theta)$ 对权重参数 $\theta_i$ 的偏导数每次迭代时步，$\theta_i$ 的更新公式：

![enter image description
here](https://images.gitbook.cn/c3d3ddd0-715d-11e9-aa96-11d2228ed903)

其中 $\eta$ 是学习率。至此公式推导全部结束。

### 正则化项

有多少特征，就有多少参数需要学习。机器学习学习过程常见的问题之一便是过拟合，过拟合的重要表现就是训练数据集上表现会很好，因为它会试图去满足很多个性化的分布，进而失去泛化的能力，因此在训练数据集上的表现就会变槽糕。

例子解释一下，特征个数 $n=5$，假如未添加正则项时，学习到 5 个参数分别为：

$$ w_0 = 0.4, w_1 = -0.5, w_2 = 1.5, w_3 = -0.4, w_4 = 0.6$$

每个参数的绝对值权重都相差不是很大，通俗理解就是每个参数都发挥差不多的作用。但是我们想惩罚某几个参数的作用，削弱它们，增强某些参数的作用。

添加常用的 $L2$ 正则项后，也就是添加一项： $\lambda W_i^2$，假如 $\lambda=1$，惩罚后各个参数的值：

$$ w_0 = 0.16, w_1 = 0.25, w_2 = 2.25, w_3 = 0.16, w_4 = 0.36$$

实施 $L2$ 正则化后，$w_3$ 的相对权重变得更加突出，并且弱化了其他参数的权重，起到惩罚的作用。

以上就是正则化项的感性认识。

### L1 和 L2 正则化

$L1$ 和 $L2$ 正则的一个主要不同：相比 $L2$，$L1$ 正则更容易使模型变稀疏，下面通俗易懂解释为什么。

$L1$ 是对模型中每个特征参数取绝对值，$L2$ 正则对特征参数取平方

如果施加 $L1$，则新的损失函数 $f$ 为：

$$ Loss() + C|w| $$

要想消除此特征的作用，只需要令 $w=0$ 时，使 $f$ 取得极小值。因为当 $f$ 取得最小值时，必然保证参数 $w$ 变为 0。

且容易证明，添加 $L1$ 正则后，只要满足：系数 $C$ 大于原函数 $Loss()$ 在 0 点处导数的绝对值。

证明过程如下，要想在 0 点处取得极小值，根据高等数学知识得到：

  1. $w$ 小于 0 时，$\frac {\sigma (Loss)}{\sigma(w)} - C < 0$
  2. 且 $w$ 大于 0 时，$\frac {\sigma (Loss)}{\sigma(w)} + C > 0$ 

上面两个式子同时满足时，可以简化为：

$$|\frac {\sigma (Loss)}{\sigma(w)}| < C (x=0)$$

但是如果施加 $L2$ 正则，则新的函数为：$Loss()+Cw^2$，求导可得：

$$\frac {\sigma (Loss)}{\sigma(w)} + 2Cw $$

要想在 $w=0$ 点处取得极小值，必须得满足：

$$\frac {\sigma (Loss)}{\sigma(w)} = 0$$

如果原函数 $Loss()$ 在 0 点处的导数不为 0，那么施加 $L2$ 正则后偏导数不会为 0，也就不会在 0 点处取得极小值。这种概率很明显小于
$L1$ 正则在 0 点处取得极小值的概率值。

由此可得，$L1$ 更容易使得原来的特征变弱或消除，换句话说就是更容易使参数变稀疏。

### 小结

今天主要介绍线型回归模型高效使用的三个基本假定，建立线型回归模型的推导过程，最大似然估计和梯度下降求特征参数，最后介绍实际算法模型中经常要考虑的惩罚项，也就是
$L1$ 和 $L2$ 正则化项，以及两个正则项的主要不同点及原理证明。

