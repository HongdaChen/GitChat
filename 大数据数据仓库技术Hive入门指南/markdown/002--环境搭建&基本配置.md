## 环境规划

### 操作系统及组件版本

各组件版本如下，学习环境尽量保持一致，避免版本不一致带来的操作问题。

**组件** | **_**_CentOS_**_** | **_**_Hadoop_**_** | **_**_Hive_**_** |
**_**_Tez_**_** | **_**_Mysql_**_**  
---|---|---|---|---|---  
**版本** | 7.2 | 2.7.7 | 2.3.7 | 0.9.1 | 5.7.28  
  
### 集群规划

使用3台虚拟机来进行搭建集群，分别为Node01、Node02、Node03。集群的规划如下：

**** | **_**_Hadoop_**_** | **_**_Hive &Tez_**_** | **_**_Mysql_**_**  
---|---|---|---  
**node01** | √ |  |  
**node02** | √ |  | √  
**node03** | √ | √ |  
  
其中Hadoop一共3个节点，主节点搭建在Node01上，从节点在Node01-Node03上分别有一个。Hive搭建在node03上，底层使用Tez引擎进行优化；Mysql作为MetaStore，安装在Node02上。

## 虚拟机准备

### 安装说明&文件下载

下载并安装Virtual Box：https://www.virtualbox.org/wiki/Downloads

准备并安装3台CentOS7.2的虚拟机，主机名命名为node01、node02、node03。

虚拟机的安装可以使用纯系统镜像，安装后配置主机名。但过程会比较繁琐，学习环境讲求开箱即用，尽量少的在环境上花费时间，否则会打击学习的热情。所以，也可以直接导入已经配置好的虚拟机镜像文件，方便使用。

使用纯镜像安装，下附CentOS镜像下载地址：

    
    
    链接：https://pan.baidu.com/s/1CV0C7bZ0-7tKziNf6RmdVg
    提取码：h931
    

推荐直接导入虚拟机镜像文件，下附虚拟机镜像下载地址：

    
    
    链接：https://pan.baidu.com/s/1T1VhTv6EwGAr2odlAUPBtA
    提取码：pljf
    

### 虚拟机镜像文件导入流程

  1. 下载虚拟机镜像文件

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/a79387970a4279adf46969393e8bc7ce.png#pic_center)

  1. 打开Virtual Box，选择导入虚拟电脑

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/2b92c638d9a57832a769fbd4ac6f903a.png#pic_center)

  1. 选择文件位置，进行导入

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/d6d961a9371c31b52e9cba731d9d9a24.png#pic_center)

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/6bafa5aa90d5127365f8af60621a715c.png#pic_center)

  1. 配置虚拟机，自定义将虚拟机文件存放到指定目录，然后点击确定，完成导入

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/387d432a01fd725c14a94f2d4b6dfc35.png#pic_center)

  1. 依次导入Node01、Node02、Node03

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/4c09468eb26b3dbf0d6593eea9136226.png#pic_center)

  1. 开启虚拟机，使用root/123456进行登录

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/69a3aa9b2087142fec22f196db17abc7.png#pic_center)

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/92bc91571657035b7896899370969218.png#pic_center)

  1. 修改虚拟机IP地址

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/c4033b4d95865430d0c96d20c03c5b2a.png#pic_center)

    
    
    vim /etc/sysconfig/network-scripts/ifcfg-enp0s3
    

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/e0f2122e5ffbfe8c823a0316580ab3cf.png#pic_center)

  1. 使用XShell，或者其它远程SSH Linux登录工具进行远程连接虚拟机

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/8ac0d750b469aab69047d8455c951f5f.png#pic_center)

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/28d0e849408f2fab0fca95679b5f64ec.png#pic_center)

## 自动化安装脚本准备

  1. 下载并上传自动化安装脚本[automaticDeploy.zip](https://github.com/MTlpc/automaticDeploy/archive/master.zip)到虚拟机node01中。

    
    
    wget https://github.com/MTlpc/automaticDeploy/archive/hive.zip
    

  1. 解压automaticDeploy.zip到/home/hadoop/目录下

    
    
    mkdir /home/hadoop/
    unzip hive.zip -d /home/hadoop/
    mv /home/hadoop/automaticDeploy-hive /home/hadoop/automaticDeploy
    

  1. 更改自动化安装脚本的frames.txt文件，配置组件的安装节点信息（如无特殊要求，默认即可）

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/eeb45ff227dee99d6e6a5ec2708b0b89.png#pic_center)

  1. 编辑自动化安装脚本的configs.txt文件，配置Mysql、Keystore密码信息（如无特殊要求，默认即可，末尾加END表示结束）

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/5679f859fbdf1bdfefa50d29edd0b871.png#pic_center)

  1. 编辑host_ip.txt文件，将3台虚拟机节点信息添加进去（需自定义进行修改）

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/8a532b4e8b3f0fd30b065d97201c3e24.png#pic_center)

  1. 对/home/hadoop/automaticDeploy/下的hadoop、systems所有脚本添加执行权限

    
    
    chmod +x /home/hadoop/automaticDeploy/hadoop/* /home/hadoop/automaticDeploy/systems/*
    

## 大数据环境一键安装

  1. 下载frames.zip包，里面包含大数据组件的安装包，并上传到node01中

    
    
    链接：https://pan.baidu.com/s/1XxuouuogndEc6vIIiUhyTw
    提取码：i4ki
    复制这段内容后打开百度网盘手机App，操作更方便哦
    

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/7b26315edc11b824e17a240ada70c358.png#pic_center)

  1. 将frames.zip压缩包，解压到/home/hadoop/automaticDeploy目录下

    
    
    unzip frames.zip -d /home/hadoop/automaticDeploy/
    

  1. 进行集群环境配置，并分发安装脚本和文件

    
    
    /home/hadoop/automaticDeploy/systems/clusterOperate.sh
    

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/936d324849b666c3363ab3abdc22b3b6.png#pic_center)

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/0ee455412d3119ebbe1687d622064583.png#pic_center)

  * 为了避免脚本中与各个节点的ssh因为环境问题，执行失败，需要手动测试下与其它节点的ssh情况，如果失败，则手动添加
  * 失败后重新添加SSH

    
    
    ssh-copy-id node02
    

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/c52edcf5cb600bc1dfce04f564d559da.png#pic_center)

  1. 在各个节点执行脚本，安装Hadoop集群

    
    
    /home/hadoop/automaticDeploy/hadoop/installHadoop.sh
    source /etc/profile
    # 在Node01节点执行，初始化NameNode
    hadoop namenode -format
    # 在Node01节点执行，启动Hadoop集群
    start-all.sh
    

  1. 使用本地浏览器访问node01:50070，成功则说明Hadoop搭建成功

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/197eb14dfc2832a56e34defde4c7e87d.png#pic_center)

  1. 在Node02节点使用脚本安装Mysql

    
    
    /home/hadoop/automaticDeploy/hadoop/installMysql.sh
    

  1. 在Node03节点安装Hive

    
    
    /home/hadoop/automaticDeploy/hadoop/installHive.sh
    source /etc/profile
    

  1. 在Node03节点启动Hive

    
    
    # 启动HiveServer2
    hive --service hiveserver2 &
    # 启动Metastore
    hive --service metastore &
    

  1. 使用beeline访问hive，beeline可以直接Ctrl+C退出

    
    
    beeline -u jdbc:hive2://node03:10000 -n root
    

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/e210f178f7e16805281cb0a0beac66b5.png#pic_center)

  1. 试着创建一张表

    
    
    create table test(id string, name string);
    

  1. 如果报错：Column length too big for column 'PARAM_VALUE' (max = 21845); use BLOB or TEXT instead。是因为MySQL中的metastore表编码问题，使用schematool工具恢复一下即可，或手动将hive数据库编码更改为latin1。

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/742afa7b8e6c970e6ee18e806866b2eb.png#pic_center)

    
    
    # 更改hive数据库编码:在Node02节点(Mysql安装节点)执行（2选1）
    mysql -uroot -pDBa2020* -e "alter database hive character set latin1;"
    # 或者在hive节点执行恢复脚本（2选1）
    schematool -dbType mysql -initSchema
    # kill掉hiveserver2和metastore进行，重新启动
    jps | grep RunJar | awk '{print $1}' | xargs kill -9
    hive --service hiveserver2 &
    hive --service metastore &
    

## 基本配置

虽然已经使用脚本，快速完成了集群的搭建，但关于Hive的一些基本配置，仍然需要学习和掌握。

#### 配置文件

Hive的配置文件有两个：hive-env.sh、hive-
site.xml，在hive安装目录下的conf目录中可以找到。使用脚本搭建的hive集群，安装目录为：/opt/app/apache-
hive-1.2.1-bin。

其中hive-site.xml是Hive的全局配置文件，可以进行一些基础、调优的配置。

hive-
env.sh设置了Hive集群运行所需的环境变量，因为Hive是Java语言开发，运行在JVM之上，并且依赖Hadoop集群，所以这里主要进行Java环境配置、Hadoop环境配置、JVM调优。

#### hive-site.xml配置

hive-site.xml文件使用xml标签进行配置。其中标签中包含多个配置项；每个标签中，使用和标签来设置配置的名称与所对应的值。

因为这里使用Mysql作为MetaStore，所以需要在这里配置Mysql相关的信息。配置项包含Mysql连接URL、JDBC
Driver、UserName、Password。

    
    
    <configuration>
    <property>
    <name>javax.jdo.option.ConnectionURL</name>
    <value>jdbc:mysql://node02:3306/hive?createDatabaseIfNotExist=true&amp;useSSL=false</value>
    <description>JDBC connect string for a JDBC metastore</description>
    </property>
    
    <property>
    <name>javax.jdo.option.ConnectionDriverName</name>
    <value>com.mysql.jdbc.Driver</value>
    <description>Driver class name for a JDBC metastore</description>
    </property>
    
    <property>
    <name>javax.jdo.option.ConnectionUserName</name>
    <value>hive</value>
    <description>username to use against metastore database</description>
    </property>
    
    <property>
    <name>javax.jdo.option.ConnectionPassword</name>
    <value>mysqlHivePasswd</value>
    <description>password to use against metastore database</description>
    </property>
    </configuration>
    

配置完成后，需要将JDBC驱动放置到Hive安装目录下的lib目录中，这里脚本已经自动完成。

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/c238695021b8cc47cc0b950ed5aecfb1.png#pic_center)

除此之外，要启动ThriftServer，需要配置thrift服务的host、port，然后设置Metastore的URL。

    
    
    <property>
    <name>hive.server2.thrift.port</name>
    <value>10000</value>
    </property>
    <property>
    <name>hive.server2.thrift.bind.host</name>
    <value>node03</value>
    </property>
    <property>
    <name>hive.metastore.uris</name>
    <value>thrift://node03:9083</value>
    </property>
    

#### hive-env.sh配置

在hive-env.sh中需要配置JAVA _HOME和HADOOP_ HOME，并且设置HIVE _CONF_ DIR。

    
    
    export JAVA_HOME=/usr/lib/java/jdk1.8.0_144
    export HADOOP_HOME=/opt/app/hadoop-2.7.7
    export HIVE_CONF_DIR=/opt/app/apache-hive-1.2.1-bin/conf
    

