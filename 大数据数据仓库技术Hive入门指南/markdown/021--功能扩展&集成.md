## HPL/SQL

### 基本概述

HPL/SQL是一个帮助Hive、Spark SQL、Impala、其它SQL-On-Hadoop实现PL/SQL支持的开源工具。

它兼容Oracle PL/SQL, ANSI/ISO标准的存储过程(IBM DB2, MySQL, Teradata等), PostgreSQL
PL/pgSQL (Netezza), Transact-SQL (Microsoft SQL Server、Sybase)
语法。使现有的SQL业务，可以更好的迁移到Hadoop中。

### 环境搭建

  1. 下载并上传hplsql安装包，这里使用的是hplsql-0.3.31.tar.gz。
  2. 解压到/opt/app目录中。

    
    
    tar -zxvf hplsql-0.3.31.tar.gz -C /opt/app/
    

  1. 编辑hplsql目录中的hplsql文件，添加hadoop集群的环境变量。

    
    
    vim /opt/app/hplsql-0.3.31/hplsql
    # 环境变量信息
    export "HADOOP_CLASSPATH=$HADOOP_CLASSPATH:/opt/app/hadoop-2.7.7*"
    export "HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HADOOP_HOME/lib/*"
    export "HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HADOOP_HOME/conf"
    
    export "HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HADOOP_HOME/share/hadoop/mapreduce/*"
    export "HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HADOOP_HOME/share/hadoop/mapreduce/lib/*"
    
    export "HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HADOOP_HOME/share/hadoop/hdfs/*"
    export "HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HADOOP_HOME/share/hadoop/hdfs/lib/*"
    
    export "HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HADOOP_HOME/share/hadoop/yarn/*"
    export "HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HADOOP_HOME/share/hadoop/yarn/lib/*"
    
    export "HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HADOOP_HOME/share/hadoop/common/*"
    export "HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HADOOP_HOME/share/hadoop/common/lib/*"
    
    export "HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HIVE_HOME/lib/*"
    export "HADOOP_CLASSPATH=$HADOOP_CLASSPATH:$HIVE_HOME/conf"
    
    export HADOOP_OPTS="$HADOOP_OPTS -Djava.library.path=$HADOOP_HOME/lib/native"
    

  1. 修改hplsql-site.xml，添加beeline信息连接hive。

    
    
    <property>
    <name>hplsql.conn.hiveconn</name>
    <value>org.apache.hive.jdbc.HiveDriver;jdbc:hive2://node03:10000</value>
    <description>HiveServer2 JDBC connection (embedded mode)</description>
    </property>
    

  1. 将hplsql添加到环境变量中。

    
    
    vim /etc/profile
    # 添加如下信息
    export HPLSQL_HOME=/opt/app/hplsql-0.3.31
    export PATH=$PATH:$HPLSQL_HOME
    # 初始化环境变量
    source /etc/profile
    

  1. 测试是否执行成功。

    
    
    hplsql -e "FOR i IN 1 .. 10 LOOP PRINT i; END LOOP;"
    hplsql -e "CURRENT_DATE + 1"
    

  1. 添加脚本test.sql，添加存储过程hello，为传入的字符串增加Hello前缀。

    
    
    CREATE FUNCTION hello(text STRING)
    RETURNS STRING
    BEGIN
    RETURN 'Hello, ' || text || '!';
    END;
    
    FOR item IN (
    SELECT id,name FROM employee limit 10
    )
    LOOP
    PRINT item.id || '|' || item.name || '|' || hello(item.name);
    END LOOP;
    

  1. 执行脚本。

    
    
    hplsql -f test.sql
    

  1. 在shell脚本中，可以获取HPL/SQL执行结果。

    
    
    START=$(hplsql -e 'CURRENT_DATE - 1')
    

## HBase

### 基本概述

Hive支持与Hbase进行整合，使用HQL语句对Hbase表进行查询、插入、Join、Union等操作。在0.6.0版本中引入，主要依靠hive-
hbase-handler-x.x.x.jar工具类实现。

### 环境准备

需要提前安装Zookeeper，并分布式部署HBase。这里直接使用脚本来完成。

首先进入脚本目录，编辑frames.txt配置文件，添加zookeeper和hbase的安装信息。

    
    
    cd /home/hadoop/automaticDeploy/
    vim frames.txt
    # 添加如下信息
    hbase-2.0.0-bin.tar.gz true node01,node02,node03 node02
    zookeeper-3.4.10.tar.gz true node01,node02,node03
    

然后将hbase-2.0.0-bin.tar.gz、zookeeper-3.4.10.tar.gz安装包上传到/opt/frames目录下。这里也可以安装其它版本，自行更改配置即可。

调用脚本进行一键安装。

    
    
    hadoop/installZookeeper.sh
    hadoop/installHBase.sh
    

然后初始化环境变量即可。

    
    
    source /etc/profile
    

### 环境配置

Hive对于单节点部署的HBase、分布式的HBase有不同的配置。

如果部署单台Hbase节点，启动Hive客户端时使用以下命令，使得Hive具有操作Hbase的功能。

    
    
    hive --auxpath $HIVE_HOME/lib/hive-hbase-handler-2.3.7.jar,$HIVE_HOME/lib/hbase-client-2.0.0.jar,$HIVE_HOME/lib/zookeeper-3.4.10.jar,$HIVE_HOME/lib/guava-14.0.1.jar --hiveconf hbase.master=node02:60000
    

部署分布式Hbase，则在启动Hive客户端时使用以下命令。

    
    
    hive --auxpath $HIVE_HOME/lib/hive-hbase-handler-2.3.7.jar,$HIVE_HOME/lib/hbase-client-2.0.0.jar,$HIVE_HOME/lib/zookeeper-3.4.10.jar,$HIVE_HOME/lib/guava-14.0.1.jar --hiveconf hbase.zookeeper.quorum=node01,node02,node03
    

主要的不同是单节点的HBase，使用hbase.master指定主节点地址，而分布式HBase则是使用hbase.zookeeper.quorum指定Zookeeper地址进行主节点的动态发现。

为了方便起见，可以直接配置持久化到hive-site.xml中。

    
    
    <property>
    <name>hive.aux.jars.path</name>
    <value>
    $HIVE_HOME/lib/hive-hbase-handler-2.3.7.jar,
    $HIVE_HOME/lib/hbase-client-2.0.0.jar,
    $HIVE_HOME/lib/zookeeper-3.4.10.jar,
    $HIVE_HOME/lib/guava-14.0.1.jar
    </value>
    </property>
    <property>
    <name>hive.zookeeper.quorum</name>
    <value>node01,node02,node03</value>
    </property>
    <property>
    <name>hive.server2.enable.doAs</name>
    <value>false</value>
    </property>
    

配置后，可使用beeline登录。

    
    
    beeline -u jdbc:hive2://node03:10000 -n root
    

### HBase表创建

使用HQL可以直接在Hive中完成对Hbase内表的创建，Hive保存映射信息，Hbase存储数据。

    
    
    CREATE TABLE hbase_inner(key int, value string)
    STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
    WITH SERDEPROPERTIES ("hbase.columns.mapping" = ":key,cf1:val")
    TBLPROPERTIES ("hbase.table.name" = "hive_inner", "hbase.mapred.output.outputtable" = "hive_inner");
    

使用STORED BY
指定存储为Hbase表类型。SERDEPROPERTIES中的hbase.columns.mapping属性，可以自定义生成的hbase表的列族与列名。TBLPROPERTIES中hbase.table.name属性指定hbase表名，
hbase.mapred.output.outputtable属性则表示支持对hbase表进行读写操作。

使用HQL创建HBase表之后，在Hive的MetaStore中，保留一张映射表hbase
_inner，拥有key(int)、value(string)两个字段。而在HBase中，会创建真实的数据表hive_
inner，有rowkey(byte[] [])两个字段。

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/b413f18f807a009288a2bc529f4ea75e.png#pic_center)

这样，在Hive中，对表hbase _inner的操作，都会映射到HBase中的hive_ inner表中。

可以在hive和hbase中分别查看表的情况。

    
    
    # 使用beeline连接hive
    beeline -u jdbc:hive2://node03:10000 -n root
    # 查看表情况
    desc hbase_inner;
    # 使用hbase shell连接hbase
    hbase shell
    # 查看hbase表
    desc hive_inner
    

对于Hbase中已经存在的表，可以为其创建外表，建立映射关系。

    
    
    CREATE EXTERNAL TABLE hbase_ext(key int, value string)
    STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
    WITH SERDEPROPERTIES ("hbase.columns.mapping" = "cf1:val")
    TBLPROPERTIES("hbase.table.name" = "hive_inner", "hbase.mapred.output.outputtable" = "hive_inner ");
    

表创建后，便可以在Hive中，对Hbase内表进行SQL操作。

    
    
    INSERT INTO TABLE hbase_inner values(1, 'value1');
    INSERT INTO TABLE hbase_inner values(2, 'value2');
    SELECT * FROM hbase_inner;
    

### 复杂数据类型映射

#### Map类型

Hive表中使用Map类型时，HBase表会将Map中的key作为列名，value作为值。

    
    
    CREATE TABLE hbase_map(value map<string,int>, row_key int)
    STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
    WITH SERDEPROPERTIES (
    "hbase.columns.mapping" = "cf:,:key"
    );
    insert into hbase_map select map(value,key), key from hbase_inner;
    

可以使用map构造方法，对查询得到的数据构造为map类型并插入到hive表中。这时，hive映射表和hbase数据表显示的内容如下。

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/539de6be566bda0182083768b970b7f7.png#pic_center)

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/9f360689a646cb85c27a63757a202336.png#pic_center)

可以看到，HBase表将Map中的Key作为了列名，Value则是它的值。

#### Struct类型

Hive表中使用Struct类型时，HBase表会将Struct中的field合并为一个字符串作为value。

    
    
    CREATE EXTERNAL TABLE hbase_struct(key struct<f1:string, f2:string>, value string)
    ROW FORMAT DELIMITED
    COLLECTION ITEMS TERMINATED BY '~'
    STORED BY 'org.apache.hadoop.hive.hbase.HBaseStorageHandler'
    WITH SERDEPROPERTIES (
    'hbase.columns.mapping'=':key,f:c1');
    
    insert into hbase_struct select named_struct('f1',cast(key as string),'f2',value), value from hbase_inner;
    

这里使用named_struct构造方法，将查询出的数据构造为struct类型。这时，hive映射表和hbase数据表显示的内容如下。

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/d89c3ed17dad0199fc7778ffd5f86449.png#pic_center)

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/7e116883c81b668538aa3397516b33d5.png#pic_center)

可以看到，struct中的所有value值使用~进行连接，合并成一个字符串存储到了HBase中。

