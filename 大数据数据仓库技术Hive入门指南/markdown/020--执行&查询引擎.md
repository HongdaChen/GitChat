## Hive On Spark

### 基本概述

Hive支持使用Spark作为底层执行引擎，以获得比MapReduce更快的处理性能。

    
    
    set hive.execution.engine=spark;
    

但要注意的是，Hive与Spark整合时，只有特定的Spark版本做过兼容度测试。

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/de393bd44b62e8ec5c49b6c224d4b935.png#pic_center)

所以，搭建此模式时，对于Spark集群版本有一定的要求。而且默认虚拟机内存为1G，运行核数为1Core，需要关闭虚拟机，并将内存提升为2G，运行核数升为2Core，环境才能搭建成功。

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/b331781e2caf2ca38855abeec69d7740.png#pic_center)

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/abaa4cd22853ad3d00218d9bd6c7c94d.png#pic_center)

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/614ae92c36b14c862c80f272b410d019.png#pic_center)

### 搭建准备

需要下载Hive源码，在其根目录下的pom.xml中可以找到对应Spark的版本信息。找到对应的Spark版本之后，便可以自行搭建。

但如果需要搭建其它的Spark版本，则需要对Spark进行编译安装。因为已编译好的Spark为兼容Spark SQL，已经添加了Hive
Jars，所以需要重新编译，得到不包含Hive Jars的Spark集群，并添加对于parquet表的支持。

    
    
    --Spark 2.0.0之前
    ./make-distribution.sh --name "hadoop2-without-hive" --tgz "-Pyarn,hadoop-provided,hadoop-2.4,parquet-provided“
    --Spark 2.0.0之后
    ./dev/make-distribution.sh --name "hadoop2-without-hive" --tgz "-Pyarn,hadoop-provided,hadoop-2.7,parquet-provided“
    --Spark 2.3.0之后
    ./dev/make-distribution.sh --name "hadoop2-without-hive" --tgz "-Pyarn,hadoop-provided,hadoop-2.7,parquet-provided,orc-provided"
    

或者可以直接安装不包含hadoop依赖的已编译版本spark-x.x.x-bin-without-hadoop.tgz。

使用编译方式，或者直接安装不包含hadoop依赖的Spark集群后，需要编辑spark-env.sh，添加当前Hadoop的依赖信息。

    
    
    # 编辑spark-env.sh
    HADOOP_CONF_DIR=/opt/app/hadoop-2.7.7/etc/hadoop
    source /etc/profile
    export SPARK_DIST_CLASSPATH=$(hadoop classpath)
    

### 环境搭建

首先，需要为hive添加Spark依赖。

    
    
    --Spark 2.0.0之前
    cp {path to spark-assembly jar} $HIVE_HOME/lib
    --Spark 2.0.0之后，Yarn模式
    cp scala-library-2.x.x.jar $HIVE_HOME/lib
    cp spark-core_2.x-2.x.x.jar $HIVE_HOME/lib
    cp spark-network-common_2.x-2.x.x.jar $HIVE_HOME/lib
    --Spark 2.0.0之后，Local模式
    cp chill-java  chill  jackson-module-paranamer  jackson-module-scala  jersey-container-servlet-core HIVE_HOME/lib
    cp jersey-server  json4s-ast  kryo-shaded  minlog  scala-xml  spark-launcher HIVE_HOME/lib
    cp spark-network-shuffle  spark-unsafe  xbean-asm5-shaded HIVE_HOME/lib
    

然后将hive的执行引擎更换为spark。

    
    
    # 编辑hive-site.xml
    <property>
    <name>hive.execution.engine</name>
    <value>spark</value>
    </property>
    

为hive配置Spark参数，添加到 hive-site.xml 或 spark-defaults.conf 中。

    
    
    --Spark集群URL，eg. spark://node01:7077
    set spark.master=<Spark Master URL>
    --Spark事件日志
    set spark.eventLog.enabled=true;
    --Spark日志存放地址，eg. hdfs://node01:9000/spark/logs
    set spark.eventLog.dir=<Spark event log folder (must exist)>
    --Spark每个Executor的内存
    set spark.executor.memory=512m;
    --Spark序列化器
    set spark.serializer=org.apache.spark.serializer.KryoSerializer;
    

对hive进行配置，以允许Yarn提前缓存Spark依赖的Jars，以便减少每次程序运行时分发的时间。

    
    
    --上传jars到HDFS中
    hadoop fs -mkdir /spark-jars
    hadoop fs -put $SPARK_HOME/jars/* /spark-jars/
    --配置hive-site.xml
    <property>
    <name>spark.yarn.jars</name>
    <value>hdfs://node01:9000/spark-jars/*</value>
    </property>
    

将hive分发到所有节点，因为spark任务被调度到某个节点运行时，会在当前节点寻找所依赖的hive包，否则会报错退出。

    
    
    --分发hive
    scp -r $HIVE_HOME root@host:/opt/app/
    

重新启动Hive。

    
    
    hive --service hiveserver2 --hiveconf hive.exec.scratchdir=/tmp/mydir &
    hive --service metastore &
    

测试是否成功：

    
    
    beeline -u jdbc:hive2://node03:10000 -n root
    create table s1(id int, name string);
    insert into s1 values(1, 'tb1');
    

## Hive On Tez

### 基本概述

Hive支持底层运算引擎更改为Tez，以支持更复杂的DAG运算，提供更优秀的性能。Tez可以将多个Job整合为一个Job进行运算，从而支持复杂DAG作业。

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/4c17470df39331bad6da86711489c23e.png#pic_center)

### 环境搭建

Tez安装，已经封装到自动化搭建脚本中，在frames.txt中指定tez的安装包和hive所在的节点即可。并将tez安装包放置在/opt/frames目录下。

    
    
    apache-tez-0.9.1-bin.tar.gz true node03
    

然后执行installTez.sh进行tez的安装。

如果在安装hive前，已经在frames.txt中指定了tez安装信息，则调用installHive.sh安装hive时，会自动对tez进行安装和配置，不需要再执行installTez.sh脚本。

默认情况下，脚本安装hive后，自动会运行在tez模式下。

### 测试运行

首先，将执行引擎切换为tez。

    
    
    set hive.execution.engine=tez;
    

然后，创建表，并插入数据，看是否执行成功。

    
    
    create table tez_test (id int ,name string);
    insert into tez_test values(1, 'zs');
    

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/e68d9af5467ca08d1f5793e808b965a3.png#pic_center)

## Hive On LLAP

### 基本概述

Hive在2.0之后，推出一个新特性LLAP（Live Long And Process），可以显著提高查询效率。

LLAP是一个常驻于Yarn的进程，并不是一个执行引擎，它将DataNode数据预先缓存到内存中，然后交由DAG引擎进行查询、处理任务使用。部分查询、权限控制将由LLAP执行，短查询任务的结果会很快的返回。

相对于Hive 1.x，提升大约25倍的性能。

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/aa27937a0d9b8731bae49f690913c3cb.png#pic_center)

### 安装准备

#### Apache Slider安装

LLAP需要常驻于Yarn之上，所以需要提前安装Apache Slider。当然，在Hadoop
3.x中，Yarn已经支持常驻任务的运行，就不需要对Apache Slider进行安装了。

首先下载Slider，如果网速较慢，可以先从Github中下载zip包，再上传到服务器进行解压。

    
    
    git clone https://github.com/apache/incubator-retired-slider/tree/branches/branch-0.90.2
    

如果下载了专栏配套的frames.zip包，并使用脚本安装集群，在/opt/frames/下可以找到slider安装包，然后对安装包进行解压。

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/1f3b128aac43384bb6544eb455f14822.png#pic_center)

因为编译过程需要maven，所以使用脚本对maven进行安装。在frames.zip包中已经有maven的安装包，也可以自行下载其它版本的maven包。

    
    
    # 进入自动化脚本安装目录
    cd /home/hadoop/automaticDeploy/
    # 编辑frames.txt，添加maven配置
    apache-maven-3.6.3-bin.tar.gz true
    # 使用脚本安装maven
    ./systems/configureMaven.sh
    # 初始化环境变量
    source /etc/profile
    # 查看是否安装成功
    mvn --version
    

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/506030b0a356c46c23d2d4a3a24a1165.png#pic_center)

进入到Slider源码目录下，进行编译。

    
    
    # 进入slider源码目录
    cd apache-slider-0.90.2-incubating/
    # 进行编译
    mvn clean site:site site:stage package -DskipTests
    

解压编译后的文件。

    
    
    mkdir /opt/app/slider/
    tar -zxvf slider-assembly/target/slider-0.90.2-incubating-all.tar.gz -C /opt/app/slider/
    

编辑conf/slider-env，添加环境变量。

    
    
    # 进入slider安装目录
    cd /opt/app/slider/slider-0.90.2-incubating
    # 编辑conf/slider-env，添加以下内容
    export JAVA_HOME=${JAVA_HOME}
    export HADOOP_CONF_DIR=${HADOOP_HOME}/etc/hadoop/
    

编辑conf/slider-
client.xml，指定Zookeeper地址，Zookeeper稍后使用脚本进行安装，这里提前规定它安装到node01、node02、node03节点。

    
    
    <property>
    <name>hadoop.registry.zk.quorum</name>
    <value>node01:2181,node02:2181,node03:2181</value>
    </property>
    

将Slider添加到PATH环境变量中。

    
    
    vim /etc/profile
    # 配置环境变量
    export SLIDER_HOME=/opt/app/slider/slider-0.90.2-incubating
    export PATH=$PATH:$SLIDER_HOME/bin
    
    # 配置完成后，初始化环境变量
    source /etc/profile
    

查看是否配置成功。

    
    
    slider version
    

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/8a16bb168bc4603edf62c34d717a7a8a.png#pic_center)

#### Zookeeper安装

因为Slider依赖Zookeeper，所以需要在各个节点提前进行安装。这里直接使用脚本来完成。

首先进入脚本目录，分别在node01、node02、node03中编辑frames.txt配置文件，添加zookeeper的安装信息。

    
    
    cd /home/hadoop/automaticDeploy/
    vim frames.txt
    # 添加如下信息
    zookeeper-3.4.10.tar.gz true node01,node02,node03
    

然后将zookeeper-3.4.10.tar.gz安装包上传到各节点的/opt/frames目录下，如果使用frames.zip包，则安装包已经在对应目录中。这里也可以安装其它版本，自行更改配置即可。

在node01、node02、node03上调用脚本进行一键安装。

    
    
    /home/hadoop/automaticDeploy/hadoop/installZookeeper.sh
    

然后初始化环境变量即可。

    
    
    source /etc/profile
    

因为启动时需要在各个节点执行启动命令，为了方便起见，使用脚本安装Zookeeper后，启动命令已经被封装为脚本zookeeper.sh，在任意一台节点执行便可以一键完成Zookeeper集群的启停：

    
    
    # 在任意一台节点执行即可
    zookeeper.sh start
    

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/e4185b88b2597ab9e10ea0509530568e.png#pic_center)

### 环境搭建

首先编辑hive-
site.xml，配置LLAP，这里hive.llap.daemon.service.hosts配置为运行在yarn上的LLAP服务名，这里可以自定义设置，但要与下一步中使用hive命令生成的LLAP环境包中的服务名一致。

    
    
    <property>
    <name>hive.zookeeper.quorum</name>
    <value>node01:2181,node02:2181,node03:2181</value>
    </property>
    
    <property>
    <name>hive.execution.mode</name>
    <value>llap</value>
    </property>
    <property>
    <name>hive.llap.execution.mode</name>
    <value>all</value>
    </property>
    <property>
    <name>hive.llap.daemon.service.hosts</name>
    <value>@llap_server</value>
    </property>
    

然后，使用Hive命令生成LLAP环境包，它包含LLAP运行所需依赖、LLAP启动脚本。其中--name参数指定的便是LLAP服务名。

    
    
    # 使用命令生成的llap环境包会存放在当前目录中
    # 所以提前进入/opt/app目录，避免llap环境包生成到其它目录
    cd /opt/app
    
    # instances指定节点数量；executors指定每个节点执行器数量
    # iothread指定实例的IO线程数
    # size指定container内存大小
    # xmx指定container JVM堆大小
    hive --service llap --name llap_server \
    --instances 2\
    --cache 512m\
    --executors 2\
    --iothreads 2\
    --size 1024m\
    --xmx 512m\
    --loglevel INFO\
    --args "-XX:+UseG1GC -XX:+ResizeTLAB -XX:+UseNUMA -XX:-ResizePLAB -XX:MaxGCPauseMillis=200"\
    --javaHome $JAVA_HOME
    

在运行前，需要在各个节点确认关闭python的ssl功能，避免因为在集群中进行https校验，而导致llap从节点无法向主节点汇报心跳而关闭。

    
    
    vim /etc/python/cert-verification.cfg
    # 确保以下参数为disable
    [https]
    verify=disable
    

然后，因为llap使用的执行引擎为tez，而当前安装的tez-0.9.1版本自带的hadoop依赖为2.7.0，集群hadoop版本为2.7.7，所以在执行SQL时会抛出异常：

    
    
    org.apache.hadoop.tracing.SpanReceiverHost.getInstance(Lorg/apache/hadoop/conf/Configuration;)Lorg/apache/hadoop/tracing/SpanReceiverHost;
    

这是因为hadoop
2.7.3中更改了SpanReceiverHost.getInstance方法而导致的，所以，需要替换掉llap中的tez依赖（如果使用更高版本的tez，则不需要进行替换）：

    
    
    # 进入生成的llap目录中，这里以llap-slider-20Jan2021为例
    cd llap-slider-20Jan2021/
    # 解压llap依赖包
    unzip llap-20Jan2021.zip -d unzipped
    cd unzipped/package/files
    tar -zxvf llap-20Jan2021.tar.gz
    # 替换Hadoop Jar包
    cd lib/tez
    
    cp ../hadoop-common-2.7.7.jar ./
    cp /opt/app/hadoop-2.7.7/share/hadoop/common/lib/hadoop-annotations-2.7.7.jar ./
    cp /opt/app/hadoop-2.7.7/share/hadoop/common/lib/hadoop-auth-2.7.7.jar ./
    
    cp /opt/app/hadoop-2.7.7/share/hadoop/hdfs/hadoop-hdfs-2.7.7.jar ./
    cp /opt/app/hadoop-2.7.7/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.7.jar ./
    cp /opt/app/hadoop-2.7.7/share/hadoop/yarn/hadoop-yarn-api-2.7.7.jar ./
    
    rm -rf hadoop-common-2.7.0.jar hadoop-annotations-2.7.0.jar hadoop-auth-2.7.0.jar hadoop-hdfs-2.7.0.jar hadoop-mapreduce-client-common-2.7.0.jar hadoop-yarn-api-2.7.0.jar
    
    # 返回到llap-slider-20Jan2021/unzipped/package/files目录
    cd ../../
    # 删除旧的压缩包
    rm -rf llap-20Jan2021.tar.gz
    # 重新打包
    tar -zcvf llap-20Jan2021.tar.gz *
    # 删除旧的文件
    rm -rf bin conf config.json lib
    
    # 返回到llap-slider-20Jan2021/unzipped目录
    cd ../../
    # 将unzipped文件重新打包
    zip -r llap-20Jan2021.zip *
    # 拷贝到原目录中覆盖旧的zip包
    rm -rf ../llap-20Jan2021.zip
    cp llap-20Jan2021.zip ../
    # 回到llap-slider-20Jan2021目录，删除unzipped目录
    rm -rf unzipped/
    

最后，进入到生成的llap目录中，执行run.sh，执行前保证zookeeper已经启动。

    
    
    # 这里，生成的llap目录为llap-slider-20Jan2021
    cd llap-slider-20Jan2021/
    ./run.sh
    

llap运行后，可以在yarn监控节点看到运行的task进程。

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/d5e2504c601f52c08e5cff2a747ef0fa.png#pic_center)

在各个节点上使用 jps 命令进行查看，可以找到SliderApplicationMaster进程，还有两个LlapDaemon守护进程，说明已经成功运行。

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/6b14da7e02930c207bea0600aaba01d6.png#pic_center)

可以使用命令停止llap。

    
    
    slider stop llap_server
    

也可以使用hive命令查看llap状态。

    
    
    hive --service llapstatus --name llap_server
    

最后，创建表，并插入数据，看是否执行成功。

    
    
    create table llap_test (id int ,name string);
    insert into llap_test values(1, 'zs');
    

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/84ff2781030d57bd2d3a855b0e0ba063.png#pic_center)

