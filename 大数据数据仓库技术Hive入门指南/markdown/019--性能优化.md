## 作业调优

### 严格模式

Hive在执行SQL命令时，可以设置严格模式，防止用户执行一些对性能影响很大的查询。

    
    
    set hive.mapred.mode=strict;
    

在Hive 2.x之前默认为nonstrict，之后默认是strict。

严格模式禁止对分区表进行全表扫描，查询时在where语句中必须包含分区字段限制范围。对使用了Order
By的查询，必须使用limit语句限制数据量，防止单任务运行时间过长。

严格模式下，会限制笛卡尔积的查询。

在企业，可以进行灵活设置，但严格模式的开启，可能会导致某些SQL语句无法执行。

### 基本属性

为作业 **配置作业名** ，便于查找：

    
    
    set mapred.job.name=my_job_{DATE};
    

对于紧急作业，可以 **提高作业优先级** ，以增加处理时的响应速度：

    
    
    --5个优先级可选：VERY_HIGH，HIGH，NORMAL，LOW，VERY_LOW
    set mapred.job.priority=VERY_HIGH;
    

### 并发控制

为作业 **设置并行处理**
，对于非严格依赖的任务，进行并发运算。并行处理开启后，并不会增加Map数、Reduce数，而是将没有严格先后依赖的任务，同时进行运算处理。假设作业有2个Map，且互不影响，则并发开启后，会同时进行处理。

    
    
    set hive.exec.parallel=true; --设置并行，0.5.0版本支持
    set hive.exec.parallel.thread.number=15;--最大并行个数
    

处理过程多的任务，可以 **开启JVM重用** ，一般设置为10-20。这样，在任务结束后，不会立即释放JVM空间，从而减少了JVM销毁和新建所用的时间。

    
    
    set mapred.job.reuse.jvm.num.tasks=10;
    

对于Hive作业，可以 **动态调节Map数量** ，但对于不可切分的压缩文件，Map数只能减少不能增加。

    
    
    -- 决定每个map处理的最大的文件大小，单位为B
    set mapred.max.split.size=256000000
    -- 决定每个map处理的最小的文件大小，单位为B
    set mapred.min.split.size=10000000
    -- 节点中可以处理的最小的文件大小
    set mapred.min.split.size.per.node=8000000;
    -- 机架中可以处理的最小的文件大小
    set mapred.min.split.size.per.rack=8000000;
    

除了可以动态调整Map数量外，也可以 **动态调节Reduce数量** ，决定输出文件数，但一般不做调整。

    
    
    --设置每个reducer默认处理300M数据，0.14.0之前默认1G，1000作为进位
    set hive.exec.reducers.bytes.per.reducer =300000000;
    --reducer数量 N=min(hive.exec.reducers.max ，总输入数据量/ hive.exec.reducers.bytes.per.reducer )
    --hive.exec.reducers.max可自定义设置，0.14前默认为999，之后改为1099
    set hive.exec.reducers.max=1099
    --如果处理时，出现了OOM，则可以手动增大Reduce个数进行处理，默认为-1，自动推算
    set mapred.reduce.tasks=100;
    

### 性能优化

可以 **调整Map Spill在环形缓存区溢写的数据量** ，能够减少最终溢写生成的文件数。

    
    
    --调整内存缓存区大小
    set mapreduce.task.io.sort.mb=100
    --调整缓存区溢写阈值
    mapreduce.map.sort.spill.percent=0.8
    --使用combiner减少map输出的数据大小
    --适当进行数据压缩
    

**调整Reduce启动时机** ，可以不必等待Map完全处理完，提前启动Reduce进行数据运算。

    
    
    --Reduce运行进度，0-33％ 表示进行 shuffle，34-66％ 表示 sort，67％-100％ 表示 reduce
    --Reducer 启动时机， 1.0将等待所有 mapper 完成； 0.0 立即启动 reducers； 0.5则是一半的 map 完成时启动
    set mapred.reduce.slowstart.completed.maps = 0.5
    

**启动Fetch抓取** ，Hive对某些情况的查询可以不必使用MapReduce进行计算，如select*、limit。

    
    
    set hive.fetch.task.conversion=more;
    

**开启推测执行**
：如果某个Task因为配置等情况导致进度远落后于整体进度，则重新启动一个Task进行运算，当其中一个Task运算完成后，另外一个Task被直接kill掉。

    
    
    --map开启推测执行
    set mapred.map.tasks.speculative.execution=true
    --reduce开启推测执行
    set mapred.reduce.tasks.speculative.execution=true
    

**开启本地化启动** ，查询处理的数据量较小时，可以提高性能。

    
    
    set hive.exec.mode.local.auto=true;
    --task数不超过配置，则启动本地模式，
    set hive.exec.mode.local.auto.input.files.max=4;
    --输入文件大小不超过配置，启动本地模式
    set hive.exec.mode.local.auto.inputbytes.max=134217728
    

## 代码调优

### 表类型&压缩

选择适当的表格式进行数据存储，可以建立临时表用于中间结果的暂存。

    
    
    create temporary table <tmp_table> as select * from <table_name> ;
    --设置临时表存储介质，可以是memory、 ssd、default，默认default为磁盘存储
    set hive.exec.temporary.table.storage=memory;
    

进行合适的分区、分桶优化。进行数据查询、处理时，筛选条件总包含某个字段，设置为分区字段，可以加快数据扫描效率。设置桶表，可以提升数据查询、Join性能。

适当开启压缩，减轻传输、存储压力。

    
    
    --对MR运行过程中的数据进行压缩，减轻数据传输压力
    set hive.exec.compress.intermediate=true;
    set mapred.map.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
    --根据表类型，对最终结果进行压缩，减轻存储压力
    set hive.exec.compress.output=true;
    set mapred.output.compression.type=BLOCK;
    set mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec;
    set mapreduce.output.fileoutputformat.compress=true;
    

### Join调优

优先数据where过滤后再join，减少参与运算的数据量。

在多个join on连接条件中，尽量包含同一个join key，如果多个join on中包含同一个join key，那么它们会合并为一个MapReduce
Job。

    
    
    --Join Key中包含同一个b.key1，所以在运行时作为MapReduce作业运行
    SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1)
    

大表要放置在join语句之后。因为Hive在执行Join时，默认会将前面的表直接加载到缓存，后面一张表进行stream处理，即shuffle操作。这样可以减少shuffle过程，因为直接加载到缓存中的表，只需要等待后面stream过来的表数据，而不需要进行shuffle，相当于整体减少了一次shuffle过程。

    
    
    --a、b加载到内存中，c进行shuffle
    SELECT a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1)
    

#### Stream Table & Map Join

所以在SQL语句中，大表放在join后面，会有很好的优化效果，或者可以直接标注为StreamTable，来指定进行stream的表。

    
    
    SELECT /*+ STREAMTABLE(a) */ a.val, b.val, c.val FROM a JOIN b ON (a.key = b.key1) JOIN c ON (c.key = b.key1)
    

也可以将小表手动指定为map表，Hive在执行时会自动缓存到Map作业中。

    
    
    SELECT /*+ MAPJOIN(b) */ a.key, a.value FROM a JOIN b ON a.key = b.key
    

当然，Hive支持开启MapJoin，自动识别小表，并进行优化。

    
    
    --开启map join，Hive自动识别小表
    set hive.auto.convert.join=true;
    --小表的阈值，默认25mb，小于阈值则在map阶段写入内存。
    set hive.mapjoin.smalltable.filesize=25000000;
    --内连接中，除了第一个表之外的其他表都是小表时，将多个mapjoin合并为一个
    set hive.auto.convert.join.noconditionaltask=true;
    --多个小表的总大小需小于设定值时，进行mapjoin合并，Hive1.2.1默认为10MB
    set hive.auto.convert.join.noconditionaltask.size=60000000;
    

#### Bucket Join & SMB Map Join

如果两张表均分桶表，且join key为分桶键，分桶数成倍数关系，可以使用Bucket
Join提升性能。此时，直接从两张表的对应桶中读取数据进行Join操作，减少了Shuffle开销。

    
    
    --开启bucket map join,只有在分桶表才会有效
    set hive.optimize.bucketmapjoin = true;
    

在Bucket Join的基础上，如果分桶键有序存储，使用SMB（Sort Merge Bucket） Map
Join可以显著提升性能。因为分桶键有序，可以每次将表一部分数据提取，再与大表进行匹配；所以没有内存限制，可以执行全外连接。

    
    
    set hive.optimize.bucketmapjoin = true;
    set hive.auto.convert.sortmerge.join=true;
    set hive.optimize.bucketmapjoin.sortedmerge =true;
    set hive.input.format=org.apache.hadoop.hive.ql.io.bucketizedHiveInputFormat;
    

### 编码习惯

在SQL编写时， **一个Job能完成的任务，尽量不要拆分为多个Job** 。所以尽量减少嵌套层次、group by数量，从而减少Job数，提高性能。

    
    
    --创建2张表tb1、tb2
    create table eg_tb(id int, category_a string, category_b string);
    insert into table eg_tb values(1,"a", "0");
    insert into table eg_tb values(1,"b", "1");
    insert into table eg_tb values(1,"b", "0");
    
    --SQL中有2个Group By操作，会启动3个Job进行处理
    select category_a as category, '1' as type, count(1) as num
    from eg_tb
    group by category_a
    union all
    select category_b as category, '2' as type, count(1) as num
    from eg_tb
    group by category_b;
    --减少为1个Group By，只启动1个job，性能提升2倍以上
    --这里先将eg_tb表的分类字段category_a、category_b进行字符串合并
    --合并时标记字段类型：'category_a'_1-'category_b'_2
    --然后对合并后的字符串直接split拆分，并使用lateral view explode分为多行
    --然后对合并后的数据进行聚合统计，整个Job只用了一次Group By
    select category, type, count(1) as num from
    (select split(part, '_')[0] as category, split(part, '_')[1] as type, id
    from eg_tb
    lateral view explode(split(concat(category_a, '_1','-', category_b, '_2'), '-')) tem_col as part
    )t group by category, type;
    

**尽量避免单个Reduce任务的写法** 。

    
    
    --统计去重ID，单个Reduce执行
    select count(distinct id) from eg_tb;
    --改写，使用group by，增加Reduce
    select count(1) from (select id from eg_tb group by id) t;
    --全局排序，取TOP N数据，单Reduce执行
    select * from eg_tb order by id limit 10;
    --改写，先使用局部排序限制数量，然后再用全局排序取TOP N
    select * from
    (select * from eg_tb distribute by id sort by id desc limit 10) t
    order by id desc limit 10;
    --笛卡尔积，单Reduce执行
    select t1.*, t2.* from t1 join t2;
    --改写，添加on条件提前限制数据量
    select t1.*, t2.* from t1 join t2 on t1.id=t2.id;
    

Hive内置了大量的函数，在SQL处理时， **尽量使用内置函数来完成** ，性能会有所保证。

在企业中使用Hive SQL需要一定的规范。一般 **在SQL编写之前，需要进行规范的注释添加** ，并设定特定的配置。

在SQL文件开始，常见的注释有：

    
    
    --@Name:所属数据库.结果表
    --@Description:描述
    --@Type:表类型，如每日汇总表
    --@Target:结果表
    --@source:数据源表1    别名1
    --@source:数据源表2    别名2
    --@Author:工号 作者
    --@CreateDate:创建日期
    --@ModifyBy:修改人
    --@ModifyDate:修改日期
    --@ModifyDesc:修改描述
    --@Copyright  版权
    

良好的注释，便于代码的阅读和版本的控制。

在注释之后，会添加公用的调优参数：

    
    
    -- 设置作业名，方便出错后作业查询
    set mapred.job.name = TASK_NAME (${hivevar:statis_date});
    -- 每个Map最大输入大小,可以适当调整。
    set mapred.max.split.size = 300000000;
    -- 每个Map最小输入大小,可以适当调整。
    set mapred.min.split.size = 100000000;
    -- 执行Map前进行小文件合并
    set hive.input.format = org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
    -- hive自动根据sql，选择使用common join或者map join（关闭）
    set hive.auto.convert.join = false;
    -- 在Map-only的任务结束时合并小文件
    set hive.merge.mapfiles = true;
    -- 在Map-Reduce的任务结束时不合并小文件
    set hive.merge.mapredfiles = false;
    -- 合并文件的大小,可以适当调整。
    set hive.merge.size.per.task = 300000000;
    

这些参数中，首先进行了作业名的设置，然后对Map数量进行了设置，便于并发的提高；Map阶段前进行小文件合并，一定程度上缓解了小文件带来的影响。

接着可以设置Join的自动判断，如果存在小表，则直接使用map join将表加载到内存中，而避免shuffle，这里设置了关闭，可以根据需求来定。

最后设置任务结束时，小文件的合并。

在此之后，便可以进行SQL的编写。良好的开发规范，可以避免很多问题。当然在SQL文件中，一般仅涉及作业调优，对集群的公共调优部分会持久化到配置文件中。

## 常见问题

### 小文件问题

因为Hive的数据存储在HDFS中，而小文件会给HDFS带来很多问题。首先是过多小文件会占用NameNode大量的内存，为NameNode带来压力；其次每个小文件在处理时，会分发一个Map任务，大量的小文件会生成众多Map任务，从而增加Map任务生成、调度的时间，直接影响整体处理效率。

如果目标表已经存在小文件问题，可以在数据输入时，设置参数，自动进行小文件的合并。这种属于已经存在问题，所进行的事后处理。

    
    
    --每个Map最大输入大小设置为2GB（单位：字节）
    set mapred.max.split.size=2048000000
    --0.5.0版本后，已经是系统默认值
    set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;
    

为了避免小文件的生成，在数据输出时，可以进行数据合并。这种是事前处理，避免产生小文件问题。

    
    
    set hive.merge.mapfiles = true #在Map-only的任务结束时合并小文件
    set hive.merge.mapredfiles= true #在Map-Reduce的任务结束时合并小文件
    set hive.merge.size.per.task = 1024000000 #合并后文件的大小为1GB左右
    set hive.merge.smallfiles.avgsize=1024000000 #当输出文件的平均大小小于1GB时，启动一个独立的map-reduce任务进行文件merge
    

### 数据倾斜问题

在生产过程中，数据可能会发生倾斜，数据倾斜会直接影响数据处理的效率。

发生倾斜的原因首先是空值，在MapReduce处理时，会被分发到Task 0中进行处理，导致Task
0数据量过多，处理缓慢。对于这种情况，应该在处理时过滤空值或赋予空值其它值。

    
    
    --过滤空值
    select * from log a join user b on a.user_id is not null and a.user_id = b.user_id;
    --将空值转换为其它无意义的值
    select * from log a left outer join user b on
    case when a.user_id is null then concat('hive',rand()) else a.user_id end = b.user_id
    

其次，不同类型数据的关联，在Join时也可能会导致数据倾斜问题。假设Join条件是a.id=b.id，但a表的id为bigint类型，而b表id为string类型，这时会自动进行类型转换。但在自动转换时，就可能存在精度丢失，或者超过数据范围的情况发生，不同版本的Hive对于这种情况的处理方式不同，有的会按照最大值截断，有的会转换为NULL值处理。

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/5adf04b2fcfd32927a681f3dd58718e6.png#pic_center)

不管是使用哪种方式进行处理，会导致NULL值或最大值变多，直接造成数据倾斜。

所以对于不同类型的数据，进行Join关联时，提前进行数据转换是必要的。

    
    
    --将数据进行类型转换
    select * from user a left outer join log b on b.user_id = cast(a.user_id as string)
    

SQL的聚合运算（Group
By），是比较容易产生数据倾斜的一种操作。此时，可以提前在Map端进行数据聚合，从而减少Shuffle时传输的数据量，以减少数据倾斜程度。

    
    
    set hive.map.aggr = true;  -- Map端聚合
    set hive.groupby.mapaggr.checkinterval=100000;    -- Map端聚合的条数
    set hive.map.aggr.hash.min.reduction=0.5;  --预先取100000条数据聚合,如果聚合后的条数/100000>0.5，则不再聚合
    

当然，对于聚合运算产生的数据倾斜现象，可以在SQL中提前对倾斜列提前进行标记，会使用两个MR任务进行处理。第一个MR任务不会理会数据的具体内容，而是将全部数据均匀分布到Reduce节点中进行运算，此时相当于Combiner部分结果聚合，不会产生数据倾斜。而第二个MR任务，才会将Key值相同的数据发送到同一个节点中进行聚合运算，得到最终结果。因为有两个MR，所以能够极大的减轻倾斜列带来的倾斜问题。

    
    
    set hive.groupby.skewindata=true; --如果是group by过程出现数据倾斜 应该设置为true
    set hive.groupby.mapaggr.checkinterval=100000;--设置group的键对应记录数超过该值会进行优化
    

最后对于复杂Join，因为两张表数据分布不一致，也会导致数据倾斜现象。此时，如果某一列的数据出现倾斜，而且有一张小表，可以使用MapJoin避免Shuffle情况。

    
    
    select /*+mapjoin(x)*/* from log a left outer join user b on a.user_id = b.user_id;
    

如果在复杂Join中，出现数据倾斜现象，而且没有小表无法使用MapJoin，则可以使用skewjoin。SkewJoin的原理是，发现列数据中明显倾斜的数据值，则不会进行Shuffle运算，而是将要Join的表对象直接加载到内存中直接进行MapJoin，而其他没有发生倾斜的数据会正常进行Shuffle运算。

    
    
    set hive.optimize.skewjoin = true; --如果是join过程出现倾斜 应该设置为true
    set hive.skewjoin.key = 100000;
    

### 内存不足问题

在Hive计算过程中，内存不足时，会提示以下错误。

    
    
    ERROR: java.lang.OutOfMemoryError: GC overhead limit exceeded
    

此时可以适当调整Container内存大小，其中Xms：启动内存，Xmx：运行时内存，Xss：线程堆栈大小。

    
    
    --设置Container内存上限
    set mapreduce.map.memory.mb=-Xmx20480m
    set mapreduce.reduce.memory.mb=-Xmx20480m
    --Container中启动的Map任务，可以使用的内存上限
    set mapreduce.map.java.opts=-Xmx16384m
    --Container中启动的Reduce任务，可以使用的内存上限
    set mapreduce.reduce.java.opts=-Xmx16384m
    

### 虚拟内存超标

虚拟内存占用过多时，会提示以下错误。

    
    
    container_e93_1537448956025_1531565_01_000033] is running beyond physical memory limits. Current usage: 5.1 GB of 4 GB physical memory used; 18.6 GB of 8.4 GB virtual memory used.
    

虚拟内存大小=yarn.nodemanager.vmem-pmem-ratio（虚拟内存比例） * mapreduce.{map or
reduce}.memory.mb（任务内存大小）。所以可以适当增大内存大小，或者虚拟内存的比率。

    
    
    set mapreduce.map.memory.mb=-Xmx20480m
    set mapreduce.reduce.memory.mb=-Xmx20480m
    --虚拟内存比率默认为2.3，即使用1M内存，则可以使用2.3M虚拟内存
    set yarn.nodemanager.vmem-pmem-ratio=2.3
    

当然也可以在yarn-site.xml中取消虚拟内存、内存检测，但这样可能会导致程序内存泄漏，从而致使集群奔溃。

    
    
    set yarn.nodemanager.vmem-check-enabled=false
    set yarn.nodemanager.pmem-check-enabled=false
    

对于出现虚拟内存占用过多的情况，首先应该考虑程序是否存在数据倾斜、内存泄漏等问题，如果一切正常，然后再调整内存或者虚拟内存比率大小。

