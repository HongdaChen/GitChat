Hive支持的函数有：普通内置函数、内置聚合函数（UDAF）、内置表生成函数（UDTF）。接下来，会对这些函数进行讲解。

可以使用SQL命令查看支持的所有函数。

    
    
    SHOW FUNCTIONS;
    

使用DESC命令可以查看函数的具体描述信息。

    
    
    DESC FUNCTION EXTENDED concat;
    

## 普通内置函数

普通内置函数，在对数据处理时，进行一对一的数据转换。对每一行数据处理后，生成新的一行数据。

普通内置函数包含：数学运算函数、集合函数、类型转换函数、日期函数、条件函数、字符串函数、数据屏蔽函数、其它混合函数。

### 数学运算函数

Hive支持对数字类型的列（Int、Double、DECIMAL ）使用数学运算函数进行运算。

    
    
    --对salary列进行四舍五入运算
    select round(salary) as costs from <table_name>;
    --对salary列求绝对值
    select abs(salary) as costs from <table_name>;
    --将salary列转换为二进制类型
    select bin(salary) as costs from <table_name>;
    

Hive官网目前支持的数学运算函数（2020年12月10日，参考最新的官方文档，进行翻译和简化）有：

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/8359805512e4d4aa39651bb4b8d86a53.png#pic_center)

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/ff8eeec92b946bab891960689478f83d.png#pic_center)

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/84343f1afd074c0709c9d1e2fee1d64d.png#pic_center)

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/9a3da994d9beaabd51cebc2e06fbc6b7.png#pic_center)

这里需要注意的是，Hive在0.13.0版本后才增加Decimal类型，所以在此之后，很多函数也对Decimal类型进行了支持。对于数字类型的列，除了使用函数之外，一般也会使用算数运算符进行操作。支持的算数运算符请见上一节数据查询。

函数列表仅作为参考使用，大家一次性不可能记住所有函数的用法，大致浏览一下留个印象即可，之后使用时进行速查即可。官方支持的函数翻译工作进行了大概一周时间，[hive文档链接](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+UDF#LanguageManualUDF-
Built-inFunctions)，因为文档对很多函数的作用描述不清，查阅了很多资料。翻译后的文档，大家翻阅起来会更友好一些。

### 集合函数

Hive支持对集合数据类型进行函数操作，但整体来说功能比较少，主要是对集合类型的元素查询操作。

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/4bc19f5f02838484435f8b464d8f2b00.png#pic_center)

### 类型转换函数

目前Hive仅支持两个类型转换函数，binary和cast。其中cast函数较为通用，可以将符合要求的数据转换为任意类型。

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/90cb9a1540bc3c258b9751d83e1e4067.png#pic_center)

    
    
    select
    cast("123" as double),
    cast("456.78" as int),
    cast("1.99" as int),
    cast("abc" as int)
    ;
    --运行结果
    123.0,    456,    1,    NULL
    

### 日期函数

对于日期的转换，Hive提供的函数比较多。可以对时间字符串数据进行操作。Hive时间字符串标准为‘yyyy-MM-dd' 或 'yyyy-MM-dd
HH:mm:ss'。当然在2.1.0版本增加Date数据类型后，相应的一些函数也逐渐开始支持Date类型。

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/509faf8dec797df117dd500f553c12ad.png#pic_center)

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/db52a66db28fbf355b7dbc50e6c914f4.png#pic_center)

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/b0c575bbf08a3c52b0aa155af4850e1b.png#pic_center)

### 条件函数

Hive支持对数据进行条件判断，以便支持数据清洗、去重等场景的应用。支持的条件函数有：

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/eff1f2d0c827623434f06e8135d391a6.png#pic_center)

条件函数在数据处理过程中会经常使用。

其中if函数，可以根据条件改变数据的返回值。条件语句中可以使用比较运算符、逻辑运算符，也可以使用isnull、isnotnull等条件函数。

    
    
    select if(case companyid=0, '未登录’ , companyid) from <table_name>;
    

当然case when也可以实现与if相同的功能，但它能嵌套的条件判断更多。

case when有两种使用方法：

    
    
    --写法一，当a=b时返回c，a=d时返回e，否则返回f
    CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END
    --写法二，当a=true返回b，当c=true返回d，否则返回e
    CASE WHEN a THEN b [WHEN c THEN d]* [ELSE e] END
    

实际使用时，根据使用习惯选择一种即可：

    
    
    select case companyid when 0 then '未登录' else companyid end from <table_name>;
    select case  when companyid=0 then '未登录' else companyid end from <table_name>;
    

### 字符串函数

Hive内置的字符串处理函数非常之多。一般而言，在进行数据处理时，字符串类型的数据会占大多数。

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/d1f171e268fd148e1ffcf082037c12c0.png#pic_center)

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/ebf7b779c86d53167031a1006be7fde6.png#pic_center)

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/92cbe7b0364af0987c98e24b867e5649.png#pic_center)

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/dd8050ea42fb5ff4694b3ca0fde434a2.png#pic_center)

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/c3d5f64c8ac691309a550695d9b78c9e.png#pic_center)

常用的字符串处理函数有：

    
    
    --length：求字符串长度
    select length('abcd');  --result: 4
    ---reverse：字符串反转函数
    select reverse('abcd');  --result: "dcba"
    --concat：字符串连接函数
    select concat('aa','bb','cc');  --result: "aabbcc"
    --concat_ws：带分隔符字符串连接函数
    select concat_ws(',','aaa','bbb','ccc');  --result: "aaa,bbb,ccc"
    --substr、substring：字符串截取函数
    select substr('abcde',3);  --从第3个字符开始截取 result: "cde"
    select substr('abcde',-1);  --截取最后1个字符串 result: "e"
    select substr('abcde',-2,2);  --从倒数第2个字符开始截取2个字符 result: "de"
    select substr('abcde',3,2);  --从第3个字符开始截取，截取2个字符 result: "cd"
    --upper,ucase：字符串转大写
    select upper('Hive');  --result: "HIVE"
    --lower,lcase：字符串转小写
    select lower('Hive');  --result: "hive"
    --trim、ltrim、rtrim：去空格函数
    select trim(' Hive ');  --result: "Hive"
    select ltrim(' Hive ');  --result: "Hive "
    select rtrim(' Hive ');  --result: " Hive"
    

对于字符串正则表达式处理的函数有：

    
    
    --regexp_replace：正则表达式替换
    select regexp_replace('foobar','oo|ar','');  --result: fb
    
    --regexp_extract：正则表达式解析
    --语法：regexp_extract(string subject, string pattern, int index)
    --index=0，返回全部表达式结果，index>0，返回对应()中的结果
    select regexp_extract('foothebar', 'foo(.*?)(bar)', 1);  --result:the
    select regexp_extract('foothebar', 'foo(.*?)(bar)', 2);  --result:bar
    select regexp_extract('foothebar', 'foo(.*?)(bar)', 0);  --result:foothebar
    

JSON解析函数get _json_ objec，可以将JSON中的某个字段解析出来：

    
    
    --待解析的数据
    +----+
    json=
    {
    "store":
    {"fruit":[{"weight":8,"type":"apple"},{"weight":9,"type":"pear"}],
    "bicycle":{"price":19.95,"color":"red"}
    },
    "email":"someone@udf.org",
    "owner":"someone"
    }
    +----+
    --创建临时表，插入数据
    create temporary table src_json(id int, json string);
    insert into src_json values(1, '{"store":{"fruit":[{"weight":8,"type":"apple"},{"weight":9,"type":"pear"}],"bicycle":{"price":19.95,"color":"red"}},"email":"someone@udf.org","owner":"someone"}');
    --JSON解析函数：get_json_object(string json_string, string path)
    --path参数中，$指代json对象，.name指定json中某个字段，[num]指定数组
    SELECT get_json_object(json, '$.owner') FROM src_json;
    SELECT get_json_object(json, '$.store.fruit[0]') FROM src_json;
    SELECT get_json_object(json, '$.non_exist_key') FROM src_json;
    --结果分别为：
    someone
    {"weight":8,"type":"apple"}
    NULL
    

get _json_ objec只能对一个字段解析，而使用json _tuple函数可以对多个字段进行解析，但json_
tuple属于后面要讲到的UDTF函数，这里先提及一下。

    
    
    --JSON解析函数：json_tuple，使用lateral view 关键字对多字段解析
    select src.id, f.* from src_json src lateral view json_tuple(src.json, 'email', 'owner') f as f1, f2;
    

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/92fd96cfd2629121af1bb53367e6abdc.png#pic_center)
URL解析函数parse_url，可以返回URL中的特定部分：

    
    
    --创建临时表
    create temporary table url_store(id int, url string);
    insert into url_store values(1, 'http://facebook.com/path1/p.php?k1=v1&k2=v2#Ref1');
    
    --parse_url(string urlString, string partToExtract [, stringkeyToExtract])
    --partToExtract 的有效值为：HOST, PATH, QUERY, REF, PROTOCOL, AUTHORITY, FILE, and USERINFO
    --函数调用
    select parse_url(url, 'HOST') from url_store;
    select parse_url(url, 'QUERY','k1') from url_store;
    --结果为
    facebook.com
    v1
    

使用UDTF函数中的parse _url_ tuple，可以对URL的多个部分进行解析。

    
    
    --函数调用
    select t.* from url_store LATERAL VIEW parse_url_tuple(url, 'HOST', 'PATH', 'QUERY', 'QUERY:k1') t as host, path, query, query_id;
    

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/e5cb84b5c556441adbef842528dfdb03.png#pic_center)

### 数据屏蔽函数（Data Masking Functions）

Hive在2.1.0版本后，支持数据屏蔽函数（Data Masking Functions），可以用于数据的快速掩码处理（脱敏）。

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/273c07f6edffd653f1dcaba096ef203a.png#pic_center)

比如mask函数，默认会将查询回的数据，大写字母转换为X，小写字母转换为x，数字转换为n。当然也可以添加参数自定义转换的字母`mask(string
str[, string upper[, string lower[, string
number]]])`，upper定义大写字母转换，lower定义小写字母转换，number定义数字转换。

    
    
    --函数调用
    select mask("abcd-EFGH-8765-4321");
    --结果为：
    xxxx-XXXX-nnnn-nnnn
    

其余函数`mask_first_n(string str[, int n])`可对前n个字符进行掩码处理，`mask_last_n(string str[,
int n])`可对后n个字符进行掩码处理。

而`mask_show_first_n(string str[, int
n])`则是除了前n个字符，其余进行掩码处理，`mask_show_last_n(string str[, int
n])`是除了后n个字符，其余进行掩码处理。

当然，最后`mask_hash(string|char|varchar str)`会返回字符串的hash编码。

使用这些函数，可以方便的对一些数据进行脱敏，从而在保证数据安全的情况下，交由教学使用。

### 混合函数（Misc. Functions）

Hive内置的混合函数如下：

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/00fcc91a307fa12d8dd1d4e133ae2714.png#pic_center)

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/50a982532dc54297ff8fbe8c173d28ff.png#pic_center)

这些混合函数中，大多是与加密相关的。还有就是返回Hive的一些信息，如用户身份、集群版本等、

## 内置聚合函数（UDAF）

与普通内置函数不同，UDAF主要提供数据的聚合操作，进行多对一的数据转换。

Hive最新版本提供的函数如下：

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/aefbee353d334bf18a5c0d4e6febaa33.png#pic_center)

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/659daa934c67afea2b3f070232e2c2c0.png#pic_center)

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/f487323397a513dc327b7c212ba0e77b.png#pic_center)

对于UDAF，大多数是一些数学计算，常用的涉及的不会很多。

基本上常用的聚合函数有：

    
    
    --sum()：求和
    select sum(salary) salaries as  from <table_name>;
    --count()：求数据总数
    select count(*) from <table_name>;
    --avg()：求平均值
    select avg(salary) from <table_name>;
    --min：求最小值
    select min(salary) from <table_name>;
    --max：求最大值
    select max(salary) from <table_name>;
    

使用聚合函数时，可以配合distinct关键字，对数据进行去重处理。

    
    
    --count()：求不重复的数据总数
    select count(distinct id) from <table_name>;
    --sum()：对不重复的数据求和
    select sum(distinct salary) salaries as  from <table_name>;
    

## 内置表生成函数UDTF

内置的UDTF表生成函数提供数据一对多的转换。Hive内置的表生成函数有：

![在这里插入图片描述](https://img-
blog.csdnimg.cn/img_convert/c779267cff431c8155cde4bbfa75a2b5.png#pic_center)

其中常用的除了在前面字符串函数中提到的json _tuple、parse_
url_tuple之外，使用最多的还是explode函数，它有两个重载形式，可以将数组、Map数据拆分为多行返回。

首先是对于explode (array)的使用。

    
    
    select explode(array('A','B','C'));
    select explode(array('A','B','C')) as col;
    select tf.* from (select 0) t lateral view explode(array('A','B','C')) tf;
    select tf.* from (select 0) t lateral view explode(array('A','B','C')) tf as col;
    --以上SQL返回相同结果
    +---------+
    | col     |
    +---------+
    | A       |
    | B       |
    | C       |
    +---------+
    

然后是explode (map)。

    
    
    select explode(map('A',10,'B',20,'C',30));
    select explode(map('A',10,'B',20,'C',30)) as (key,value);
    select tf.* from (select 0) t lateral view explode(map('A',10,'B',20,'C',30)) tf;
    select tf.* from (select 0) t lateral view explode(map('A',10,'B',20,'C',30)) tf as key,value;
    --以上SQL返回相同结果
    +------+--------+
    | key  | value  |
    +------+--------+
    | A    | 10     |
    | B    | 20     |
    | C    | 30     |
    +------+--------+
    

可以使用posexplode (array)函数，将数组array拆分为多行的同时，标记每一行的下标值。

    
    
    select posexplode(array('A','B','C'));
    select posexplode(array('A','B','C')) as (pos,val);
    select tf.* from (select 0) t lateral view posexplode(array('A','B','C')) tf;
    select tf.* from (select 0) t lateral view posexplode(array('A','B','C')) tf as pos,val;
    --以上SQL返回相同结果
    +------+------+
    | pos  | val  |
    +------+------+
    | 0    | A    |
    | 1    | B    |
    | 2    | C    |
    +------+------+
    

使用inline (array of structs)函数进行结构体拆分。

    
    
    select inline(array(struct('A',10,date '2015-01-01'),struct('B',20,date '2016-02-02')));
    select inline(array(struct('A',10,date '2015-01-01'),struct('B',20,date '2016-02-02'))) as (col1,col2,col3);
    select tf.* from (select 0) t lateral view inline(array(struct('A',10,date '2015-01-01'),struct('B',20,date '2016-02-02'))) tf;
    select tf.* from (select 0) t lateral view inline(array(struct('A',10,date '2015-01-01'),struct('B',20,date '2016-02-02'))) tf as col1,col2,col3;
    
    --以上SQL返回相同结果
    +-------+-------+-------------+
    | col1  | col2  |    col3     |
    +-------+-------+-------------+
    | A     | 10    | 2015-01-01  |
    | B     | 20    | 2016-02-02  |
    +-------+-------+-------------+
    

使用stack (num, values)函数，则可以将一行数据value拆分为num行：

    
    
    select stack(2,'A',10,date '2015-01-01','B',20,date '2016-01-01');
    select stack(2,'A',10,date '2015-01-01','B',20,date '2016-01-01') as (col0,col1,col2);
    select tf.* from (select 0) t lateral view stack(2,'A',10,date '2015-01-01','B',20,date '2016-01-01') tf;
    select tf.* from (select 0) t lateral view stack(2,'A',10,date '2015-01-01','B',20,date '2016-01-01') tf as col0,col1,col2;
    --以上SQL返回相同结果
    +-------+-------+-------------+
    | col0  | col1  |    col2     |
    +-------+-------+-------------+
    | A     | 10    | 2015-01-01  |
    | B     | 20    | 2016-01-01  |
    +-------+-------+-------------+
    

