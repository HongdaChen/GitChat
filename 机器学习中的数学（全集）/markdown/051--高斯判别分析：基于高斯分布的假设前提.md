在上一讲里我们说了，概率判别模型是直接对 $p(Y|X)$ 进行建模，例如在逻辑回归当中，我们就是直接去求得 $p(y=1|x)$ 和 $p(y=0|x)$
的概率值，并通过概率值直接来判断是分类是取 0 还是 1。

### 概率生成模型的关注问题

这一讲里所要介绍的是概率生成模型：它关心的是 $p(y=0|x)$ 和 $p(y=1|x)$ 两个概率谁更大，注意！是只比较二者的大小，而不是无谓地去求取
$p(y|x)$ 的具体取值。这里就要借助贝叶斯公式：

$$p(y|x)=\frac{p(x|y)p(y)}{p(x)}$$

而分母 $p(x)$ 是样本的概率，是常数值，它与我们最终的取值无关，因此就有 $p(y|x)\propto p(x|y)p(y)$，即正比于联合概率。

此时我们就看到了生成模型的样子：

$$y=argmax_{y \in \\{0,1\\}}~p(y|x)=argmax~p(y)p(x|y)$$

### 高斯判别模型原理及要素

那么现在我们分别来看 $p(y)$ 和 $p(x|y)$。

$y$ 的取值是 1 或 0，这是一个二分类的问题，因此随机变量 $y$ 服从伯努利分布：

y | 0 | 1  
---|---|---  
P | ϕ | 1-ϕ  
  
也就是说，$p(y=1)=\phi^y$，$p(y=0)=(1-\phi)^{1-y}$。数学上，我们把它集成起来，就是
$p(y)=\phi^y(1-\phi)^{1-y}$。

我们再来看 $p(x|y)$。模型的名字为高斯判别分析，之所以含有高斯二字，其要义就体现在了 $p(x|y)$
之上，高斯判别模型中有一个非常强的假设，那就是：当确定了样本类别时，样本存在的概率服从高斯分布。写成表达式就是：

$$p(x|y=1) =N(\mu_1,\Sigma)$$$$p(x|y=0)=N(\mu_0,\Sigma)$$

从这里我们可以进一步详细地描述模型的假设，那就是：基于不同分类的条件概率满足高斯分布，它们拥有不同的均值（或均值向量），但是它们的方差（或协方差矩阵）是相同的。

我们将这两个式子结合起来就是：

$$p(x|y)=N(\mu_1,\Sigma)^yN(u_2,\Sigma)^{1-y}$$

### 高斯判别模型的参数估计

那么，明确了联合概率的前后两部分 $p(y)$ 和 $p(x|y)$ 之后，我们即针对 $p(x|y)p(y)$
建立似然函数，然后利用极大似然估计的方法去估计高斯判别模型的各个参数。因此，我们拿出对数似然函数：

$$L(\theta)=log\prod_{i=1}^N(p(x_i|y_i)p(y_i))\\\=\sum_{i=1}^Nlog(p(x_i|y_i)p(y_i))\\\=\sum_{i=1}^N(log~p(x_i|y_i)+log~p(y_i))\\\=\sum_{i=1}^N(log~N(\mu_1,\Sigma)^{y_i}N(\mu_2,\Sigma)^{1-y_i}+log\phi^{y_i}(1-\phi)^{1-y_i})\\\=\sum_{i=1}^N(log~N(\mu_1,\Sigma)^{y_i}+log~N(\mu_2,\Sigma)^{1-y_i}+log\phi^{y_i}(1-\phi)^{1-y_i})$$

上述，我们就获得了似然函数的最终形式，下面利用极大似然估计的方法，针对这个似然函数去估计模型的参数
$\theta$，模型共有四个具体的参数：$\theta=(\phi,\mu_1,\mu_2,\Sigma)$，其中 $y=1$ 的样本数个数为
$N_1$，$y=0$ 的样本数个数为 $N_0$，$N_1+N_2=N$。

我们先来估计参数 $\phi$。

参数 $\phi$ 只跟对数似然函数的第三项有关，因此：

$$\phi_{mle}=argmax_{\phi}\sum_{i=1}^Nlog\phi^{y_i}(1-\phi)^{1-y_i}\\\=argmax_{\phi}\sum_{i=1}^N(y_ilog\phi+(1-y_i)log(1-\phi))$$

$$\frac{\partial}{\partial
\phi}\sum_{i=1}^N(y_ilog\phi+(1-y_i)log(1-\phi))\\\=\sum_{i=1}^N(y_i\frac{1}{\phi}-(1-y_i)\frac{1}{1-\phi})=0$$

经过一些简单的运算：

$$\sum_{i=1}^N[y_i(1-\phi)-(1-y_i)\phi]=0\\\\\sum_{i=1}^N(y_i-\phi)=\sum_{i=1}^Ny_i-\sum_{i=1}^N\phi=\sum_{i=1}^Ny_i-N\phi=0$$

最终我们成功的估计出了第一个参数 $\phi$：

$$\phi_{mle}=\frac{1}{N}\sum_{i=1}^Ny_i=\frac{N_1}{N}$$

接着我们再来看第二个参数 $\mu_1$，显然它只和似然函数的第一项有关：

$$\sum_{i=1}^N(log~N(\mu_1,\Sigma)^{y_i}+log~N(\mu_2,\Sigma)^{1-y_i}+log\phi^{y_i}(1-\phi)^{1-y_i})$$

$$\mu_1=argmax_{\mu_1}\sum_{i=1}^Nlog~N(\mu_1,\Sigma)^{y_i}\\\=argmax_{\mu_1}\sum_{i=1}^Ny_ilog\frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma|^{1/2}}exp\\{-\frac{1}{2}(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)\\}\\\=argmax_{\mu_1}\sum_{i=1}^Ny_ilog~exp\\{-\frac{1}{2}(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1)\\}\\\=argmax_{\mu_1}\sum_{i=1}^Ny_i(-\frac{1}{2}(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1))$$

最终就落脚到：

$$\frac{\partial}{\partial
\mu_1}\sum_{i=1}^Ny_i(-\frac{1}{2}(x_i-\mu_1)^T\Sigma^{-1}(x_i-\mu_1))\\\=\frac{\partial}{\partial
\mu_1}-\frac{1}{2}\sum_{i=1}^Ny_i(x_i^T\Sigma^{-1}x_i-
x_i^{T}\Sigma^{-1}\mu_1-\mu_1^T\Sigma^{-1}x_i+\mu_1^T\Sigma^{-1}\mu_1)$$

这里又是那个小技巧，我们发现，$x_i^{T}\Sigma^{-1}\mu_1$ 和 $\mu_1^T\Sigma^{-1}x_i$
两项互为转置的关系，并且它们最终都表示一个实数，因此二者显然是相等的。$x_i^T\Sigma^{-1}x_i$
是一个常数项，因此也可以忽略不计的，因此进一步变为：

$$\frac{\partial}{\partial
\mu_1}-\frac{1}{2}\sum_{i=1}^Ny_i(-2\mu_1^T\Sigma^{-1}x_i+\mu_1^T\Sigma^{-1}\mu_1)\\\=-\frac{1}{2}\sum_{i=1}^Ny_i(-2\Sigma^{-1}x_i+2\Sigma^{-1}\mu_1)=0\\\\\sum_{i=1}^Ny_i(\mu_1-x_i)=0\Rightarrow
\sum_{i=1}^Ny_i\mu_1=\sum_{i=1}^Ny_ix_i$$

最终，我们得到了 $\mu_1$ 的极大似然估计值：

$$\mu_1=\frac{\sum_{i=1}^{N}y_ix_i}{\sum_{i=1}^Ny_i}=\frac{\sum_{i=1}^{N}y_ix_i}{N_1}$$

至于说另一个均值参数 $\mu_2$，求解思路和具体技巧都是一模一样的，这里我们就不重复推导了。

最后我们来看一下协方差矩阵 $\Sigma$ 的估计过程。

再次地看一下似然函数：

$$\sum_{i=1}^N(log~N(\mu_1,\Sigma)^{y_i}+log~N(\mu_2,\Sigma)^{1-y_i}+log\phi^{y_i}(1-\phi)^{1-y_i})$$

第一部分 $\sum_{i=1}^Nlog~N(\mu_1,\Sigma)^{y_i}$ 和第二部分
$\sum_{i=1}^Nlog~N(\mu_2,\Sigma)^{1-y_i}$，只有这两项和协方差矩阵 $\Sigma$ 有关。

这里我们为了简化计算，引入一个小的技巧，样本不是分为两类吗？我们分为 $C_1$ 和 $C_2$ 分开来写：

$$C_1=\\{x_i|y_i=1,i=1,2,3,...,n_1\\}，|C_1|=n_1\\\C_2=\\{x_i|y_i=0,i=1,2,3,...,n_2\\}，|C_2|=n_2$$

满足：$n_1+n_2=N$。

于是第一部分化简为：

$$\sum_{i=1}^Nlog~y_iN(\mu_1,\Sigma)^{y_i}=\sum_{i=1}^Ny_ilog~N(\mu_1,\Sigma)=\sum_{x_i
\in C_1}log~N(\mu_1,\Sigma)$$

同理第二部分化简为：

$$\sum_{i=1}^Nlog~N(\mu_2,\Sigma)^{1-y_i}=\sum_{i=1}^N(1-y_i)log~N(\mu_2,\Sigma)\\\=\sum_{x_i
\in C_2}log~N(\mu_2,\Sigma)$$

同样，我们将利用极大似然估计法，去求：

$$\frac{\partial}{\partial \Sigma}(\sum_{x_i \in
C_1}log~N(\mu_1,\Sigma)+\sum_{x_i \in C_2}log~N(\mu_2,\Sigma))$$

似乎看起来比较复杂，我们来琢磨一下这里面应该如何化简，注意这里我们看一下化简的通用形式，用 $\mu$ 来表示均值：

$$\sum_{i=1}^Nlog~N(\mu,\Sigma)=\sum_{i=1}^Nlog\frac{1}{(2\pi)^{\frac{p}{2}}|\Sigma|^\frac{1}{2}}exp\\{\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\\}\\\=-\sum_{i=1}^N\frac{p}{2}log(2\pi)-\sum_{i=1}^N\frac{1}{2}log|\Sigma|-\sum_{i=1}^N\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)$$

似乎越来越复杂了，这里化简技巧又来了，首先第一项是常数，我们不用管，第三项重点我们来看一下。

这里我们引入矩阵的一个概念：迹。一个 $n$ 阶方阵 $A$ 的迹 $tr(A)$，表示为方阵 $A$ 对角线上所有的元素之和，而
$(x-\mu)^T\Sigma^{-1}(x-\mu)$ 运算的结果是一个标量，标量可以看作是特殊的 $1\times 1$ 方阵，因此：

$$(x-\mu)^T\Sigma^{-1}(x-\mu)=tr((x-\mu)^T\Sigma^{-1}(x-\mu))$$

方阵的迹有一个这样的特性 $tr(AB)=tr(BA)$，因此：

$$\sum_{i=1}^N(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)\\\=\sum_{i=1}^Ntr((x_i-\mu)^T\Sigma^{-1}(x_i-\mu))\\\=\sum_{i=1}^Ntr((x_i-\mu)^T(x_i-\mu)\Sigma^{-1})\\\=tr\sum_{i=1}^N((x_i-\mu)^T(x_i-\mu)\Sigma^{-1})\\\=tr[\Sigma^{-1}\sum_{i=1}^N(x_i-\mu)^T(x_i-\mu)]$$

我们看到了一个熟悉的身影：

$$\sum_{i=1}^N(x_i-\mu)^T(x_i-\mu)$$

还记得样本方差的表达式吗？

$$S=\frac{1}{N}\sum_{i=1}^N(x_i-\mu)^T(x_i-\mu)\\\\\Rightarrow
\sum_{i=1}^N(x_i-\mu)^T(x_i-\mu)=NS$$

最终：

$$\sum_{i=1}^N(x_i-\mu)^T\Sigma^{-1}(x_i-\mu)=Ntr(S\Sigma^{-1})$$

那么，合并起来：

$$\sum_{i=1}^Nlog~N(\mu,\Sigma)\\\=-\sum_{i=1}^N\frac{p}{2}log(2\pi)-\sum_{i=1}^N\frac{1}{2}log|\Sigma|-\sum_{i=1}^N\frac{1}{2}(x-\mu)^T\Sigma^{-1}(x-\mu)\\\=C-\frac{1}{2}Nlog|\Sigma|-Ntr(S\Sigma^{-1})$$

好，回到第一部分中的表达式：

$\sum_{x_i \in C_1}log~N(\mu_1,\Sigma)$

这里面包含的是属于 $C_1$ 中的所有样本 $x_i$，因此：

$$\sum_{x_i \in
C_1}log~N(\mu_1,\Sigma)=C_1-\frac{1}{2}N_1log|\Sigma|-\frac{1}{2}N_1tr(S_1\Sigma^{-1})$$

同样，第二部分也同理化作：

$$\sum_{x_i \in
C_2}log~N(\mu_2,\Sigma)=C_2-\frac{1}{2}N_2log|\Sigma|-\frac{1}{2}N_2tr(S_2\Sigma^{-1})$$

最终似然函数的结果为：

$$\sum_{i=1}^N(log~N(\mu_1,\Sigma)^{y_i}+log~N(\mu_2,\Sigma)^{1-y_i}+log\phi^{y_i}(1-\phi)^{1-y_i})\\\=\sum_{x_i
\in C_1}log~N(\mu_1,\Sigma)+\sum_{x_i \in
C_2}log~N(\mu_2,\Sigma)\\\=-\frac{1}{2}N_1log|\Sigma|-\frac{1}{2}N_1tr(S_1\Sigma^{-1})-\frac{1}{2}N_2log|\Sigma|\\\\-\frac{1}{2}N_2tr(S_2\Sigma^{-1})+C\\\=-\frac{1}{2}Nlog|\Sigma|-\frac{1}{2}N_1tr(S_1\Sigma^{-1})-\frac{1}{2}N_2tr(S_2\Sigma^{-1})+C$$

后面就又是惯常手法，让似然函数对$\Sigma$求导：

这里面又涉及到一些矩阵求导的公式，我们把要用到的公式先摆出来，这些大家也不用死记硬背，要用的时候，网上都有：

$$\frac{\partial tr(AB)}{\partial A}=B^T$$$$\frac{\partial |A|}{\partial
A}=|A|A^{-1}$$$$tr(AB)=tr(BA)$$$$tr(ABC)=tr(CAB)=tr(BCA)$$

好了，有了这几个公式作为利器，似然函数的求导问题就迎刃而解了：

$$\frac{\partial}{\partial\Sigma}[Nlog|\Sigma|+N_1tr(S_1\Sigma^{-1})+N_2tr(S_2\Sigma^{-1})]\\\=N\frac{1}{|\Sigma|}|\Sigma|\Sigma^{-1}-N_1S_1\Sigma^{-2}-N_2S_2\Sigma^{-2}=0\\\N\Sigma-
N_1S_1-N_2S_2=0$$

最后，我们得到了协方差矩阵的极大似然估计值：

$$\Sigma=\frac{1}{N}(N_1S_1+N_2S_2)$$

### 小结

这一讲可以说非常艰辛，大篇幅的公式让作者辛劳，让读者难受，不过读者可以沿着推导的思路细细品味一番，了解一下整个模型推进的过程，过程虽然冗长，但是没有跳步，大家一步一步慢慢看，相信会有很大的收获。

