### 利用 EM 迭代模型参数的思路

这一讲，我们来介绍如何利用 EM 的方法，迭代地求取高斯混合模型的参数，先拿出迭代公式，我们令：

  * 样本的观测数据集为 $X=\\{x_1,x_2,...,x_N\\}$
  * 隐变量集为 $Z=\\{z_1,z_2,...,z_N\\}$
  * 参数为 $\theta=\\{p_1,p_2,...,p_K,\mu_1,\mu_2,...,\mu_K,\Sigma_1,\Sigma_2,...,\Sigma_K\\}$

则有：

$$\theta^{(t+1)}=argmax_{\theta}\int_{Z}log~p(X,Z|\theta)~p(Z|X,\theta^{(t)})dZ$$

我们令：

$$Q(\theta,\theta^{(t)})=\int_{Z}log~p(X,Z|\theta)~p(Z|X,\theta^{(t)})dZ$$

由于 $Z$ 是离散型变量，包含了 $N$ 个样本 $\\{z_1,z_2,...,z_N\\}$，因此进一步转化为：

$$Q(\theta,\theta^{(t)})=\int_{Z}log~p(X,Z|\theta)~p(Z|X,\theta^{(t)})dZ\\\=\sum_{Z}[log\prod_{i=1}^Np(x_i,z_i|\theta)\prod_{i=1}^{N}p(z_i|x_i,\theta^{(t)})]\\\=\sum_{z_1,z_2,...,z_N}[(\sum_{i=1}^Nlog~p(x_i,z_i,\theta))(\prod_{i=1}^Np(z_i|x_i,\theta^{(t)}))]\\\=\sum_{z_1,z_2,...,z_N}[(log~p(x_1,z_1|\theta)+...+log~p(x_N,z_N|\theta))](\prod_{i=1}^Np(z_i|x_i,\theta^{(t)}))$$

这就有点复杂了，我们任取其中一项进行分析，看看能否化简：

$$\sum_{z_1,z_2,...,z_N}[log~p(x_1,z_1|\theta)\prod_{i=1}^Np(z_i|x_i,\theta^{(t)})]\\\=\sum_{z_1,z_2,...,z_N}[log~p(x_1,z_1|\theta)p(z_1|x_1,\theta^{(t)})\prod_{i=2}^Np(z_i|x_i,\theta^{(t)})]$$

我们知道有这么一条性质：

$$\sum\sum x_iy_i=\sum x_i\sum y_i$$

因此上式进一步化简为：

$$\sum_{z_1,z_2,...,z_N}[log~p(x_1,z_1|\theta)p(z_1|x_1,\theta^{(t)})\prod_{i=2}^Np(z_i|x_i,\theta^{(t)})]\\\=\sum_{z_1,z_2,...,z_N}[log~p(x_1,z_1|\theta)p(z_1|x_1,\theta^{(t)})]\sum_{z_1,z_2,...,z_N}\prod_{i=2}^Np(z_i|x_i,\theta^{(t)})\\\=\sum_{z_1}[log~p(x_1,z_1|\theta)p(z_1|x_1,\theta^{(t)})]\sum_{z_2,...,z_N}\prod_{i=2}^Np(z_i|x_i,\theta^{(t)})$$

上面的最后一步是依据各个求和等式中的有关项和无关项，化简了求和公式下面的下标。

我们单独看后面一部分：

$$\sum_{z_2,...,z_N}\prod_{i=2}^Np(z_i|x_i,\theta^{(t)})\\\=\sum_{z_2,...,z_N}p(z_2|x_2,\theta^{(t)})p(z_3|x_3,\theta^{(t)})p(z_4|x_4,\theta^{(t)})...p(z_N|x_N,\theta^{(t)})\\\=\sum_{z_2}p(z_2|x_2,\theta^{(t)})\sum_{z_3}p(z_3|x_3,\theta^{(t)})...\sum_{z_N}p(z_N|x_N,\theta^{(t)})$$

对于其中任意一项，由于 $z_i$ 取遍所有的值，根据概率加和为 1 的基本定理，因此有：

$$\sum_{z_i}p(z_i|x_i,\theta^{(t)})=1 \Rightarrow
\sum_{z_2,...,z_N}\prod_{i=2}^Np(z_i|x_i,\theta^{(t)})=1$$

所以：

$$\sum_{z_1,z_2,...,z_N}[log~p(x_1,z_1|\theta)\prod_{i=1}^Np(z_i|x_i,\theta^{(t)})]\\\=\sum_{z_1}[log~p(x_1,z_1|\theta)p(z_1|x_1,\theta^{(t)})]\sum_{z_2,...,z_N}\prod_{i=2}^Np(z_i|x_i,\theta^{(t)})\\\=\sum_{z_1}log~p(x_1,z_1|\theta)p(z_1|x_1,\theta^{(t)})$$

这只是其中一项，那我们所有项都类比地考虑进来，得到 $Q(\theta,\theta^{(t)})$ 最终的化简形式：

$$Q(\theta,\theta^{(t)})\\\=\sum_{z_1,z_2,...,z_N}[(log~p(x_1,z_1|\theta)+...+log~p(x_N,z_N|\theta))](\prod_{i=1}^Np(z_i|x_i,\theta^{(t)}))\\\=\sum_{z_1}[log~p(x_1,z_1|\theta)p(z_1|x_1,\theta^{(t)})+...+\sum_{z_N}[log~p(x_N,z_N|\theta)p(z_N|x_N,\theta^{(t)})\\\=\sum_{i=1}^N\sum_{z_i}[log~(x_i,z_i|\theta)p(z_i|x_i,\theta^{(t)})]$$

那么具体的 $p(x_i,z_i|\theta)$ 和 $p(z_i|x_i,\theta^{(t)})$ 应该表示成什么样呢？

$$p(x|\theta)=\sum_{k=1}^Kp_kN(x|\mu_k,\Sigma_k)$$

$$p(x,z|\theta)=p(z|\theta)p(x|z,\theta)=p_zN(x|\mu_z,\Sigma_z)\quad$$

z 表示取第几个高斯分布。

$$p(z|x,\theta)=\frac{p(x,z|\theta)}{p(x|\theta)}=\frac{p_zN(x|\mu_z,\Sigma_z)}{\sum_{k=1}^Kp_kN(x|\mu_k,\Sigma_k)}$$

最终代入到：

$$Q(\theta,\theta^{(t)})=\sum_{i=1}^N\sum_{z_i}[log~p(x_i,z_i|\theta)p(z_i|x_i,\theta^{(t)})]\\\=\sum_{i=1}^N\sum_{z_i}log~[p_{z_i}N(x_i|\mu_{z_i},\Sigma_{z_i})]\frac{p_{z_i}^{(t)}N(x_i|\mu_{z_i}^{(t)},\Sigma_{z_i}^{(t)})}{\sum_{k=1}^Kp_k^{(t)}N(x_i|\mu_{k}^{(t)},\Sigma_{k}^{(t)})}$$

这就是在第 $t$ 轮向第 $t+1$ 轮迭代的 E 步，所有带上标 $(t)$ 的参数都是已知的，都是上一轮迭代出来的结果。

为了书写方便，我们还是简记作：

$$p(z_i|x_i,\theta^{(t)})=\frac{p_{z_i}^{(t)}N(x_i|\mu_{z_i}^{(t)},\Sigma_{z_i}^{(t)})}{\sum_{k=1}^Kp_k^{(t)}N(x_i|\mu_{k}^{(t)},\Sigma_{k}^{(t)})}$$

最终我们得到了 E 步的表达式：

$$Q(\theta,\theta^{(t)})=\sum_{i=1}^N\sum_{z_i}log~[p_{z_i}N(x_i|\mu_{z_i},\Sigma_{z_i})]p(z_i|x_i,\theta^{(t)})\\\=\sum_{z_i}\sum_{i=1}^Nlog~[p_{z_i}N(x_i|\mu_{z_i},\Sigma_{z_i})]p(z_i|x_i,\theta^{(t)})\\\=\sum_{k=1}^K\sum_{i=1}^Nlog~[p_{k}N(x_i|\mu_{k},\Sigma_{k})]p(z_i=C_k|x_i,\theta^{(t)})\\\=\sum_{k=1}^K\sum_{i=1}^N[log~p_{k}+log~N(x_i|\mu_{k},\Sigma_{k})]p(z_i=C_k|x_i,\theta^{(t)})$$

倒数第二步就是把 $z_i$ 取遍所有取值 $\\{C_1,C_2,...,C_K\\}$ 换了一种写法，本质没有变化。

完成了 E 步，下一步就是 M 步，就是去求使得：

$$Q(\theta,\theta^{(t)})=\sum_{k=1}^K\sum_{i=1}^N[log~p_{k}+log~N(x_i|\mu_{k},\Sigma_{k})]p(z_i=C_k|x_i,\theta^{(t)})$$

取值最大的参数 $\theta$，作为 $\theta^{(t+1)}$ 的取值，具体化就是
$p_k^{(t+1)},\mu_k^{(t+1)},\Sigma_k^{(t+1)}$。

### 参数估计实际举例说明

这里我们以参数 $p_k$ 的估计举例说明，大家可以从中明白手工计算的过程，实际上这个还是非常复杂的。

首先我们要明确一点：$p_k$ 中的下标 $k$ 是一个变量，$p_k$ 指代了 $\\{p_1,p_2,p_3,...,p_K\\}$
中的任意一个，$K$ 对应的就是高斯混合模型中高斯分布的个数。

$$p_{k}^{(t+1)}=argmax_{p_k}\sum_{k=1}^K\sum_{i=1}^N[log~p_{k}+log~N(x_i|\mu_{k},\Sigma_{k})]p(z_i=C_k|x_i,\theta^{(t)})$$

我们简化一下，$log~N(x_i|\mu_{k},\Sigma_{k})$ 和待估计参数 $p_k$ 无关，因此可以略去：

$$p_{k}^{(t+1)}=argmax_{p_k}\sum_{k=1}^K\sum_{i=1}^N[log~p_{k}+log~N(x_i|\mu_{k},\Sigma_{k})]p(z_i=C_k|x_i,\theta^{(t)})\\\=argmax_{p_k}\sum_{k=1}^K\sum_{i=1}^Nlog~p_{k}p(z_i=C_k|x_i,\theta^{(t)})$$

而这里的求最值问题是一个带约束的优化问题：

$$\sum_{k=1}^Kp_k=1$$

因此这个带约束的优化问题就可以用我们熟悉的拉格朗日乘子法来实现：

$$l(p,\lambda)=\sum_{k=1}^K\sum_{i=1}^Nlog~p_{k}p(z_i=C_k|x_i,\theta^{(t)})+\lambda(\sum_{k=1}^Kp_k-1)$$

在 $\frac{\partial}{\partial p_k}l(p,\lambda)$ 的求导运算中，我们是逐一求取每一个 $p_k$
的偏导数和最值，而 $l(p,\lambda)$ 里包含的是所有的 $p_k:\\{p_1,p_2,...,p_K\\}$，比如我们求 $p_3$
的偏导，那么其他的 $\\{p_1,p_2,p_4,p_5,...,p_K\\}$ 就都与 $p_3$ 的求导运算无关，因此：

$$\frac{\partial}{\partial
p_k}\sum_{k=1}^K\sum_{i=1}^Nlog~p_{k}p(z_i=C_k|x_i,\theta^{(t)})+\lambda(\sum_{k=1}^Kp_k-1)\\\=\frac{\partial}{\partial
p_k}\sum_{i=1}^Nlog~p_{k}p(z_i=C_k|x_i,\theta^{(t)})+\lambda(p_k-1)=0\\\\\Rightarrow
\sum_{i=1}^N\frac{1}{p_k}p(z_i=C_k|x_i,\theta^{(t)})+\lambda=0\\\\\Rightarrow
\sum_{i=1}^Np(z_i=C_k|x_i,\theta^{(t)})+p_k\lambda=0$$

这里我们再次施加一点运算技巧，对加号两边的部分同时加上 $\sum_{k=1}^K$ 求和运算：

$$\sum_{i=1}^N\sum_{k=1}^Kp(z_i=C_k|x_i,\theta^{(t)})+\sum_{k=1}^Kp_k\lambda=0$$

这里就初现端倪了，首先：

$$\sum_{k=1}^Kp_k=1$$

同时：

$$\sum_{k=1}^Kp(z_i=C_k|x_i,\theta^{(t)})$$

在这里，$z_i$ 也是取遍了所有的 $K$ 个高斯分布，因此：

$$\sum_{k=1}^Kp(z_i=C_k|x_i,\theta^{(t)})=1$$

$$\sum_{i=1}^N\sum_{k=1}^Kp(z_i=C_k|x_i,\theta^{(t)})+\sum_{k=1}^Kp_k\lambda=0\\\\\Rightarrow
\sum_{i=1}^N1+\lambda=0\Rightarrow N+\lambda=0 \Rightarrow \lambda=-N$$

最终我们得到 $p_k^{(t+1)}$ 的估计值：

$$\sum_{i=1}^Np(z_i=C_k|x_i,\theta^{(t)})+p_k\lambda=0\\\\\Rightarrow
p_k^{(t+1)}=\frac{1}{N}\sum_{i=1}^Np(z_i=C_k|x_i,\theta^{(t)})$$

其中：

$$p(z_i|x_i,\theta^{(t)})=\frac{p_{z_i}^{(t)}N(x_i|\mu_{z_i}^{(t)},\Sigma_{z_i}^{(t)})}{\sum_{k=1}^Kp_k^{(t)}N(x_i|\mu_{k}^{(t)},\Sigma_{k}^{(t)})}$$

剩下的参数 $\mu$ 和 $\Sigma$ 我们就不这么一点点推导了，方法也是分别对 $\mu_k$ 和 $\Sigma_k$
求偏导，而且没有约束条件。不过说实话确实都还比较复杂，我们以 $p_k$ 的参数迭代估计的过程也是为了让大家在高斯混合模型的背景下，进一步了解 EM
方法的精髓。

这里，我们直接给出 $\mu_k^{(t+1)}$ 和 $\Sigma_k^{(t+1)}$ 的迭代公式：

$$\mu_k^{(t+1)}=\frac{\sum_{i=1}^Np(z_i=C_k|x_i,\theta^{(t)})x_i}{\sum_{i=1}^Np(z_i=C_k|x_i,\theta^{(t)})}$$

$$\Sigma_k^{(t+1)}=\frac{\sum_{i=1}^Np(z_i=C_k|x_i,\theta^{(t)})(x_i-\mu_k)(x_i-\mu_k)^T}{\sum_{i=1}^Np(z_i=C_k|x_i,\theta^{(t)})}$$

有了这三组参数的迭代公式，我们在初始值的基础上，不断迭代最终就能成功收敛到实际的参数。实际工作中如果让大家这么手推肯定是灾难性的，放心都有第三方库能够帮忙完成所有参数的估计。

### 高斯混合模型的应用场景

不过在展示 Python
代码之前，我猜想大家肯定有个疑问，啰啰嗦嗦搞了两讲的内容了，对，我们花了很大的代价，利用高斯混合模型描述了这些样本点的分布情况，并且成功地利用 EM
迭代的方法，迭代求取了模型的参数，那么我们想干嘛呢？

当我们获取了这个高斯混合模型的所有参数之后，我们就可以求出每个样本属于哪一类，也就是属于哪一个高斯分布。思路和软分类问题是一样的，我们分别计算样本属于
$K$ 个高斯分布的概率，然后选择概率值最高的那个高斯分布作为它的分类。

则样本 $x_i$ 属于第 $k$ 个高斯分布的概率如下：

$$p(z_i=C_k|x_i)=\frac{p(z_i=C_k)p(x_i|z_i=C_k)}{p(x_i)}\\\\\propto
p(z_i=C_k)p(x_i|z_i=C_k)=p_kN(x_i|\mu_k,\Sigma_k)$$

相当于说，样本 $x_i$
对于高斯混合模型中的每一个高斯分布，都能计算出这么一个概率值，那么，哪一个高斯分布的概率最大，这个样本就属于哪一个高斯分布，也就是说属于哪一个类别。

### 模型的代码试验

**代码片段：**

    
    
    import matplotlib.pyplot as plt
    import seaborn as sns; sns.set()
    from sklearn.mixture import GaussianMixture
    from sklearn.datasets.samples_generator import make_blobs
    
    #产生并绘制实验数据
    X, y_true = make_blobs(n_samples=1000, centers=4)
    fig, ax = plt.subplots(1,2,sharey='row')
    ax[0].scatter(X[:, 0], X[:, 1], s=5, alpha=0.5)
    
    #高斯混合模型拟合样本
    gmm = GaussianMixture(n_components=4)
    gmm.fit(X)
    
    #打印GMM三组参数：权重、均值、协方差矩阵
    print(gmm.weights_)
    print(gmm.means_)
    print(gmm.covariances_)
    
    #打印每个样本属于每个高斯分布的概率
    print(gmm.predict_proba(X)[:10].round(5))
    
    #通过GMM模型推测每个样本所属的类别
    labels = gmm.predict(X)
    print(labels)
    #不同的类别标记为不同的颜色
    ax[1].scatter(X[:, 0], X[:, 1], s=5, alpha=0.5, c=labels, cmap='viridis')
    plt.show()
    

**运行结果：**

    
    
    各分布的权重：
    [0.25148671 0.25       0.24989449 0.2486188 ]
    各分布的均值：
    [[-5.19121377  9.86024381]
     [-3.54084266 -4.68931779]
     [ 2.03148194  3.78611289]
     [-4.16132022  5.64705763]]
     各分布的协方差矩阵：
    [[[ 0.82119643  0.01901415]
      [ 0.01901415  0.98944862]]
     [[ 0.96865844 -0.03162233]
      [-0.03162233  0.87747708]]
     [[ 1.03296444  0.01892392]
      [ 0.01892392  1.01083161]]
     [[ 1.056754    0.01604062]
      [ 0.01604062  1.0460872 ]]]
      样本点属于每个分布的概率（取前十个）：
    [[3.0000e-05 0.0000e+00 0.0000e+00 9.9997e-01]
     [0.0000e+00 0.0000e+00 1.0000e+00 0.0000e+00]
     [9.9840e-01 0.0000e+00 0.0000e+00 1.6000e-03]
     [0.0000e+00 0.0000e+00 9.9861e-01 1.3900e-03]
     [9.9993e-01 0.0000e+00 0.0000e+00 7.0000e-05]
     [3.0000e-05 0.0000e+00 0.0000e+00 9.9997e-01]
     [9.2662e-01 0.0000e+00 0.0000e+00 7.3380e-02]
     [0.0000e+00 0.0000e+00 1.0000e+00 0.0000e+00]
     [0.0000e+00 0.0000e+00 1.0000e+00 0.0000e+00]
     [0.0000e+00 1.0000e+00 0.0000e+00 0.0000e+00]]
     每个样本点所属的类别：
    [3 2 0 2 0 3 0 2 2 1 2 1 1 3 3 1 0 1 1 1 0 0 1 2 3 1 0 2 0 1 0 3 3 3 3 3 0
     0 2 3 0 0 0 0 2 0 0 1 1 0 0 1 1 1 1 1 3 3 1 3 3 1 2 3 1 2 3 3 3 0 3 3 0 1
     0 0 2 0 1 1 1 2 3 3 2 3 2 2 0 1 1 0 0 3 0 3 3 1 3 1 2 0 2 0 2 3 2 3 1 1 3
     1 2 2 0 0 3 0 2 1 2 1 3 2 1 0 2 3 3 0 0 2 2 2 1 1 0 1 2 3 3 0 1 0 2 0 0 0
     3 0 2 0 2 3 0 3 1 2 2 2 0 0 1 0 1 0 2 1 1 1 1 2 3 0 3 1 2 2 0 3 2 3 2 1 2
     0 3 2 3 2 3 2 2 3 3 1 3 3 3 0 3 2 2 2 3 3 1 2 2 2 0 3 2 0 2 3 3 1 0 2 2 3
     3 1 0 3 0 1 0 1 1 1 3 2 0 1 2 2 3 1 1 0 0 2 2 1 1 0 1 0 3 0 0 0 0 2 0 2 3
     2 2 2 0 2 2 1 2 3 2 3 1 3 1 0 3 0 0 3 1 1 1 2 3 1 2 1 0 2 0 2 1 2 2 3 2 3
     3 2 3 0 1 1 0 0 0 0 2 1 2 2 3 1 2 1 2 1 0 3 3 1 1 1 1 1 2 3 2 1 3 2 2 1 0
     2 2 1 0 2 1 2 1 3 2 1 0 2 3 2 3 1 2 0 1 0 3 1 0 2 0 1 0 3 2 0 3 0 3 1 3 3
     2 2 2 1 2 3 3 1 2 0 0 3 0 2 2 2 1 1 2 1 3 2 1 0 2 2 3 3 1 1 0 3 1 2 3 2 2
     3 0 0 3 2 1 3 1 1 3 2 1 3 3 3 3 3 2 3 2 3 2 2 0 2 0 0 2 1 3 1 1 3 2 2 1 0
     0 3 0 0 2 1 0 1 2 1 2 1 2 2 0 0 3 2 3 0 0 1 0 1 1 1 2 1 2 2 2 3 3 0 2 2 0
     2 0 1 1 2 0 3 0 2 0 0 1 3 2 3 3 3 0 1 2 3 1 1 3 0 2 1 2 2 2 1 0 2 3 0 2 0
     2 3 3 1 2 3 1 1 0 3 0 2 3 0 3 2 1 3 3 3 2 3 3 0 0 0 0 1 3 2 2 0 0 3 3 0 0
     3 0 3 0 2 3 1 0 3 3 2 0 2 1 0 0 2 3 3 0 3 1 3 0 0 2 1 2 2 3 1 1 0 1 3 2 1
     0 3 0 1 2 1 3 3 0 3 1 2 3 3 1 2 2 1 3 3 2 3 0 2 2 3 3 3 0 1 0 0 0 3 2 3 0
     1 0 0 3 0 3 2 3 1 3 2 2 1 2 2 1 0 1 0 3 2 3 3 2 1 3 1 1 3 3 0 1 3 1 3 1 2
     2 1 1 2 3 1 2 2 3 0 2 1 1 3 0 3 1 1 3 2 1 3 0 1 0 3 0 0 0 2 3 3 3 2 2 3 0
     1 0 2 1 0 3 1 2 0 3 2 2 2 3 1 1 3 0 1 3 3 1 2 1 2 1 3 1 0 0 1 3 0 2 3 2 1
     1 1 3 1 3 2 0 1 3 0 0 1 1 0 1 1 2 0 1 0 1 2 0 2 0 3 2 3 3 3 3 3 0 1 1 0 3
     1 0 1 1 3 1 0 0 3 2 1 1 3 3 1 2 3 1 0 2 0 2 1 1 0 0 1 1 1 0 3 2 2 2 0 0 1
     2 0 3 1 0 2 1 2 0 0 0 1 2 3 1 0 3 2 1 0 0 3 0 3 2 1 2 1 3 3 0 1 1 1 1 2 1
     1 0 1 1 0 2 3 0 1 0 1 2 0 1 1 3 0 1 3 2 1 0 1 1 0 3 3 2 1 2 0 2 3 3 1 1 2
     1 1 3 2 0 2 1 2 0 0 0 2 3 0 2 3 2 3 0 0 3 0 3 1 0 1 0 0 2 0 1 0 1 2 2 2 0
     3 2 0 0 1 3 3 1 0 2 2 2 0 2 2 0 3 0 3 0 2 3 3 0 2 3 3 3 1 3 3 2 0 1 1 2 1
     2 0 1 1 0 1 3 1 3 1 0 1 2 3 0 0 2 3 2 0 3 2 0 0 2 1 0 3 0 0 3 2 0 3 2 3 1
     0]
    

![图1
高斯混合模型试验用图](https://images.gitbook.cn/88ea2580-8e6c-11ea-86f7-9b5ee2f8f4da)

我们简单地解读一下代码：

  * 首先我们利用 sklearn 中提供的方法，生成了一些样本数据，我们发现应该用 4 个高斯分布组成的高斯混合模型去描述这个数据分布。
  * 于是，我们通过指定参数为 4 个高斯分布的高斯混合模型去拟合这些样本，并且分别打印出了这 4 个分布各自的权重、均值和协方差矩阵。
  * 最后，我们示意了前 10 个样本点基于 GMM 模型判定其属于各个高斯分布的概率，哪个分布的概率最大，它就最终判定属于哪个高斯分布，最终打印出我们推测出的所有样本点的类别，并依据类别通过不同的颜色对其进行可视化。

