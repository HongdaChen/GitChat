在这个专栏中，我们开篇首先介绍高斯分布，它的重要性体现在两点：

  * 第一：依据中心极限定理，当样本量足够大的时候，任意分布的均值都趋近于一个高斯分布，这是在整个工程领域体现出该分布的一种普适性；
  * 第二：高斯分布是后续许多模型的根本基础，例如线性高斯模型（卡尔曼滤波）、高斯过程等等。

因此我们首先在这一讲当中，结合一元高斯分布，来讨论一下极大似然估计，估计有偏性、无偏性等基本建模问题。

### 极大似然估计问题背景

首先我们来回顾一下一元高斯分布的概率密度函数 $pdf$，假设我们有一组观测样本数据 $X=(x_1,x_2,x_3,...,x_N)$，它们服从参数
$\theta=(\mu,\sigma^2)$ 的一元高斯分布，那么我们如何利用这组样本来对分布的参数 $\theta$
进行估计呢，换句话说也就是估计出样本所服从高斯分布的均值和方差？

这里需要使用我们非常熟悉的极大似然估计方法，我们首先来看一下一元高斯分布的概率密度函数的表达式：

$$p(x)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(x-\mu)^2}{2\sigma^2})$$

在之前的《机器学习中的数学：概率统计》专栏中，我们曾经介绍过，极大似然估计的本质就是估计出模型的参数 $\theta$，使得我们所观测出的这组样本
$X=(x_1,x_2,x_3,...,x_N)$ 出现的概率最大。

这里我们还要熟悉另一个表达式的写法，那就是 $p(x|\theta)$，我们常常能够看到这个条件概率的写法，它在这里就是指明确了参数
$\theta$（也就是
$\mu,\sigma^2$）的情况下，服从高斯分布的样本$x$出现的概率，那不难看出，实际上这个表达式最终的写法和上面的概率密度函数形式上是一样的：

$$p(x|\theta)=\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(x-\mu)^2}{2\sigma^2})$$

那么对于服从高斯分布的这一组样本 $X=(x_1,x_2,x_3,...,x_N)$ 而言，我们的目标是估计出（换做更直白的语言，就是求出）这个分布的参数
$\theta$，也就是 $\mu,\sigma^2$，使得在这个高斯分布的框架下，这组样本出现的概率最大，也就是 $p(X|\theta)$
的概率值最大，这样就确定了 $p(X|\theta)$ 是我们的求取参数的优化目标了。

### 极大似然估计的求法

由于这一组样本 $X=x_1,x_2,x_3,...,x_N$ 中的每一个样本 $x_i$
都是独立同分布的，即满足同一个高斯分布，并且彼此之间相互独立，那么依据随机变量独立的性质，优化目标 $p(X|\theta)$ 进一步被变化为：

$$p(X|\theta)=\prod_{i=1}^np(x_i|\theta)$$

我们的目标就是找到一组参数$\theta$，使得上述概率值取得最大。同时，为了能够简化后续计算，我们对目标函数取对数，一方面因为对数满足单调递增，另一方面对数函数能够使得连乘运算变化为连加运算，可以说是一个不错的选择：

$$log \,p(X|\theta)=log\prod_{i=1}^np(x_i|\theta)=\sum_{i=1}^nlog
\,p(x_i|\theta)$$

我们进一步带入概率密度函数的表达式：

$$\sum_{i=1}^nlog\,p(x_i|\theta)=\sum_{i=1}^nlog\frac{1}{\sqrt{2\pi}\sigma}exp(-\frac{(x_i-\mu)^2}{2\sigma^2})\\\=\sum_{i=1}^n[log\frac{1}{\sqrt{2\pi}}+log\frac{1}{\sigma}-\frac{(x_i-\mu)^2}{2\sigma^2}]$$

那么，现在目标明确了，我们来寻找最佳的 $\mu_{mle}$ 和 $\sigma_{mle}$，使得：

$$\sum_{i=1}^n[log\frac{1}{\sqrt{2\pi}}+log\frac{1}{\sigma}-\frac{(x_i-\mu)^2}{2\sigma^2}]$$

取得最大。

我们对这两个参数，一个个来看，具体处理方法很简单，就是求使得上面式子偏导数为 $0$ 的 $\mu$ 和 $\sigma$ 取值。

先看参数 $\mu$：

$$argmax\,\mu_{mle}=argmax_{\mu}\sum_{i=1}^n[log\frac{1}{\sqrt{2\pi}}+log\frac{1}{\sigma}-\frac{(x_i-\mu)^2}{2\sigma^2}]$$

由于前面两项都与 $\mu$ 无关，最终：

$$argmax\,\mu_{mle}=argmax_{\mu}\sum_{i=1}^n[-\frac{(x_i-\mu)^2}{2\sigma^2}]\\\=argmin_{\mu}\sum_{i=1}^n[(x_i-\mu)^2]$$

后面的就很好办了，直接求 $\mu$ 的偏导即可：

$$\frac{\partial}{\partial
\mu}\sum_{i=1}^{N}(x_i-\mu)^2=\sum_{i=1}^{N}2(x_i-\mu)(-1)=0$$

$$\sum_{i=1}^{N}2(x_i-\mu)(-1)=
\sum_{i=1}^{N}(x_i-\mu)\\\=\sum_{i=1}^{N}x_i-\sum_{i=1}^{N}\mu=\sum_{i=1}^{N}x_i-N\mu=0$$

最终我们得出：

$$\mu_{mle}=\frac{1}{N}\sum_{i=1}^{N}x_i$$

也就是说，样本的均值就是高斯分布参数 $\mu$ 的极大似然估计值。

同样地，我们再看如何利用样本数据对高斯分布的方差进行极大似然估计：

$$argmax\,\sigma_{mle}=argmax_{\sigma}\sum_{i=1}^n[log\frac{1}{\sqrt{2\pi}}+log\frac{1}{\sigma}-\frac{(x_i-\mu)^2}{2\sigma^2}]\\\=argmax_{\sigma}\sum_{i=1}^n[log\frac{1}{\sigma}-\frac{(x_i-\mu)^2}{2\sigma^2}]$$

同样地，我们还是利用求偏导的方法来解决问题：

$$\frac{\partial}{\partial
\mu}\sum_{i=1}^n[log\frac{1}{\sigma}-\frac{(x_i-\mu)^2}{2\sigma^2}]=\sum_{i=1}^n[-\frac{1}{\sigma}+\frac{1}{2}(x_i-\mu)^22\sigma^{-3}]\\\=\sum_{i=1}^n[-\sigma^2+(x_i-\mu)^2]=\sum_{i=1}^n-\sigma^2+\sum_{i=1}^n(x_i-\mu)^2=0$$

因此，最终我们也得出了高斯分布方差的极大似然估计值：

$$\sigma^2_{mle}=\frac{1}{N}\sum_{i=1}^n(x_i-\mu_{mle})^2$$

### 参数估计的有偏性和无偏性

我们在前面已经花了较大的篇幅，通过样本 $X={x_1,x_2,x_3,...,x_N}$ 求得了高斯分布两个参数 $\mu$ 和 $\sigma^2$
的极大似然估计值，那么此时我们自然就要关心了：

通过这种求解方法得到的参数估计值，它和模型参数的真实值有没有差距？我们如何来衡量极大似然估计值的性质？

这里我们来回顾一下无偏估计的概念：如果估计量的期望等于被估计参数的真实值，那么这个估计值就满足无偏性。那么从高斯分布的样本中推断出来的均值和方差的极大似然估计
$\mu_{mle}$ 和 $\sigma^2_{mle}$ 是否满足无偏性呢？

### 无偏性的验证

首先我们还是从均值的极大似然估计入手，我们对 $\mu_{mle}$ 求期望：

$$E[\mu_{mle}]=E[\frac{1}{N}\sum_{i=1}^nx_i]=\frac{1}{N}\sum_{i=1}^nE[x_i]=\frac{1}{N}\sum_{i=1}^n\mu=\mu$$

我们看到 $E[\mu_{mle}]=\mu$，即估计值的期望等于模型参数的真实值，因此均值的极大似然估计 $\mu_{mle}$ 是无偏的。

而方差的情况就要特殊一些，我们通过下面的公式推导就会看到方差的极大似然估计并不是无偏的，这个推导过程比较复杂，我们一点一点来解析：

$$\sigma_{mle}^2=\frac{1}{N}\sum_{i=1}^N(x_i-\mu_{mle})^2=\frac{1}{N}\sum_{i=1}^N(x_i^2+\mu_{mle}^2-2x_i\mu_{mle})\\\=\frac{1}{N}\sum_{i=1}^Nx_i^2+\frac{1}{N}\sum_{i=1}^N\mu_{mle}^2-\frac{1}{N}\sum_{i=1}^N2x_i\mu_{mle}\\\=\frac{1}{N}\sum_{i=1}^Nx_i^2+\frac{1}{N}\sum_{i=1}^N\mu_{mle}^2-2\mu_{mle}\frac{1}{N}\sum_{i=1}^Nx_i$$

我们发现最后一项当中的 $\frac{1}{N}\sum_{i=1}^Nx_i$ 就是模型均值的极大似然估计 $\mu_{mle}$，因此我们稍作代换就有：

$$\sigma_{mle}^2=\frac{1}{N}\sum_{i=1}^Nx_i^2+\frac{1}{N}\sum_{i=1}^N\mu_{mle}^2-2\mu_{mle}\frac{1}{N}\sum_{i=1}^Nx_i\\\=\frac{1}{N}\sum_{i=1}^Nx_i^2+\mu_{mle}^2-2\mu_{mle}^2=\frac{1}{N}\sum_{i=1}^Nx_i^2-\mu_{mle}^2$$

因此，我们的目标就明确为求取下面式子的期望：

$$E[\sigma_{mle}^2]=E[\frac{1}{N}\sum_{i=1}^Nx_i^2-\mu_{mle}^2]$$

这个期望的求取有一定的技巧，我们引入模型均值的真实值 $\mu$：

$$E[\sigma_{mle}^2]=E[\frac{1}{N}\sum_{i=1}^Nx_i^2-\mu^2-(\mu_{mle}^2-\mu^2)]\\\=E[\frac{1}{N}\sum_{i=1}^Nx_i^2-\mu^2]-E[\mu_{mle}^2-\mu^2]$$

这前后两个期望的式子，我们分开来看，首先看 $E[\frac{1}{N}\sum_{i=1}^Nx_i^2-\mu^2]$：

$$E[\frac{1}{N}\sum_{i=1}^Nx_i^2-\mu^2]=E[\frac{1}{N}\sum_{i=1}^Nx_i^2-\frac{1}{N}\sum_{i=1}^N\mu^2]\\\=\frac{1}{N}E[\sum_{i=1}^N(x_i^2-\mu^2)]=\frac{1}{N}\sum_{i=1}^NE[(x_i^2-\mu^2)]$$

这个式子有何端倪？我们仔细观察一下变换后的形式：

$$E[(x_i^2-\mu^2)]=E[x_i^2]-\mu^2=E[x_i^2]-E[x_i]^2=var(x_i)=\sigma^2$$

经过这么一变换，发现 $E[x_i^2]-E[x_i]^2$ 就是方差的定义，因此 $E[(x_i^2-\mu^2)]$ 表示的就是方差的实际值
$\sigma^2$，前面一部分期望的结果就是：

$$E[\frac{1}{N}\sum_{i=1}^Nx_i^2-\mu^2]=\frac{1}{N}\sum_{i=1}^N\sigma^2=\sigma^2$$

我们再看后半部分 $E[\mu_{mle}^2-\mu^2]$，对于这一部分的处理，也有颇多技巧：

$$E[\mu_{mle}^2-\mu^2]=E[\mu_{mle}^2]-E[\mu^2]=E[\mu_{mle}^2]-\mu^2$$

之前说过了，由于均值的极大似然估计是无偏估计，因此 $\mu=E[\mu_{mle}]$，我们进行替换可以得到：

$$E[\mu_{mle}^2]-\mu^2=E[\mu_{mle}^2]-E[\mu_{mle}]^2=var(\mu_{mle})\\\=var(\frac{1}{N}\sum_{i=1}^Nx_i)=\frac{1}{N^2}var(\sum_{i=1}^Nx_i)=\frac{1}{N^2}\sum_{i=1}^Nvar(x_i)\\\=\frac{1}{N^2}\sum_{i=1}^N\sigma^2=\frac{1}{N}\sigma^2$$

真是千回百转啊，我们现在把前后两个化简后的期望结果进行合并，就可以得到方差的极大似然估计值的期望值了：

$$E[\sigma_{mle}^2]=E[\frac{1}{N}\sum_{i=1}^Nx_i^2-\mu^2]-E[\mu_{mle}^2-\mu^2]\\\=\sigma^2-\frac{1}{N}\sigma^2=\frac{N-1}{N}\sigma^2$$

结果出来了，我们发现方差的极大似然估计值并不等于该参数的真实值 $\sigma^2$，而是要偏小的，因此方差的极大似然估计是有偏的。

那么，如果想要它变得无偏，进行一下系数的简单修正，在前面乘上一个 $\frac{N}{N-1}$ 即可：

$$\hat{\sigma^2}=\frac{N}{N-1}\sigma^2_{mle}=\frac{N}{N-1}\frac{1}{N}\sum_{i=1}^N(x_i-\mu_{mle})^2\\\=\frac{1}{N-1}\sum_{i=1}^N(x_i-\mu_{mle})^2$$

### 小结

这一讲的内容就到此为止，核心内容就是以一元高斯分布为例，围绕着极大似然估计的方法，来介绍如何对高斯分布的参数进行估计，并且高斯分布的极大似然参数估计值的有偏性进行了判断和验证，结论和思想都很简单，希望读者能够多注意其中的运算技巧。

