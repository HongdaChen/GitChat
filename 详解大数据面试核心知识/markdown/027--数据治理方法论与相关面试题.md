在前几年大数据发展的初期，各个公司都在风风火火地搞平台，搞数仓，接入各种数据，在那个阶段注重的是数据的存储和计算，慢慢地一年一年的数据积累下来，数据量大了之后人们开始关注质量的问题，这也是数据治理这个词这几年变得非常火的原因，平台那一套东西已经很成熟了，基本上即插即用，能满足大部分不太刁钻的需求了。所以，现在各个企业都会把保证平台的稳定，保证数据的高质量放在第一位，毕竟用于指导决策的数据是不容许出现大错误的，所以数据治理也就成了数仓建设非常重要的一个环节。

**本篇面试内容划重点：元数据管理、数据质量、数据安全。**

### 元数据管理

元数据即数据的数据，数据仓库的特点是数据种类多、数量大，相应的元数据也有很多，如果没有一个集中管理元数据的地方，在使用上会非常不便。元数据可以说是数据治理的基础，基于元数据我们还可以做很多的相关应用，比如血缘分析、数据资产地图、数据质量管理等等。

元数据分为业务元数据、技术元数据和操作元数据，三者之间关系紧密。业务元数据指导技术元数据，技术元数据以业务元数据为参考进行设计，操作元数据为两者的管理提供支撑。

**业务元数据**

业务元数据是定义和业务相关数据的信息，用于辅助定位、理解及访问业务信息。业务元数据的范围主要包括

  * 指标名称、计算口径、业务术语解释、衍生指标等
  * 业务规则引擎的规则、数据质量检测规则、数据挖掘算法等
  * 数据的安全或敏感级别等

**技术元数据**

它可以分成结构性技术元数据和关联性技术元数据。结构性技术元数据提供了在信息技术的基础架构中对数据的说明，如数据的存放位置、数据的存储类型等；关联性技术元数据描述了数据之间的关联和数据在信息技术环境之中的流转情况。技术元数据的范围主要包括：

  * 数据库表的 Schema 信息，包括表名、列名称、列属性、备注、约束信息等
  * 数据存储类型、位置、数据存储文件格式或数据压缩类型等
  * 数据访问权限、组和角色
  * 字段级血缘关系、ETL 抽取加载转换信息
  * 调度依赖关系、进度和数据更新频率

**操作元数据**

操作元数据主要指与元数据管理相关的组织、岗位、职责、流程，以及系统日常运行产生的操作数据。

  * 系统执行日志
  * 访问模式、访问频率和执行时间
  * 调用的业务名称和描述
  * 备份、归档时间、归档存储、版本信息

元数据管理目前比较流行的开源方案是 Apache
Atlas，它提供了包括数据分类、集中策略引擎、数据血缘、安全和生命周期管理在内的元数据治理核心能力。Atlas
整合了多种异构数据源元数据的采集，可以构建大数据平台完整的数据资产目录，并对这些数据资产进行分类和管理，展示字段级的数据血缘关系图，为大数据的从业人员提供元数据服务。另外
Atlas 还与 Apache Ranger 整合，实现了数据权限控制策略。

### 数据质量

#### **数据质量管理需要检测的异常情况？**

监控各种数据异常情况并告警，典型的问题有这些：

**数据缺失**

比如：丢数据和字段缺失。

数据缺失的问题可以统计大数据平台数据流过程各个节点的数据量，从采集、到 Kafka、到清洗、到入库等等，监控每一步的数据量，然后发送到 opentsdb
存储，在 grafana 上展示。在整条链路对比中，很容易看出是不是在哪个环节丢失了数据。

**业务层面的数据异常**

比如：数据分布异常；数值异常。

业务层面的异常没法做到非常细粒度的监控，毕竟对于业务的理解我们确实还是不如业务方的，如果是游戏或者电商或者其他的 App
业务都有可能遇到各种活动，活动期间的数据值是很异常的， **但是对于业务来说又是正常的**
。所以业务监控起到的是辅助的作用，是粗粒度的。比较常规的办法是计算一些环比和同比的数值，如果数值差异较大则很可能出现了问题，要提醒业务方或者对平台全局的监控做一个排查，看看是不是哪个步骤出了问题。

**工程层面的数据异常**

比如：数据类型异常、大量 null、格式异常等等。

工程层面上的数据异常可以在数据清洗的过程中，即写 Spark，或者 HiveSQL
的代码里加个异常解析的小模块。也可以做事后的异常检测，这样可以更方便地自定义和管理监控规则，发现异常则告警。

数据质量目前比较流行的开源方案是 Apache Griffin，Griffin
是属于模型驱动的方案，基于目标数据集合或者源数据集（基准数据），用户可以选择不同的数据质量维度来执行目标数据质量的验证。支持两种类型的数据源：batch
数据和 streaming 数据。对于 batch 数据，可以通过数据连接器从 Hadoop 平台收集数据。对于 streaming 数据，可以连接到
Kafka 等消息队列来做实时数据分析。在拿到数据之后，模型引擎将在 Spark 集群中根据规则计算数据质量。

### 数据安全

这里的数据安全指的是数据使用的安全问题。

  * **数据权限** ：数据权限的控制需要借助一些开源组件工具来做比如 Apache Sentry/Apache Ranger，这两者大同小异，都是基于角色的访问控制（role-based access control，RBAC），也就是说，当新来一个用户时，我们会给他赋予角色而不是权限，然后这个用户的执行权限就是这个角色所拥有的权限。基于角色的访问控制，能够大大减轻系统对于大数据量用户的直接 ACL 控制。
  * **数据脱敏** ：敏感数据的处理。在做测试或者做一些分析的时候，可能会用到线上的真实数据，但是这些数据的某些字段是敏感的，比如身份证、手机号等等，所以需要对原始数据做个预处理，屏蔽掉敏感字段。
  * **系统可用性** ：上面说的是人与数据之间的安全问题，实际上系统与数据也存在安全问题。数据可能会由于系统的故障导致丢失，也可能由于系统权限的缺失和不完善导致数据被盗取或者篡改等等问题。

数据质量目前比较流行的开源方案有 Apache Sentry，Sentry 提供了给用户定义和配置访问规则的方法，当数据访问工具，例如
Hive，收到用户访问数据的请求，例如从一个表读一行数据或者删除一个表。Hive 会请求 Sentry 验证访问是否合理。Sentry
构建请求用户被允许的权限的映射并判断给定的请求是否允许访问。Hive 这时候根据 Sentry 的判断结果来允许或者禁止用户的访问请求。

