数据同步是每个大数据人都绕不开的工作，因为大数据的存储组件太多了，数据常常因为各种需求需要从一个地方导到另一个地方，如果是数据量小，可能我们写个普通的脚本就可以完成，但是大数据的场景下，我们不得不借助一些工具来达到我们同步海量数据的目的。说实话
ETL
工具实在是太多了，而且很多都可以满足我们日常的数据同步需求，所以这里不可能罗列所有的技术出来，主要是从场景的角度来说明，因为这一块的内容，面试官更关注的是场景。

**本篇面试内容划重点：离线同步、增量同步、实时同步、数据预处理。**

### 离线同步场景

离线数据同步也叫做批量同步，是大数据非常常见的场景，数据源一般为数据库或者日志文件，针对这个场景只列举两个比较常用的同步组件：阿里开源的数据同步组件
Datax、Apache 的老牌项目 Sqoop。它们功能相似，但是实现上有很大的区别。

**Datax**

Datax 目前的开源版本是单机多线程的版本，任务提交后 DataX 会将提交的 Job 分成多个小的 Task（子任务），以便于并发执行，小 Task
组成 TaskGroup 以方便管理，具体的 Task 任务是由线程来执行的，Job 会监控并等待多个 TaskGroup 模块任务完成，等待所有
TaskGroup 任务完成后 Job 成功退出。所以 Datax 在设计上其实还是比较轻量级的，而且它的 Reader 和 Writer
的接口非常友好，很适合二次开发，由开发者自定义读和写的存储介质。

![enter image description
here](https://images.gitbook.cn/5c7b6780-e8d9-11ea-9e67-3d4549ae97a3)

图片来自官方[GitHub](https://cloud.githubusercontent.com/assets/1067175/17850849/aa6c95a8-6891-11e6-94b7-39f0ab5af3b4.png)

**Sqoop**

Sqoop 底层是基于 MapReduce 实现的，所以它的执行需要依赖于 Hadoop 环境，显然没有 Datax 那么轻量级，但是 Datax
目前官方版本只支持单机，数据量大的场景下，Sqoop 拥有更好的性能。另外，Sqoop 只可以在关系型数据库和 Hadoop
组件之间进行数据迁移，Datax 的话上面也提到了在数据库的读写支持上更加丰富。

#### **全量与增量场景**

这里主要讲两个比较重要的问题，离线数据的全量同步和增量同步。无论是 Datax 还是 Sqoop 都有这样的场景。

  * 全量同步实现起来比较简单，直接读全表，然后配置覆盖写入或者追加写入即可，因为不存在状态的问题。
  * 增量同步的话，MySQL 中必须有自增 ID 或者更新时间（update_time）。
    * 自增 ID 适用于 MySQL 表 **只有新增数据没有更新数据** 的场景，因为数据更新之后自增 ID 是不变的，而增量是通过记录本次同步的最大的自增 ID 值（offset），在下一次同步时指定 `id > ${offset}` 来实现的，所以如果更新 `id < ${offset}` 的数据就不会同步成功。
  * update_time 适用于 MySQL 表 **有更新数据** 的场景，同样需要记录 offset，但是这次记录的是 update_time，数据更新的同时也会更新 update_time，所以只需要指定 `update_time > ${offset}` 就能够获取到从上次同步数据到现在的所有有新数据和更新过的数据。

### 实时同步场景

实时同步相比离线同步的差别主要是实时同步有一个 **自动增量**
的属性，不用手动记录同步的状态，因为实时同步工具是一个长进程的服务，而不是执行完成就退出的任务。当同步完上一批数据后，这个服务需要实时监控数据的增量情况，如果有增量则需要继续处理，通俗地说就是增加一条数据就处理一条数据。

#### **数据库同步**

Canal 也是阿里的一个开源作品，它的作用可以用下面一幅图来说明，不同于上面离线同步的组件，Canal 的数据源只支持 MySQL，它是作为一个
slave 服务通过实时同步解析主库 binlog 来实现的 MySQL 数据实时同步。图下图所示，输出方面支持的数据库就比较多了。

![enter image description
here](https://images.gitbook.cn/bdf552a0-e8d9-11ea-91d9-89a6127f3a5c)

图片来自官方 GitHub

**Canal 的设计和工作原理？**

首先要简单说下 MySQL 的主从数据同步机制，MySQL Master 主节点可开启配置将数据变更记录写入二进制日志（binary log），MySQL
Slave 从节点会实时将 master 的 binary log events 拷贝到它的中继日志（relay log），然后 MySQL Slave
会重放中继日志中的事件，即将主节点的操作记录在当前的从节点也执行一次，保证主从的数据同步。

以上是 MySQL 主从复制的简单流程，Canal 的设计就是基于主从复制的原理，模拟了 MySQL Slave 的交互协议，伪装自己为 MySQL
Slave，向 MySQL master 发送获取操作日志的请求，MySQL master 收到 Canal 伪装的请求，并没有怀疑，开始直接推送
binary log 给 Canal。Canal 收到 binary log 后和 MySQL Slave 一样对日志进行解析，然后发向下游的存储。

#### **日志同步**

实时的日志数据同步组件这里提一个使用率比较高的 Apache Flume。使用 Flume 采集日志文件的过程较简洁，只需选择恰当的
Source、Channel 和 Sink 并且配置起来即可，若有特殊需求也可自己进行二次开发实现个人需求。

![enter image description
here](https://images.gitbook.cn/e5755280-e8d9-11ea-91aa-4deb1f5b5838)

Flume 的架构主要分为三个部分，如上图所示。

  * Source 即数据源，负责收集数据，将数据以 Flume Event 形式传递给一个或多个 Channel。（Event 是一个数据单元，由消息头和消息体组成，可以是日志记录、avro 对象等）
  * Channel 是连接 Sources 和 Sinks，类似队列的组件。它会临时存放 Source 端传递过来的 Event 数据直到数据被 Sink 端消费，另外存放的方式是可配置的，基于内存或者磁盘等等。
  * Sink 是 Flume 的数据输出端，负责从 Channel 收集数据，然后将数据分发到下游的存储（HDFS/HBase 等）。

### 数据预处理

在同步的过程中的预处理一般是轻量级的预处理，比如数据结构上与输出数据的表结构不符合，时间格式不符合需求等问题，因为复杂的处理会影响性能，毕竟我们用这个工具的主要目的还是同步数据。这里还是举几个典型的例子。

  * Datax 对于关系型数据库的数据源支持 SQL 的查询，我们可以直接用 SQL 来预处理数据，将 SQL 执行的结果同步到下游数据库。
  * Flume 对于数据的预处理相对会灵活一些，但是开发成本会比较高，因为需要自定义开发拦截器（Interceptor）。Flume 在执行的过程中会调用 Interceptor 的逻辑对数据做预处理，虽然灵活但是也不建议在这里处理复杂的逻辑。

这里说的预处理是初级的预处理，如果需要对数据做更复杂的操作一般会通过 Flink 或者 Spark 来完成。

### 总结

上面列举了几个比较常用的数据同步工具，和各自适用的场景，掌握这些内容能解决大部分的数据问题，如下图所示，企业中的数据同步场景抽象后可以用下面一张图来概括。

![enter image description
here](https://images.gitbook.cn/23873700-e8da-11ea-9e67-3d4549ae97a3)

日志文件可以包括用户行为日志，系统异常日志等文本结构的数据，可以通过 Flume
实时监控数据路径，触发增量采集，将数据写入消息队列中，作为实时数据计算（SparkStreaming/Flink）的数据源。同时 Kafka 的数据可以通过
Flume 写 入 HDFS，作为离线数据来做 T+1 计算。也可以通过 Datax 的 TxtFileReader 来读系统的本地文件并通过
hdfsWriter 写 入 HDFS 文件系统中。

数据库的数据包括关系型数据库和非关系型数据库，Sqoop 目前只支持了关系型数据库，通过 MapReduce 离线任务的方式将外部数据同步到 HDFS
中，Canal 只支持了 MySQL 的实时数据解析和同步，最终可以选择将数据写入异构的数据存储中，比如 Kafka、HBase
等。对于非关系型数据库可以通过 Datax 来进行数据同步，最终写入 HDFS 作为离线的数据源。

以上是比较通用的 ETL
逻辑，对于不同的公司由于业务上的差异架构设计上会有所不同，但是整体上都是类似的，面试的过程中这一块的内容可以结合业务，分实时/离线、全量/增量、数据库/日志等不同的场景来说明。

