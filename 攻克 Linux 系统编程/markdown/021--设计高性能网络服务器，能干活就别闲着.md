在软件开发行业中，一提到高性能，就会立即给人一种高大上的感觉。那么在同样的硬件平台上，为什么有的服务器可以称为高性能，而有的就不行呢？

在本节课中，我们就拿一个网络服务器作为例子，看看它是如何从最简单的模型一步步演化而来，并且讨论每一步演化过程的改进之处，主要内容包含：

  * 最简单的网络服务器模型
  * 多进程服务器模型
  * I/O 复用线程池模型
  * 增强版的 I/O 复用模型
  * 关于可写事件

### 1 概述

要讨论高性能服务器，首先要对它做一个定义。

服务器的最终目标，无非就是服务用户，而高性能服务器的设计目标，就是要 **同时服务尽可能多的用户，同时还要让单个用户感受到的服务延迟尽量少** 。

那么，对一台服务器来说，CPU 的计算能力、存储系统读写速度和网络带宽都是有固定上限的，如何才能实现尽量高的性能呢？

其实，不只对网络服务器，对其他任何程序来说，要实现高性能，无非就是两个方面：

  * 不浪费时间
  * 不浪费精力

下面的内容，我们就 **从网络服务器用到的最基本的系统接口出发，看看在各个不同的演化步骤下，是如何践行上面两点，从而实现性能的提高的** 。

### 2 最简单的网络服务器模型

Linux 为网络服务器程序提供的最基本的编程接口包括：

    
    
    int socket(int domain, int type, int protocol);   //创建网络套接字
    int bind(int sockfd, const struct sockaddr *addr, 
                socklen_t addrlen); //把套接字与一个具体的网络地址绑定
    int listen(int sockfd, int backlog);    //设置指定的套接字为被动监听状态
    int accept(int sockfd, struct sockaddr *addr, socklen_t *addrlen);
                  //接受一个连接请求，并为之创建新的套接字
    

此外，还有在网络上读写数据使用的通用读写接口：

    
    
    ssize_t read(int fd, void *buf, size_t count);
    ssize_t write(int fd, const void *buf, size_t count);
    

网络服务器最基本的工作流程为：

  1. 创建网络套接字；
  2. 绑定本地的网络地址；
  3. 当收到网络连接请求时，接受连接，为新的连接创建新的套接字；
  4. 随后，在这个新的套接字上读写数据，就可以实现与对端的网络通信。

按照这个思路，可以写出一个最简单的网络服务器程序：

    
    
    #include<stdio.h>
    #include<stdlib.h>
    #include<string.h>
    #include<unistd.h>
    #include<sys/socket.h>
    #include<netinet/in.h>
    
    #define BUFF_SIZE 1024
    #define SVR_PORT 6677
    
    //业务处理逻辑
    int doWork(int sockfd)
    {
        char buffer[BUFF_SIZE];
        int n = read(sockfd, buffer, BUFF_SIZE); //读取对端输入
        int resn;
        char result[BUFF_SIZE];
        //对请求执行处理，把处理结果放在 result，resn 保存结果数据长度
        write(sockfd, result, resn);        //处理结果发送给对端
    }
    
    int main(void)
    {
        struct sockaddr_in server_addr, client_addr;
        socklen_t clientaddr_len;
    
        int listenfd, connfd;
    
        listenfd = socket(AF_INET, SOCK_STREAM, 0);  //创建 TCP 网络套接字
    
        bzero(&server_addr, sizeof(server_addr));
        server_addr.sin_family = AF_INET;
        server_addr.sin_addr.s_addr = htonl(INADDR_ANY);
        server_addr.sin_port = htons(SVR_PORT);     //设置监听的本地地址和端口
    
        bind(listenfd, (struct sockaddr *)&server_addr, sizeof(server_addr));   //套接字与本地地址绑定
        listen(listenfd, 20);
    
        while(1)
        {
            clientaddr_len = sizeof(client_addr);
            connfd = accept(listenfd, (struct sockaddr *)&client_addr, &clientaddr_len);  //接受连接请求，创建新的连接
            doWork(connfd);      //处理请求
            close(sockfd);       //处理完毕，关闭连接
        }
    
        return 0;
    }
    

先不去关心这个程序的容错处理，而只关心它的工作模型。在这个服务器程序中，所有的都是线性的：等待一个客户的连接请求；创建连接之后，读取它的请求数据；处理请求并发送响应数据，最后关闭连接。

这样的程序仅仅用来演示网络服务器程序的工作过程还可以，但是几乎不太可能应用在实际的任何服务器上，因为它的性能实在是太差了，大量的 CPU 时间都被浪费了。

首先，这样的程序完全是单线程的，即使运行在 32 核的高端主机上，也最多只能用上其中的一个核。

其次，在服务用户的过程中，它使用的好几个系统调用都是阻塞的，比如 read 和 write。我们知道，计算机 I/O 的速度和 CPU
相比慢了好几个数量级，而在这个程序中，当系统在执行阻塞的 I/O 读写操作时，CPU
都在无聊地等着，即使又有新用户连接进来，它也要完全响应完第一个用户之后，再去处理新的用户连接。

### 3 多进程服务器模型

对这个最初级的网络程序，改造的第一种思路，就是把它变成并发式的，也就是说，要让服务器可以同时服务多个用户。而
**并发式服务器的最直接的思路，就为每个用户都创建一个单独的服务进程，专门为其服务**
。这样，最早启动的父进程只需要处理新连接请求，对每个连接请求，都创建一个新的子进程，由子进程负责与该客户端的所有后续交互。

改造方法很简单，把循环等待部分的代码修改成如下所示就可以了：

    
    
    pid_t pid = 0;
    while(1)
    {
        int connfd = accept(listenfd, ...);
        if( (pid = fork()) == 0 )
        {
            close(listenfd);
            dowork(connfd);
        }
        close(connfd);
    }
    

经过这样的处理之后，程序在时间上严重浪费的问题就能得到大幅度的改善。因为现在对每个连接的用户，都有一个单独的进程为它服务了，所以，当某个子进程阻塞在 I/O
操作上的时候，宝贵的 CPU 时间可以被分配给父进程处理新的连接，也可以被分配给别的子进程处理输入请求。这就可以极大地提高系统的整体性能。

著名的老牌 Web 服务器 Apache，使用的就是这种多进程处理模型。

### 4 I/O 复用线程池模型

多进程模型能很好地解决 CPU 时间浪费的问题，但是在“不浪费精力”方面做得并不好。因为给每个用户都创建了一个进程，如果对用户每次请求的处理，不像 Web
请求那样是无状态的，处理完成就可以关闭并结束进程，而是每个用户都需要保持连接相对较长的时间，需要与服务器执行多次交互，那么，当用户量增加的时候，系统中就会有很多个进程同时在运行。

进程的调度和切换也是需要花费 CPU 时间的，如果同时运行的进程数量过多，光进程调度的压力就足以把系统拖垮。要进一步提高服务器的性能，我们需要想办法减少
CPU 花在进程切换上的浪费。

  * **一个最直接的想法，既然进程切换花销太大，那可以换成轻量级进程，也就是线程，来减少切换开销** 。确实是可行的，但是提高的幅度很有限，当进程数量太多时，仍然会拖垮系统，只是其临界点比多进程高一点罢了。

  * **另外一个思路是，既然大量进程的切换会拖垮系统，那我们如果采用进程池，限制进程的总数量，让每个进程同时负责多个用户，就是可以大幅减少 CPU 花费在进程调度上的开销** 。

要在单个进程里面同时服务多个用户，就需要同时监听多个网络套接字上发生的事件，解决这个问题有两种思路：

  * 一种是把套接字设置为非阻塞的，然后依次轮询每一个套接字；
  * 另一种是使用 I/O 复用技术。

**第一种方案中，轮询的过程本身就是个浪费精力的事情**
，因为它需要进程一刻不停地依次询问每个套接字：“先生，请问有没有需要帮助的？”问完一圈再回来问一圈，一直这样循环下去，而且，在绝大多数的情况下，得到的答复可能都是：“不需要，谢谢！”

**而 I/O 复用技术，就可以让这方面的浪费大大减少**
。它是让进程预先告诉内核需要监听的一组事件，当其中的任何一个事件发生的时候，内核会通知进程，此时进程就可以被唤醒，来检查哪个套接字上有事件需要处理，而不需要一刻不停地轮询了。

Linux 中的最古老的 I/O 复用接口是 select：

    
    
    int select(int nfds, fd_set *readfds, fd_set *writefds, fd_set *exceptfds, struct timeval *timeout);
    
    //用于操作 fd_set 的宏定义
    void FD_CLR(int fd, fd_set *set);         //从 set 中清除 fd
    int  FD_ISSET(int fd, fd_set *set);       //检查 set 中有没有指定的 fd
    void FD_SET(int fd, fd_set *set);         //在 set 中设置 fd
    void FD_ZERO(fd_set *set);                //清空 set
    

要使用 select 函数，首先需要自己准备一个最长为 FD_SETSIZE
的文件描述符数组，用于记录所有需要监听的文件描述符，同时要记录这些描述符中数值最大的一个，并把它 +1 传给 select
的第一个参数。初始状态下，当只有最早的监听套接字时，这个值就是监听套接字的文件描述符：

    
    
    int client[FS_SETSIZE];
    int maxfd = listenfd;
    fd_set init_set, rec_set;
    for(int i = 0; i < FS_SETSIZE; i++)
        client[i] = -1;   //用复数表示该位置空闲
    FD_ZERO(&init_set);
    FD_SET(listenfd, &init_set);
    
    while(1)
    {
        rec_set = init_set;
        int nevent = select(maxfd + 1, &rec_set, NULL, NULL, NULL);
        ...
    }
    

然后，当 select 函数返回时，表示在传入的文件描述符集合中，至少有一个描述符上发生了需要处理的事件，所以，就需要找出是哪个，并依次处理：

    
    
    if(FD_ISSET(listenfd, &rec_set))
    {
        //表示服务器的监听套接字上发生了新的接入事件
        connfd = accept(listenfd, ...);
    
        //把新创建的描述符加入到监听集合，并记录在 client 数组
        FD_SET(connfd, &init_set);
        if(connfd > maxfd) maxfd = connfd;
    
        //找一个空位记录新的文件描述符
        for(int i = 0; i < FD_SETSIZE; i++){
            if(client[i] < 0) {
                client[i] = connfd;
                break;
            }
        }
    }
    
    //依次检查还有哪个文件描述符上发生了需要处理的事件
    for(ini i = 0;  i < FD_SETSIZE; i++) {
        if(client[i] < 0) continue;   //跳过空位
        int sockfd = client[i];
        if(FD_SET(sockfd, &rec_set)) {
            doWork(sockfd);
        }
    }
    

select 能同时监听的文件描述符集合，有最大 1024 的数量限制，要修改这个限制，需要修改宏定义 FD_SETSIZE，并重新编译内核。

有这样的限制，是因为在它被设计的那个年代，认为 1024 已经足够大了。Richard Stevens 教授的名著《Unix 网络编程 卷1：联网套接字
API》（ _Unix Network Programming, Volume 1: The Sockets Networking API_
）中，也有原文说：

> 头文件 <sys/select.h> 中定义的 FD_SETSIZE 常值是数据类型 fd_set 中的描述符总数，其值通常是
> 1024，不过很少有程序用到那么多的描述符。

时代在发展，现在的情况已经变成了： **很少有服务器程序只能同时支持不超过 1024 个连接。**

在现代，接任 select，担当 I/O 复用功能的函数是 poll，它破除了受到宏定义限制的最大连接数限制。其函数定义为：

    
    
    struct pollfd {
                   int   fd;         /* 要监听的文件描述符 */
                   short events;     /* 感兴趣的监听事件 */
                   short revents;    /* 实际发生的事件 */
               };
    
    int poll(struct pollfd *fds, nfds_t nfds, int timeout); 
    

各参数释义如下：

  * 参数 fds 指定要监听的 pollfd 结构的数组；
  * 参数 nfds 说明了被监听的 pollfd 结构的数量；
  * timeout 则指定超时时间。

poll 的使用方式与 select 很像，不过它把要监听的文件描述符数组与 fdset
合并在了一个结构中，所以更加方便直观。只要初始时把监听套接字放入监听数组，新连接建立之后再依次向后追加就可以了：

    
    
    struct pollfd clients[MAX_LIMIT];    //MAX_LIMIT 是自定义的长度限制
    
    clients[0].fd = listenfd;
    clients[0].events = POLLRDNORM;   //监听新连接事件
    for(int i = 1; i < MAX_LIMIT; i++) {
        clients[i].fd = -1;           //把其他的文件描述符标记为无效
    }
    int maxIdx = 1;
    
    while(1) {
        int nevent = poll(clients, maxIdx, 0x8FFFFFFF);
        ...
    }
    

然后，在收到任何事件通知，从 poll
调用返回后，需要依次检查每个事件。如果是发生在服务器监听套接字上的连接请求，就建立新的套接字，并添加到监听数组中；其他套接字上的事件，就执行消息读取，并依次处理：

    
    
    if(clients[0].revent & POLLRDNORM) {
        //处理新连接请求
        int connfd = accept(listenfd, ...);
        //找个空闲的位置，保存新创建的套接字
        for(int i = 1; i < MAX_LIMIT; i++) {
            if(clients[i].fd < 0) {
                clients[i].fd = connfd;
                clients[i].events = POLLRDNORM;
                if(i > maxIdx) maxIdx = i;
                break;
            }
        }
    }
    
    for(int i = 1; i < maxIdx; i++) {
        if(clients[i].fd < 0) continue;
    
        if(clients[i].revents & POLLRDNORM) {
            doWork(clients[i].fd);
        }
    }
    

上面的程序并没有处理客户端连接的关闭事件，因为本节课重点关注的是整个处理流程中影响性能的方面，所以，只关注正常业务的处理流程。

可以看到，I/O 复用技术能比较高效地实现对多个网络套接字的同时监听，因此可以减少系统中需要启动的进程或线程的数量，从而降低 CPU
计算资源在调度和进程切换上的消耗。

而且，相比轮询处理，I/O
复用技术的处理过程就好比是空姐跟飞机上的所有人都说了一句：“有需要帮助请按铃”然后就回到自己的休息舱等着了；当有任何人需要帮助，并按铃时，空姐会从休息舱出来，从前往后依次问每位乘客：“请问是您需要帮助吗？”这比一刻不停地一直轮询要有效率多了。

### 5 增强版的 I/O 复用模型

使用 I/O
复用，并结合进程或线程池的方案，只要合理分配好业务逻辑，就能实现很高的服务器性能。但是，拿空姐的例子一类比，读者肯定马上就发现了其中存在的问题：每当有任意一位乘客按铃的时候，你怎么忍心让空姐再依次把所有乘客都问一遍呢？把这个铃背后所关联的信息直接告诉空姐，哪个座位上的乘客需要帮助不就行了吗？

没错，确实是这样的。

而且，select 和 poll
在调用和返回时，在内存空间的利用上也不是很高效。因为不论有几个文件描述符上实际发生了事件，都需要在内核和用户空间之间传递整个文件描述符集合数组。

**增强版的 I/O 复用 epoll，在这两个方面上都有所改进，因此可以达到更好的性能** 。在 epoll
中，只有真正发生了需要处理的事件的文件描述符，内核才会返回给用户应用，所以，不再需要遍历整个文件描述符数组，找出发生事件的对象；而只需要把返回的事件依次处理掉就行了。

epoll 相关的接口定义如下：

    
    
    int epoll_create(int size);
    int epoll_wait(int epfd, struct epoll_event *events, int maxevents, int timeout);
    int epoll_ctl(int epfd, int op, int fd, struct epoll_event *event);
    

使用 epoll 编写的基本网络服务器程序的实例为：

    
    
    #define MAX_EVENTS 50
    struct epoll_event ev, events[MAX_EVENTS];
    int listen_sock, conn_sock, nfds, epollfd;
    
    epollfd = epoll_create(100);
    if (epollfd == -1) {
        exit(EXIT_FAILURE);
    }
    
    ev.events = EPOLLIN;
    ev.data.fd = listen_sock;
    if (epoll_ctl(epollfd, EPOLL_CTL_ADD, listen_sock, &ev) == -1) {
        exit(EXIT_FAILURE);
    }
    
    for (;;) {
        nfds = epoll_wait(epollfd, events, MAX_EVENTS, -1);
        if (nfds == -1) {
            exit(EXIT_FAILURE);
        }
    
        for (n = 0; n < nfds; ++n) {
            if (events[n].data.fd == listen_sock) {
                conn_sock = accept(listen_sock, ...);
                if (conn_sock == -1) {
                    exit(EXIT_FAILURE);
                }
                setnonblocking(conn_sock);
                ev.events = EPOLLIN | EPOLLET;
                ev.data.fd = conn_sock;
                if (epoll_ctl(epollfd, EPOLL_CTL_ADD, conn_sock,
                                   &ev) == -1) {
                    exit(EXIT_FAILURE);
                }
            } else {
                doWork(events[n].data.fd);
            }
        }
    }
    

可以看到，epoll_wait() 的返回会指示发生了几个需要处理的事件，并且只有需要引用处理的事件才会被写入到 events
数组内。这就有效地避免了每次发生事件的时候，都要依次遍历所有监听的文件描述符，以确定是哪个上面发生了什么事件。

对绝大多数的应用来说，epoll 模型已经足够高效了，使用 epoll 模型开发出的服务器程序能不能达到性能要求，就完全依赖于自身的业务逻辑了。

### 6 关于可写事件

对网络套接字的可写事件的处理，是另一个可能造成计算资源浪费的地方。在上面的示例程序中，只出现了对读事件的注册，而有时，与客户端建立连接之后，服务器还需要主动向客户端发消息，这时就需要监听网络套接字的可写事件。

但是，可写事件不应该一直注册在监听列表中，因为只要对端连接准备好接收数据，可写事件就会一直反复被触发，但是大多数时候，服务器可能并没有实际要发送的数据，就会造成一些浪费。

对可写事件，正确的做法是， **只有在服务器有实际要发给对端的数据时，再去注册相应的事件监听，并且，在发送完数据之后，还要记得把该事件的监听移除。**

### 7 总结

在本节课中，我们把一个具有基本结构的网络服务器，逐渐改进为使用 epoll 模型的高性能服务器，并解释了每步改进过程中对性能的优化。

影响网络服务器性能的因素远不止本节课所讨论的与网络模型有关的这几个点，还有很多其他的东西会影响性能，比如把连续快速调用系统函数发送的多个小包（太多次的系统调用，浪费精力），合并成为一个大包一次性发出，也能显著提高程序性能。不过总结起来，最终还是会归结为这两点：

  * 不浪费时间（别让 CPU 闲下来）
  * 不浪费精力（别让 CPU 做些无用的事情）

其实，不止网络服务器，学习、生活，还有工作，都是一样的。要提高产出，都可以归结为：别浪费时间，别浪费精力。只有感情除外，不能量化，不能在乎产出。

