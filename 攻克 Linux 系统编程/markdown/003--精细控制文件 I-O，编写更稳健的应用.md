在 Linux 中有条重要的哲学，即一切皆文件，本文就来 **着重讲讲文件操作**
。首先从磁盘文件开始，探讨除打开、读、写、关闭等常规操作外，还有哪些可控操作。理解和掌握各个步骤的行为细节，可以
**帮助开发者写出性能更好、更加稳健的应用** 。本文主要包括以下几部分内容：

  * 默认的文件读写行为
  * 在应用层选择合适的文件 IO 缓存
  * 控制文件内容在磁盘设备上的更新
  * 控制文件内容的预读取策略
  * 混合使用库函数和系统调用

### 1 默认的文件读写行为

要想更精准地控制文件读写行为，首先要弄清楚，在不做任何额外控制的情况下，系统默认动作是什么。了解了这些，我们才能知道默认行为适合哪些情况，而在哪些情况下需要我们介入附加控制。

#### 1.1 文件读写默认操作

我们首先分别看下文件读取、文件写入的系统默认动作。

  * **先说文件读取**

默认情况下，当使用 read 系统调用从文件中读取一些字节时，Linux
内核除了读取指定字节数的数据外，还会额外预读取一些数据到内核缓冲区。下次再读取文件内容时，会先从内核缓冲区中查找，如果正好找到了，则省去了等待慢速磁盘定位和数据传输的时间。在大多数
Linux 系统中，预读取数据的长度为 128 KB，也可能根据系统可用内存的大小动态调整。

  * **再看文件写入**

当用 write 系统调用写入文件内容时，函数将数据回写到内核缓冲区之后便返回，Linux
内核负责在稍后一段时间内将文件内容真正写入到磁盘中。除了更新文件本身的内容，还会更新文件的元数据，如文件大小、文件关联的
inode、文件最后修改时间等信息。

#### 1.2 特殊读写处理

默认的文件读写操作已能很好地满足绝大多数应用的需求，但仍 **有一些特殊的读写操作需做特别处理** 。

**比如**
，某些应用对数据可靠性有很高要求，某段代码逻辑需保证文件内容确实已经更新到磁盘上后，才会向下运行。显然，默认的写操作（内容会在稍后一段时间内由操作系统内核写入磁盘）是无法满足这一需求的。

**还有** ，某些应用在读取数据时有固定的访问模式，比如读取某段数据之后，后续肯定会再读取文件某偏移处的数据，这时需针对该固定访问模式做深度优化。

**还有一些数据库应用** ，已在应用层实现了自己的数据管理策略，当使用 write
系统调用时，希望系统将数据直接写入磁盘，无需缓冲数据，进而节省内核资源。

另外， **还有一点需要注意**
，系统调用在内核中已经实现了一套高速缓存，但这并不意味着，应用可以随意执行系统调用，而不用关心性能问题。倘若每次使用系统调用读取或写入的字节数很少，系统调用本身的开销将占用总开销很大比例。因此，在应用层上加一层数据缓存，以尽量减少系统调用的次数，可明显提升应用的性能表现。

本文接下来依次讨论各个层次上的那些可控制选项。

### 2 选取合适的文件 IO 缓存

先从应用层开始，谈谈应用层缓存问题。

前面提到，应用层应该加一层缓存，以尽量减少系统调用的使用次数，以此来提高应用的整体性能表现。那么该缓存大小应该如何确定呢？

在某文献中有这样一个测试，以 Ext2 文件系统中的常规文件为例，当该文件系统的块大小为 4K 字节时，在应用层设置 4K
字节的缓存，相比单字节调用系统调用，性能可以提升两个数量级。进一步增大应用层的缓存大小，性能不再有明显提升。基于这个测试结果，我们可以得出，
**应用层缓存大小至少应该等于所使用文件系统的块大小** 。

实际上，还有一个获取文件信息的系统调用 stat。其获得的文件属性信息中，有一项建议了文件 IO
缓存大小，低于此值的缓存大小会被认为是低效的。其函数原型为：

    
    
    int stat(const char * pathname, struct stat *statbuf);
    

这一项正是 struct stat 结构体中的 st_blksize 字段。设置应用层的缓存大小，至少不小于该字段给出的数值。

此外， **glibc 中提供的 fread 和 fwrite 函数，其内部都维护了一个数据缓存，用来尽量减少系统调用次数**
。默认选择的缓存大小已进行了充分优化。如果还是不满意，可以用 glibc 的 setvbuf 和 setbuffer
函数自定义缓存大小和缓存行为。这两个函数的原型分别为：

    
    
    int setvbuf(FILE *stream, char *buf, int mode, size_t size);
    void setbuffer(FILE *stream, char *buf, size_t size);
    

其中的 setvbuf 函数允许开发者指定缓冲方式，主要有以下三种可选方式。

  * _IONBF：不缓冲，标准错误输出默认选择该缓冲方式，以保证错误信息及时输出来。
  * _IOLBF：行缓冲，也就是遇到换行符时，对之前的内容执行 read、write 系统调用，终端设备默认执行该缓冲方式。
  * _IOFBF：全缓冲，也叫块缓冲，当指定大小的缓冲区满了之后，才会触发调用一次系统调用，磁盘文件默认使用该缓冲方式。同时，glibc 还提供了 fflush 函数，应用可以在缓冲区数据满之前，手动将数据刷新到内核缓冲区。

出于性能上的考虑，读写磁盘文件应该使用 fread 和 fwrite 函数，而不是直接使用 read 和 write
系统调用。同时，可以使用库函数默认的缓冲区，也可以根据 stat 的建议设置合适大小的自定义缓冲区。

### 3 文件内容在磁盘上的更新

知道了如何设置缓冲区，下面就来说说如何控制数据在磁盘上的更新。

在 Linux 内核中，文件数据的磁盘同步状态有两个层次，分别为“同步 IO 数据完整性”和“同步 IO
文件完整性”，名称有点长，后面我们简称为“数据完整性”和“文件完整性”。

  * **数据完整性** 是指文件的内容数据已经写入到磁盘中了。
  * **文件完整性** 指的是不止文件的内容数据，文件的元数据也已写入磁盘中。

文件的元数据是指描述文件信息的数据，如文件创建时间、大小、占用节点数据等。文件完整性包含了数据完整性。还有一点需要明确，文件的元数据和文件数据并未保存在同一磁盘位置上。

#### 3.1 fdatasync 和 fsync 系统调用

Linux 提供了 fdatasync 和 fsync 两个系统调用，分别用来保证数据完整性和文件完整性。函数原型分别为：

    
    
    int fdatasync(int fd);
    int fsync(int fd);
    

所以，前面提到的对数据可靠性要求很高的应用，就可以在 write 之后调用 fdatasync，强制将数据从内核缓冲区刷新到磁盘中。

当然也可以调用 fsync ，更加彻底地把数据元数据信息也刷新到磁盘中去。

不过大家需要清楚，这样做会带来 **更高的操作延迟**
，因为文件的元数据通常保存在不同的磁盘位置上，需多花一些寻道时间来保存元数据。具体使用哪种层次的数据同步，可以根据应用需求灵活选择。

#### 3.2 O_DSYNC 和 O_SYNC 标志

除此之外，使用 open 打开文件时指定 O_DSYNC 或 O_SYNC 标志，也可以让 write
系统调用在该文件上写入数据时，分别达到数据完整性、文件完整性同步状态，即让 write 系统调用在文件上的表现等同于 write + fdatasync 或
write + fsync。但是， **使用这两个标志会影响到在该文件上的每一次 write 系统调用的执行** 。

> 编码时，推荐根据具体需求谨慎选择调用 fdatasync 或是 fsync。不建议使用打开标志，该方式会损失一些灵活性与性能。

### 4 控制文件内容的预读取策略

如何根据数据访问顺序的特定模式深入优化应用，针对这个问题，Linux 内核提供了 posix_fadvise
系统调用。它允许应用程序告知内核访问某个文件数据时采取的模式。系统调用原型为：

    
    
    int posix_fadvise(int fd, off_t offset, off_t len, int advice);
    

其中，advice 参数指定了所采用的模式，支持的模式有以下几种。

  * POSIX_FADV_NORMAL：默认模式，内核会把文件预读窗口设置为默认值（128K）。
  * POSIX_FADV_SEQUENTAL：顺序访问模式，内核会把文件预读窗口设置为默认值的两倍。
  * POSIX_FADV_RANDOM：完全随机访问数据，在这种模式下，内核不使用文件预读。
  * POSIX_FADV_WILLNEED：在这种模式下，应用可以同时提供 offset 和 len 参数，指示接下来要使用的文件的位置和数据长度。根据这两个参数，内核会将数据预读到内核缓冲区。
  * POSIX_FADV_DONTNEED：在这种模式下，应用同样可以提供 offset 和 len 参数，但指示的是接下来不会使用的文件的位置和数据长度，内核会释放这部分数据所占用的内核缓冲区。
  * POSIX_FADV_NOREUSE：这种模式指定的文件区域，会在接下来仅访问一次。在下次被读取后，相应的缓存页面会从缓冲区中释放掉。

灵活使用这些参数，可以帮助内核对应用采取更加友好的行为策略，或者可以提高缓存的命中率，从而提高性能表现，或者可以节省内核所占用的宝贵内存资源。

最后，再来谈谈不使用内核缓冲区， **直接与磁盘进行数据传输** 的问题。在 Linux 中，将这一过程称为直接 IO，实现方式是在打开文件的同时指定
O_DIRECT 标志。

使用这种方式写入文件时，内存边界需要是文件系统块大小的整数倍，传递数据的长度也需要是块大小的整数倍。这时，可能需要使用 memalign
此类技术来分配数据内存块。详细信息，请参看 Linux 手册，或自行 Google 查找专门的介绍材料，这里就不展开了。

### 5 混合使用 glibc 函数与系统调用

经过上面的解说，我们已经知道如何应对某些对文件 IO 有特殊要求的需求了。不过，似乎还有最后一个问题有待解决。

前面讨论过，为了尽量减少系统调用的次数，我们推荐使用 glibc 的 fread 和 fwrite 函数操作文件，这两个函数需要的参数是 FILE 类型。

如果我们既希望使用系统调用控制数据同步和内核缓冲行为，又需要使用以整数型为参数的系统调用，该如何混合使用两者操作同一个文件呢？

其实，C 标准库提供了实现两者间互相转换的函数：

    
    
    int fileno(FILE *fp)
    FILE * fdopen(int fd, const char * mode)
    

其中 fdopen 中的文件模式需要和 open 打开文件时的模式相同，否则会失败。

终于，最后一块拼图也完整了。

### 6 总结

本文深入讨论了除 **常规打开、读写、关闭** 之外，对磁盘文件还可进行的 **其他更高级的控制功能** ，以及 **适用场景**
。希望它们能成为读者工具箱中的一部分，变成打磨读者自己应用的利器。

点击了解：[《攻克 Linux
系统编程》](https://gitbook.cn/gitchat/column/5bfbbe9b7d496f13396961de?utm_source=ywtsd001)

