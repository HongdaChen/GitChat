在开发完协同过滤之后，整个地鼠商城的推荐页都使用了这唯一的召回算法，场景包括了首页推荐、商品详情页推荐、购物车页推荐等等。总的来说各项指标都比原来的千人一面都由很大的提升，这是显而易见的。

我以为可以躺在功劳簿上，每天 965 了。可是“勤劳”的 BOSS 哪里允许手下的虾兵蟹将如此的“安逸”，于是就跑到我的跟前：

> BOSS：小周啊，你们的推荐系统做的很不错，不仅为公司带来了更多的收益，也节省了很多的人力成本，你们厉害。
>
> 我：哪里哪里，您指导有方，方案设计得好。
>
> BOSS：不过呢，我们也不能就此打住，我跟上面报的目标是 点击率提升 20%……
>
> 我：啊，这么多！
>
> BOSS：我看了下各场景的指标分析，虽然购物车、收藏和订单页三个场景的指标也有所提升，但是幅度最小，你们看看怎么优化？
>
> 我：好的。
>
> BOSS：嗯，给你们半个月的时间优化，写好了方案告诉我，我们一起过一遍。
>
> 我：好的。
>
> BOSS：嗯，另外要注意下团队工作饱和度，我走了。
>
> 我：……

看来不能再 965 了，又要投入到战斗中来了。

### 问题描述

推荐系统在上述三个（购物车、收藏和订单）点击率提升最少的场景中发生作用的位置如下图所示：

![关联规则使用场景](https://images.gitbook.cn/5057da10-274a-11eb-bb9f-e35884210416)

其中“你可能还喜欢”为具体的推荐列表，一般会根据用户的行为（浏览、加车、收藏和购买）去进行推荐。那么，为什么协同过滤在首页和商品详情页的表现会比这三个场景好呢？

首先，读者们应该已经很清楚了，协同过滤是一种“找相似”的算法，所推荐的商品与行为商品具有很高的相似性，在一般的场景可能还
OK，但是在以上三个场景就会出现问题。

换位思考下，假设我们要去商场买一双运动鞋，在没有买到之前，我们会一直货比三家，最终会将一双性价比最高的鞋子收入囊中。那么，在已经买到了一双耐克的运动鞋之后，我们还继续去逛阿迪或者彪马的运动鞋吗？大概率不会。

但是，如果我们继续在商场里闲逛，很可能会驻足在运动手环、运动耳机或者透气性吸汗性比较好的袜子前，因为买了运动鞋，就要把这一套运动装备都置办齐全。

再回到地鼠商城中来，显然当用户将商品放入购物车、添加到收藏夹或者已经购买了之后，很大的概率该用户并不会再去对相似的商品产生行为，如果再推相似性很高的商品，用户可能会感到审美疲劳以及对此商城感到失望——为什么我买了华为手机了，还要给我推苹果手机？——就不能给我推手机壳吗？

而这，是协同过滤解决不了的事情，因为手环耳机和袜子都与鞋子不那么相似，手机壳与手机从直观上来说，也没有什么相似性。

关联规则，就是用来解决这个问题的。

### 关联规则

当我们去商场购物时，通常都提前准备好了购物清单。比如学生可能会购买本子和笔，极客可能会购买机械键盘和多功能鼠标等等。了解这些购物规律（Buying
Patterns）显然可以帮助商场售卖出更多的商。这些购物规律总结成一句话就是：

> 用户在购买商品 X 时，是否会同时购买商品 Y？

如果商场掌握了这些规律，当它知道用户经常 X 和 Y 一起购买时，就可以做出如下决策：

  * X 和 Y 可以摆放在同一个货架上，道理不言而喻；
  * 对于已经购买了 X 的目标群体，商家可以给他们展示 X 的广告；
  * X 和 Y 可以做成捆绑销售；
  * ……

上述“规律”，真是我们苦心追寻的，也正好可以解决我们的问题：在购物车、收藏和订单页，用户已经购买/接近购买商品 X 了，我们要做的并不是推荐 X
的相似商品，而是与 X 经常一起出现在购物清单中的商品，也即 **相关商品** 。

问题是，我们怎样才能找到这条”规律“，发现出商品与商品之间的 **关联** ？

#### **定义** $^1$

关联规则挖掘，指的是数据挖掘中最活跃的技术之一，可以用来发现事情与事物之间的联系，比如经典的啤酒和尿布的例子。

在介绍 **相关性衡量指标** 之前，先熟悉以下几个概念：

  * **Transaction** ：一次交易，可以理解为同时出现在购物清单中的商品集合，那么多个用户就会产生多个交易。
  * **Itemset** ：项集，顾名思义，若干个商品的集合。如果这个集合长度为 k，那么就称为 k 项集。

下面以如下 transaction 来说明：

Transaction | Items  
---|---  
Transaction 1 | 🍺🍖🍕🍉  
Transaction 2 | 🍺🍖🍕  
Transaction 3 | 🍺🍖  
Transaction 4 | 🍖🍭  
Transaction 5 | 🍼🍺🍕  
Transaction 6 | 🍼🍺🍕  
Transaction 7 | 🍼🍺  
Transaction 8 | 🍼  
  
一般有三个常见的指标来衡量 **相关性** 。

**1\. 支持度 Support**

这个指标用来衡量 itemset 的流行度，即 itemset 出现的次数在所有 transaction 中的占比，如下表所示：

$$Support\\{ 🍖 \\} = {4\over 8}$$

**2\. 置信度 Confidence**

这个指标用来衡量 **当 X 被购买时，Y 也被购买的可能性，表示为 { X - > Y}**。通过计算 X、Y 的支持度除以 X 的支持度，即可得到 {X
-> Y} 的置信度，可以看出置信度就是条件概率 P(Y / X)，在上表中，{🍖 -> 🍺} 的置信度等于 3/4。

$$Confidence\\{ 🍖\rightarrow🍺 \\} = {Support\\{ 🍖,🍺\\}\over Support\\{ 🍖
\\}}$$

但是，读者们发现这个指标有什么问题了吗？

对，如果 X 和 Y 都是特别热门的商品，比如都是日用品，那么置信度就会很高，但是可能两者并无关联。熟悉协同过滤的同学应该立刻就想到了 **热门商品惩罚**
，是的，在考虑了 **热门商品惩罚** 之后，置信度就会变成 **提升度（Lift）** 。

**3\. 提升度 Lift**

这个指标与 Confidence 类似，但是要考虑 Y 的流行度，依然以 🍖 和 🍺 为例：

$$Lift\\{ 🍖\rightarrow🍺 \\} = {Support\\{ 🍖,🍺\\}\over Support\\{ 🍖 \\} \times
Support\\{ 🍺 \\}} = { Confidence\\{🍖\rightarrow🍺\\} \over Support\\{ 🍺 \\} }$$

很容易计算出来 {🍖 -> 🍺} 的提升度等于 1，也就是 🍖 和 🍺 并没有什么关系。Lift > 1 时，在已经购买 X 的情况下，才有可能购买
Y。Lift < 1 时，购买 Y 的可能性比较小。

在掌握了以上三种指标之后，就可以介绍 **频繁项集（Frequent Itemset）** 的概念了。

#### **频繁项集**

如果项集（Itemset）的 support 和 confidence 都能满足一定的阈值，就可以称为频繁项集（Frequent Itemset）。

之所以只考虑满足条件的 项集，一方面起到降低计算复杂度的作用，另一方面也有去除噪声提高结果质量的作用。

整个关联规则挖掘最重要的就是从数据集中找到频繁项集。

  1. 找出数据集中所有大于或等于最小支持度的项集。
  2. 利用上一步中的项集生成关联规则，根据最小置信度筛得到强关联规则。

假设商店中总共有 10 个商品，那么到底一共有多少个这样的 项集 需要去挖掘呢？我们来好好算一下：

  * 1 项集：含有 1 个商品的项集，共有 $C_{10}^1 = 10$
  * 2 项集：含有 2 个商品的项集，共有 $C_{10}^2 = 45$
  * 10 项集：含有 10 个商品的项集，共有 $C_{10}^{10} = 1$

一共 1023 个。

具有 N 个商品的数据集需要计算 support 和 confidence 的项集总数为：

$$N(Itemset) = C_N^1 + C_N^2 + ... +C_N^N = 2^N - 1$$

也就是说如果不加任何限制，项集的遍历时间复杂度为 $O(2^N)$。这在实际应用过程中是不可接受的，那么有没有办法提前减少肯定不是频繁项集的项集呢？

Apriori 算法便是其中之一。

### Apriori 算法

Apriori 算法基于这样一种思想：

> 如果一个 Itemset 不是频繁项集，那么所有包含此 Itemset 的 Itemsets 都不是频繁项集。

这就意味着，如果项集 {🍖} 不是频繁的，那么 {🍖🍕} 等所有包括 {🍖} 的项集均不是频繁项集。

下面来详细描述下 Apriori 算法的执行步骤。

#### **生成频繁项集**

根据 support 生成频繁项集：

  1. 求 k（初始值为 1）项集 的 support，比如{🍖} 和 {🍺} 等；
  2. 去除小于 support 阈值的 项集，留下满足条件的；
  3. 使用第 2 步中得到的项集，排列组合所有可能的组合，得到 k+1 项集；
  4. 重复 2 和 3，直到没有新的项集产生。

假设 support 阈值为 2/8，举例如下：

  1. 1 项集：{🍺}: 6/8 {🍖}: 4/8 {🍕}: 4/8 {🍉}: 1/8 {🍭}: 1/8 {🍼}:4/8
  2. 去除小于 2/8 的项集，得到：{🍺}{🍖} {🍕} {🍼}
  3. 排列组合第 2 步的项集，得到：{🍺🍖}: 3/8 {🍺🍕}:4/8 {🍺🍼}:3/8 {🍖🍕}:2/8 {🍖🍼}:0 {🍕🍼}:2/8
  4. 去除小于 2/8 的项集，得到：{🍺🍖}{🍺🍕} {🍺🍼}
  5. 排列组合第 4 步的项集，得到 {🍺🍖🍕}:2/8 {🍺🍖🍼}:0/8 {🍺🍕🍼}:2/8
  6. 去除小于 2/8 的项集，得到{🍺🍖🍕}{🍺🍕🍼} 
  7. 排列组合第 6 步的项集，得到 {🍺🍖🍕🍼}:0/8

所以根据 Support 得到的项集如下：

{🍺🍖}{🍺🍕} {🍺🍼}{🍺🍖🍕}{🍺🍕🍼}

#### **生成关联规则**

基于频繁项集，根据 Confidence 或者 Lift，生成最终需要的关联规则。在此以计算 Confidence 为例子：

1\. 输入为上一小节中的输出：{🍺🍖}{🍺🍕} {🍺🍼}{🍺🍖🍕}{🍺🍕🍼}

2\. Confidence{🍺->🍖} = Support{🍺🍖} / Support{🍺} = 1/2

Confidence{🍺->🍕} = Support{🍺🍕} / Support{🍺} = 2/3

Confidence{🍺->🍼} = Support{🍺🍼} / Support{🍺} = 1/2

{🍺🍖🍕} 又可以分裂成{🍺 -> 🍖🍕}{🍖 -> 🍺🍕}{🍖🍕->🍺}等多种组合，{🍺🍕🍼} 同理。

3\. 过滤小于阈值的弱关联关系，得到最终的强关联关系。

挖掘结束。

最终的可供线上使用的结果表：

itemset1 | itemset2 | confidence/lift  
---|---|---  
X | Y | 关联度  
  
与协同过滤类似，根据用户行为商品（集）X 来推荐关联度比较高的商品 Y 。这一步就步多说了。

#### **优缺点**

相信细心的读者应该发现了 Apriori 算法的优缺点了：

优点：

  * 易懂
  * 易实现

缺点：

  * 当商品个数很大时，计算量巨大
  * 需要多次扫描整个数据集

Apriori 算法的优化也有很多方法，比如采样、数据分区等。

好在已经有巨人为我们搭好了桥铺好了路，早在 2000 年，韩嘉炜等人就研究出了另一种频繁项集挖掘算法——FPGrowth$^2$。

而这种算法，我们打算运用在地鼠商城生产环境中，来解决购物车等弱/强转化场景推荐列表的点击率上不去的问题。

咱们，下篇见。

* * *

注释 1：<https://en.wikipedia.org/wiki/Association_rule_learning>

注释 2：<https://www.cs.sfu.ca/~jpei/publications/sigmod00.pdf>

