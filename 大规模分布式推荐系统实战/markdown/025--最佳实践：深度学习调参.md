地鼠商城自从上线了深度模型之后，业务指标又有了大幅度的提升，这下不仅 BOSS
开心，推荐组的小伙伴们也乐此不疲地投入到“数据处理—调参—训练模型—优化数据—调参—训练模型……”这个循环往复的过程中去，每个人都对 Deep
Learning 时不时发明出的“酷炫”概念痴迷不已。但是好景不长，渐渐地大家就感受到了枯燥和厌烦——为什么论文中的结果我复现不出来？为什么这么厉害的
tricks 我用起来就不行？为什么有这么多需要人工干预的参数？

是的，深度模型在能够很好的拟合高纬度非线性数据之外，也有它自身的缺点，这其中最显著的缺点之一就是：超参太多了。稍微罗列一下就有：learning
rate、batch size、hidden layers、hidden size、epochs、optimizer......

这些超参的名字有的翻译成中文比较别扭，比如 batch size 翻译为“批大小”，这就很难以理解，所以本篇就保留了部分参数的英文名。

这么多超参，不可能也不允许每一个都花大量的时间去调节。一般采用的策略是将参数排好优先级，然后按照优先级，高优先级的精调，低优先级的粗调或者不调，以此来训练出一个不错的模型。即便如此，在精调/粗调的过程时，不管对参数进行
**网格搜索** （Grid Search）还是 **随机搜索** （Randomized
Search），在面对海量训练数据和复杂模型结构时，都显得力不从心，整个调参的过程特别耗时，而且也特别占用训练资源，时间和资源成本都有点高。

本篇从我在地鼠商城摸爬滚打若干年的经验出发，对深度模型中常见的重要超参给出一些合理的建议和默认值的选取，读者们可以以此为基础，根据自己的实际情况进行一定程度的调整，相信调参这一步不会成为整个建模流程中的一道屏障。

深度模型的超参实在太多了，每个参数的重要性可能在每个人的眼里都不一样，超参的调节一直都是深度学习领域一大痛点，这方面的研究以及工具的发展目前也是如火如荼，感兴趣的读者可以自行搜索
AutoML 等相关概念和技术，可能未来的某一天，作为算法工程师的我们，再也不用关心超参如何调节了。

本篇的调参仅限于推荐系统的模型建模，CV 或者 NLP 领域的参数调节会有各自的 tricks。

### 学习率

学习率是所有超参中当之无愧最重要的超参。如果只允许调节一个超参，那么它一定是学习率。如果想要训练出一个质量好的模型，学习率必须要好好的调节。如果学习率太大，模型训着训着就容易发散（一般表现为
loss 不降反升），如果太小，模型收敛速度又让人无法接受（一般表现为 loss 下降得特别特别慢）。

训练初期，模型远远没有拟合数据，所以此时的学习率会设置的稍微大一点也不会错过最优解。随着模型看到越来越多的数据，拟合的越来越好，学习率就应该适当地减小，以便让模型在最优解附近不断的小幅震荡并最终达到最优解。初始的学习率一般设定在
$10^{-6}$ 和 $1$ 之间，可以从 0.01 开始$^1$，那么重点就在于如何让学习率有效地变化起来。

> 分布式训练时，初始学习率的设定，可以参考下 Facebook
> [这篇文章](https://arxiv.org/abs/1706.02677)，能够获得更多启发。

将 **学习率的变化方式** 被称为学习率调整策略（Learning Rate
Scheduler），下面就来总结一下常用的学习率调整策略，均假设初始学习率为 $\eta$。

常数策略基本上已经弃用了，所以不再赘述。

#### **Inverse Time Decay**

学习率衰减公式为：

$$\eta_t = \eta \times \frac{1}{1+r \times \frac{t}{s}}$$

$\eta_t$ 表示第 t 步（相当于 TensorFlow 中的 global step，自动增加） 的学习率，r 表示衰减系数（decay
rate，小于 1，人工设定），s 表示衰减步长（decay steps，人工设定）。

#### **Exponential Decay**

学习率衰减公式为：

$$\eta_t = \eta \times r^{\frac{t}{s}}$$

参数含义同 Inverse Time Decay，但是相比于 Inverse Time Decay 在 t
增大时学习率衰减的越来越缓慢，此衰减策略衰减的更快，因为学习率每 s 步就会衰减 r 倍。

#### **Polynomial Decay**

学习率衰减公式为：

$$\begin{aligned}t &= min(t, s) \\\\\eta_t &= (\eta - \eta_{min}) \times
(1-\frac{t}{s})^{p} + \eta_{min}\end{aligned}$$

$\eta_{min}$ 表示最小学习率（需要人工设置）。p 是指数项，当 t>s 时，学习率不会再变，恒为
$\eta_{min}$。如果不想让学习率固定不变，Polynomial Decay 也提供了对应的解决办法，如下式：

$$\begin{aligned}s &= s \times \lceil \frac{t}{s} \rceil \\\\\eta_t &= (\eta -
\eta_{min}) \times (1-\frac{t}{s})^{p} + \eta_{min}\end{aligned}$$

变化在于步长 s 的取值，它不再是定值，当 t 是 s 的整数倍时，s 被置为 t，$\eta_{t} = \eta_{min}$；当 t 不是 s
的整数倍时，s 被置为第一个大于 t 且是 s 整数倍的数，$\eta_{t} >
\eta_{min}$。可见这是一个周期变化的学习率，它不会保持恒定，也不会一直递减，而是在 $\eta_{min}$ 和 $\eta$ 之间不断震荡。

#### **Piecewise Constant**

这种学习率策略很好理解，分段学习率——在前 N 步采用学习率 $\eta_1$，在第 N 步到第 M 步采用学习率 $\eta_2$，接下来一直采用学习率
$\eta_3$。虽然它也达到了学习率衰减的效果，但是需要手动调节各阶段的步数以及学习率，所以在实际应用中使用的并不多。

最佳实践：从 Exponential Decay 或者 Polynomial Decay 开始都是不错的尝试。还有一种 Learning Rate
Range Test 技术，可以先用少量数据探测出学习率的上界和下界，然后正式训练时在上下界之间不断的震荡，有兴趣的读者可以研究一番。

### Batch Size

Batch Size 不是越大越好。深度学习界的大神 Yann Lecun 2018 年 4 月 在推特上发过一段推文，引用如下：

> Training with large minibatches is bad for your health. More importantly,
> it's bad for your test error. Friends don’t let friends use minibatches
> larger than 32. Let's face it: the only people have switched to minibatch
> sizes larger than one since 2012 is because GPUs are inefficient for batch
> sizes smaller than 32. That's a terrible reason. It just means our hardware
> sucks.

中心思想就是不要设置 Batch Size 超过 **32** 。那么为什么还有那么多的机器学习任务的 Batch Size 设置的很大甚至都达到了
4K、8K 这样的量级呢？Lecun 大神说那是因为我们当前的硬件太差了，没法很有效地训练 Batch Size 小于 32 的数据。

这也说明，在训练模型时，需要在速度和精度上找到一个折中，大的 Batch Size 一般可以更好的利用硬件资源，提高训练速度；小的 Batch Size
相当于自然引入了噪声从而可能会带来更好的泛化性。

因此：

  * 如果算力跟的上，使用小一点的 Batch Size，32 是一个很好的初始值。
  * 否则使用大一点的 Batch Size，提高训练资源利用率，256、512 都是很好的初始值。

最佳实践：Batch Size 设置为 32 是个很不错的尝试，如果因此带来的训练时长不可接受，可以调高到 256、512
或者更大，以便更好的利用计算机资源。

### Epoch

训练轮/次数，不用特别的设置，早停（early stopping）技术可以让我们不怎么需要关心这个超参。

最佳实践：使用 early stopping 自动找到合适的训练轮/次数。

### 隐藏层数

隐藏层的个数一般可以设置为 3 层，当然要是时间充裕的话，可以从 1 层开始慢慢叠加。

最佳实践：全连接层的隐藏层一般设为 3 层即可，隐藏层不包含输入和输出层。

### 隐藏节点数

一般全连接层呈塔型，从输入层到输出层的节点个数呈递减趋势。假设输入层维度为 D，则可以将第一层节点数设置为小于 D 的最大 2
的幂次方，每一层的节点数可以设置为上一层的一半。

比如输入层的维度为 1000，小于 1000 的最大 2 的幂次方为 512，即第一层可以设置为 512，第二次为 256，以此类推。

最佳实践：节点个数为 2 的 N 次方，更好的利用计算机资源。

### 激活函数

现如今的激活函数越来越多，从当初的 sigmoid，到现如今的 elu、selu 等等，不管怎样，首先让所有的隐藏层都使用 relu 或者 leaky
relu 一般也不会有太大的问题，而且像 elu、selu 会让训练速度减慢，如果训练时间和线上性能不是问题的话，不妨尝试一下。

输出层的激活函数与具体的任务类型有关，二分类一般是 sigmoid，多分类一般是 softmax 等等。

最佳时间：激活函数可以默认使用 relu/leaky relu。

### 权重初始化

对于深度模型中的参数初始化问题，以下原则基本可以让模型初始化不会成为问题：

  * 如果激活函数是 tanh，则初始化策略可以选择 Xavier/ Glorot。
  * 如果激活函数是 relu/leaky relu，则初始化策略可以选择 He。

最佳实践：根据不同的激活函数，选择不同的初始化策略，想知道为什么的读者可以参考[这篇博文](https://towardsdatascience.com/hyper-
parameters-in-action-part-ii-weight-initializers-35aee1a28404)。

### Optimizer

常见的优化器有 SGD、Momentum、Ftrl、Adagrad、Adadelta、Adam、Nadam
等等，对于像电商这样的平台来说，特征数据可能会非常稀疏，有的经常出现（比如热销的商品），有的又可能一个月才出现不了几次（比如不常来的用户），所以那些学习率自适应的
Optimizer 就成为了首选。

类似 Adam 这样的优化器采用了二阶矩的方式来处理稀疏特征，对于训练速度是有很大的影响的，而且会带来一定的不稳定性。

这里推荐优先尝试 Adagrad 或者 SGD + Momentum， 兼顾了速度和精度。如果时间充裕，可以再尝试 Adam 或者 Nadam
这样的更为复杂的 Optimizer。

最佳实践：优先尝试 Adagrad 或者 SGD + Momentum。

### 其他

  * 抵挡住从零手写模型的诱惑：优先从 GitHub 上找已有实现。
  * 抵挡住随意改模型结构的诱惑：经典 Paper 中的网络结构一般都是作者们精心设计的，如果没有特别需求，不推荐自行修改。
  * 数据质量的重要性 >>> 模型的结构复杂性：一定要把重心放在特征质量上，往往这会给模型质量带来质的提升。
  * 避免把大量时间花在调参上：模型的好坏取决于数据的质量，数据质量高的情况下，使用默认的超参也能取得很好的效果，切不可把调参的 tricks 当作首要任务。
  * ……

### 小结

深度模型中的超参个数之多令人头疼，在日常工作中其实并没有很多的时间留给我们去尝试各种各样的组合，时间成本和人力成本都比较大，而且超参的调节其实并不是我们想象中的那么重要，通常来说只要数据质量高，特征做的足够精准，即使只用默认的超参，也能得到很好的线上效果。

学习率是最最重要的超参，比较值得去调试，相对来说收益也是最大的一个。

其他的超参按照上述最佳实践来设置，也基本可以满足需求。

到目前为止，我们已经掌握建模和调参的技术，但是还有一个很重要的方面没有涉及：现实世界中使用真实数据建模时需要的很多问题，要知道真实数据可不是比赛或者实验数据，各种各样的异常值缺失值很容易让人抓狂。下一篇，我们就将目光转向现实世界中对于推荐系统排序算法建模时遇到的一堆问题。

咱们，下篇见。

注释 1：[Practical recommendations for gradient-based training of deep
architectures](https://arxiv.org/abs/1206.5533)

