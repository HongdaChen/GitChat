在第 17 篇中，对于召回模型常用的离线评估指标，例如精准率、召回率和 NDCG
等等，已经做了很详细的说明，介绍了各自的原理和计算方法。同样地，排序模型也有其对应的离线评估指标。排序模型最常见的应用场景主要在点击率预估和转化率预估，两者都属于
0/1 分类任务，此类任务最常用的离线指标是 AUC（Area Under Curve），本篇将会对 AUC 的原理以及实现方法。

既然 AUC 的全称为 **曲线下的面积** ，那么其中的 **曲线** 又是什么曲线呢？在具体到 AUC 之前，先来看看 ROC，因为 AUC
定义中的曲线正是 ROC 曲线。

### ROC

ROC 曲线的全称是 Receiver Operating Characteristic
curve（中文名就不翻译了，太过别扭了），乍一看名字很长，也很难以理解，但是其背后的原理却是极其的简单。既然它是一条曲线，那么它就由横坐标和纵坐标组成，好在
ROC 的横纵坐标我们并不陌生——FPR 和 TPR.

#### **FPR 和 TPR**

在二分类任务中，label 为 0 的样本称为 [N]egative，为 1 的样本称为 [P]ositive。

  * $FPR=\frac{FP}{N}$，也即预测为正，实际为负的样本数，与真实负样本数的比例
  * $TPR = \frac{TP}{P}$，也即预测为正，实际为正的样本数，与真实正样本数的比例

如果不记得 TP 和 FP 是什么意思的同学，记得再去熟悉下第 17 篇。从计算公式上来看，TPR 与召回率是一样的。

计算方式虽然简单，但是有一个问题，虽然 label 有确切的 0 和 1，但是二分类任务的预测值一般都是概率，介于 0 到 1
之间，怎么能根据预测的概率值计算 FPR 和 TPR 呢？

#### **ROC**

既然预测值只是一个概率，没法确定是 0 还是 1，那么就人为的制造 0 和 1，这种人为制造是通过 **阈值** 来实现的。

以点击率预估任务为例，假设模型预测一条样本中某个物品被点击的概率为 0.20，如果阈值设为 0.30，则这条样本就被预测为 0（负样本），如果阈值设置为
0.10，那么就会被预测为 1（正样本）。这样一来，预测值也有了 0 和 1，就可以计算 FPR 和 TPR 了。

ROC 曲线的 **绘制步骤** ：

  1. 设定阈值集合 L，比如 [0,0.01, 0.02, 0.03, …, 1.0]，假设集合长度为 N
  2. 模型对当前数据集做出预测，得出数据集中每条样本的概率值
  3. 遍历集合 L 中的每个阈值
    1. 根据当前阈值与第二步中的预测概率值，得到预测值被判定为 0 还是 1
    2. 计算数据集在当前阈值下的 FPR 和 TPR.
  4. 利用第 3 步生成的 N 个 (FPR, TPR)，对应二维坐标系中的 N 个点，连成一条线，画出 ROC 曲线。

ROC 曲线体现的是在所有阈值处，模型的 0/1 分类能力。

从上述步骤可以得出以下结论：

  1. 阈值为 0 时，预测全部为正，FPR = 1，TPR = 1
  2. 阈值为 1 时，预测全部为负，FPR = 0，TPR = 0

ROC 曲线图$^1$ 如下所示。

![](https://images.gitbook.cn/804ea200-7f45-11eb-b512-e7e8583927cc)

上图中展现出几条不同的 ROC 曲线，不同的曲线对应的模型具有不同的分类能力，ROC
曲线的拐角越接近左上角，其分类能力越好，越接近红色虚线，分类能力越差。当模型是完全随机分类时，几乎没有什么分类能力，绘制出的 ROC
曲线即是红色虚线（RANDOM CLASSIFIER，随机分类器）。

既然 ROC 可以衡量模型的分类能力，那么可不可以量化这种分类能力呢，比如下图所示，到底是分类器 c1 的分类能力好还是分类器 c2 的分类能力好呢？

![](https://images.gitbook.cn/a44776f0-7f45-11eb-99ed-79d8da7ea20b)

由于 ROC 只能定性地给出模型分类能力的好坏，而无法定量地给出确切的结论，因此 AUC 就应运而生了。

### AUC

AUC 计算的是 ROC 曲线下的面积，计算的方法有多种，这里主要讲两种——梯形法和概率法。

以上图中分类器 c1 的 ROC 曲线为例，其对应的 AUC 需要计算的面积如下图所示：

![](https://images.gitbook.cn/b0456160-7f45-11eb-97c6-6bc10d28a8ab)

#### **梯形法**

这个面积要怎么算呢，相信观察仔细的同学应该看出来了，上图中的 AREA 由 5 个小梯形组成，如下图所示。

第一个梯形其实是三角形，不过可以将三角形理解成上底为 0 的梯形：

![](https://images.gitbook.cn/b9ac7450-7f45-11eb-8b9c-0fb0e23bbf21)

可见，分别将 5 个梯形的面积计算出来之后，再相加就算出来了 ROC 曲线对应的 AUC 值了。

每个梯形面积的计算公式如下：

$$\begin{aligned}Area_{trapezoid}=\frac{(tpr_2+tpr_1) \times (fpr_2-fpr_1)}{2}
\end{aligned} \tag{1}$$

AUC 的 **计算步骤** ：

  1. 设定阈值集合 L，比如 $[0.01, 0.02, 0.03, ..., 1.0]$，假设集合长度为 N。
  2. 模型对当前数据集做出预测，得出数据集中每条样本的概率值
  3. **从小到大** 遍历集合 L 中的每个阈值。
    1. 根据当前阈值与第二步中的预测概率值，得到预测值被判定为 0 还是 1。
    2. 计算数据集在当前阈值下的 FPR 和 TPR。
  4. 利用第 3 步生成的 N 个 (FPR, TPR)，对应二维坐标系中的 N 个点，根据公式 (1) 计算出 N - 1 个梯形面积，累加得到 AUC。

AUC 的计算步骤与 ROC 的绘制步骤差别仅仅在最后一步。

AUC 计算完毕后，是一个介于 [0, 1] 的值，越大越好，0.5 表示该分类器完全是随机分类，小于 0.5 表明模型出了问题，需要排查。

模型的 AUC 越高并不代表线上的效果越好，涉及到线上线下指标一致性的问题，下一章节会有详细探讨。

AUC 同时也有它的物理意义，它表示 **给定任意一条正样本和任意一条负样本，模型把正样本排在负样本前面的概率** 。

现在已经知道 AUC 是如何计算了，但是有没有更好的实现方法？毕竟当前这种计算方法还需要人为指定 **阈值列表** ，可不可以摆脱这种人为指定呢？

可以，当然可以$^2$。

#### **概率法**

不需要阈值的 AUC 计算方式有很多种，这里介绍一种通俗易懂的，对 AUC 感兴趣的同学可以做更进一步的探索。

既然 AUC 的物理意义是 **正样本排在负样本前面的概率** ，那么我们可以把这个 **概率** 给算出来。

假定数据集共有 M + N 个样本，M 个正样本，N 个负样本。那么正样本的预测概率 $P_P$ 和 负样本的预测概率 $P_N$ 可以组成 M * N 个
$(P_P, P_N)$ 对。统计一下 M * N 个对中 $P_P$ 大于 $P_N$ 的个数，再除以 M * N，即可得出
**正样本排在负样本前面的概率** 。

$$AUC = \frac{\sum I(P_P,P_N)}{M \times N} \tag{2}\\\$$

$$I(P_P,P_N)=\left\\{\begin{aligned}1.0 &, & P_P > P_N \\\0.5 &, & P_P = P_N
\\\0.0 &, & P_P < P_N\end{aligned}\right. \tag{2}$$

可以看到这种计算方式的时间复杂度为 $O(MN)$，且不再不需要梯形法中的阈值了。

那么，用梯形法计算出来的 AUC 和 用概率法计算出来的 AUC 能一样吗？接下来做个小测试，校验一下这两者是否给出相同的 AUC。

#### **AUC 校验**

在很多算法面试中，很多应聘者会被要求“手撕”AUC 实现方法。

这一小节，就来手动实现上述梯形法和概率法计算的 AUC。为了验证实现的正确性，导入了 scikit-learn 库，将手动实现的结果与 sklearn
库计算的结果加以比对。代码如下：

    
    
    from sklearn.metrics import roc_curve
    from sklearn.metrics import auc
    
    
    # 梯形法
    def trapezoidal(labels, predictions, thresh_num):
        # 阈值从大到小排列
        thresholds = [(thresh_num - i) / thresh_num for i in range(thresh_num + 1)]
        tpr_fpr = []
    
        p = sum(labels)
        n = len(labels) - p
    
        for threshold in thresholds:
            this_tp = 0
            this_fp = 0
            for label, prediction in zip(labels, predictions):
                if prediction >= threshold:
                    if label > 0:
                        this_tp += 1
                    else:
                        this_fp += 1
            tpr = this_tp / p
            fpr = this_fp / n
            tpr_fpr.append((tpr, fpr))
    
        _auc = 0
        for i in range(1, len(tpr_fpr)):
            tpr_1, fpr_1 = tpr_fpr[i - 1]
            tpr_2, fpr_2 = tpr_fpr[i]
            _auc += (tpr_2 + tpr_1) * (fpr_2 - fpr_1) / 2
    
        return _auc
    
    
    # 概率法
    def probabilistic(labels, predictions):
        p_ranks = [i for i in range(len(labels)) if labels[i] == 1]
        n_ranks = [i for i in range(len(labels)) if labels[i] == 0]
        m = len(p_ranks)
        n = len(n_ranks)
    
        num_p_ge_n = 0.0
        for p_rank in p_ranks:
            for n_rank in n_ranks:
                p_p = predictions[p_rank]
                p_n = predictions[n_rank]
                if p_p > p_n:
                    num_p_ge_n += 1.0
                elif p_p == p_n:
                    num_p_ge_n += 0.5
    
        return num_p_ge_n / (m * n)
    
    
    if __name__ == '__main__':
        _labels = [1, 1, 1, 0, 0, 0, 0, 0, 0, 0]
        _predictions = [0.3, 0.5, 0.7, 0.9, 0.8, 0.6, 0.4, 0.1, 0.2, 0.0]
    
        print('trapezoidal: {}'.format(trapezoidal(_labels, _predictions, 100)))
        print('probabilistic: {}'.format(probabilistic(_labels, _predictions)))
        _fpr, _tpr, _ = roc_curve(_labels, _predictions, pos_label=1)
        print('sklearn: {}'.format(auc(_fpr, _tpr)))
    
    ### output
    # trapezoidal: 0.5714285714285714
    # probabilistic: 0.5714285714285714
    # sklearn: 0.5714285714285714
    

这里也能看出，梯形法的时间复杂度 $O(T(M+N))$，T 是阈值个数。概率法的时间复杂度已经提过，是 $O(MN)$.
当然还有更加快速的实现版本，可以将计算时间复杂度降低到 $O((M+N)log(M+N))$，在这里就不展开了，充满好奇心的同学可以自行搜索。

### 小结

推荐系统中的排序模型建模大部分都是二分类任务，一般是点击率预估、加车率预估或者转化率预估等等，此类任务最常用的评估指标就是 AUC 了，它是 ROC
曲线下的面积。

AUC 的好处在于：

  * 完全与排序有关，不关心预测值是否精确
  * 与阈值无关，从概率法中就可以看出，完全不需要阈值，也能算出 AUC

当然，它也有缺点：

  * 不关心预测值是否精确，这又成为了它的缺点，有时候这个缺点是致命的，特别是当预测值与其他值相结合时（比如，利用点击率乘以转化率再乘以物品价格得到的结果进行排序，如果点击率/不准，排序质量会受到很大的影响）。
  * 只能从整体上给出排序能力，但是推荐系统中我们更在乎头部位置的排序能力，毕竟大部分用户只会看头部的物品，不会再往下翻了。

对于 AUC，通常算法工程师是不需要手动实现，但是在求职过程中经常会被遇到如何实现 AUC
的问题，因此对于它的物理意义和实现方法，最好还是知其所以然会比较好。

掌握 AUC 的原理，最最重要的是在实际工作中遇到线下 AUC 高但是线上表现不好时，知道从何入手，这也正是下一节的主要内容——线上线下不一致问题的探究。

咱们，下篇见。

注释 1：<https://commons.wikimedia.org/w/index.php?curid=70212136>

注释 2：[The meaning and use of the area under a receiver operating
characteristic (ROC)
curve.](https://pubs.rsna.org/doi/10.1148/radiology.143.1.7063747)

