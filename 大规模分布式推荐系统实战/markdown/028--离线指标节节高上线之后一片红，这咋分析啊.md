到目前为止，地鼠商城的深度模型开发之旅经历了以下 3 步：

  1. 做好了数据
  2. 搭建了网络
  3. 生成了模型：线下 0.9 的 AUC

此时不上线更待何时，于是我摩拳擦掌满怀地希望的告诉 BOSS 模型可以上线了并且拍着胸脯信誓旦旦地保证业务指标（点击率/转化率/GMV
等）绝对有正向提升。BOSS 静静地看着我，好像是让我不要这么激动舒缓下情绪，等我平复完了之后，BOSS 才回答道，先放量观察几天。

我心想，小样儿，瞧你这语气，是不相信我是不！

几天后，看着 AB 平台上每天的红色数字，透心凉，像是大庭广众之下被人打耳光，不由自主地脸颊发红，感叹自己还是太年轻，对 BOSS 的佩服又增加了一点。

> AB 平台一般正向提升显示为绿色，负向提升显示为红色。

模型当然逃脱不了最终下线的命运，但是这其中的缘由必须要找出来，为什么线下 AUC 那么高，但是线上的指标那么差呢？

我们需要找到问题的突破口，在深度学习领域，模型 DEBUG 是非常困难的，特别是已经上线的模型，因为一个模型从诞生到上线，需要经过除了上述 3
个步骤外，还有 Serving 时从各个独立的服务获取特征喂入模型得到预测值等，链路特别地长，任何一个环节出了问题都可能导致线上效果变差。

这也侧面说明了在实际应用中，对于数据质量、模型质量的监控是必不可少的，不仅能够及时发现问题减少损失，也能够帮助工程师快速定位问题、解决问题。

特征包括了用户特征、物品特征等，这些特征应该有专门的服务对外提供接口供下游调用。

本篇会探讨常见的可能会出现的问题，但是线上环境错综复杂，这里也没有办法罗列出所有的原因。

### 算法与业务目标不一致

虽然这个问题很少发生，但是还是需要检查一下，必须保证算法优化的目标与业务关心的目标是一致的。

比如算法优化的是 **点击率** ，业务关心的也是 **点击率** ，这样两者就达成了一致。或者算法优化的是 **点击率** ，业务关心的是 **GMV**
（单位时间内的成交额），那么也算是达成一致，因为点击率提升一般会带来 GMV 的提升。如果算法优化的依然是 **点击率** ，但是业务关心的是
**转化率** ，这个时候就要注意了，点击率的提升不一定会带来转化率的提升。

好在一般情况下，这个问题不会成为问题，因为在一开始便会根据业务场景具体的指标去建模，大方向上不会发生太大的偏差。

### 验证集的选择

关于训练、验证和测试集的划分，会有 k-fold cross validation 和 holdout。

简单说明一下这两者的工作原理。

k-fold cross validation: 将训练数据平均分成 k 份，其中 1 份作为验证集，剩下 k - 1 份作为训练集。训练 k 次，这样 k
份数据中每 1 份都被当作验证集 1 次。

![](https://images.gitbook.cn/cc4e9640-7f47-11eb-a903-df02001ec930)

holdout：可以认为是 k-fold cross validation 的特例，将数据按一定的比例分成 2 份，1 份作为训练集，另 1 份作为验证机。

![](https://images.gitbook.cn/db6643d0-7f47-11eb-8b07-ed093c29262b)

在海量数据下使用 k-fold cross validation 显得有点力不从心了，那么就只能使用 holdout 了吗？也不是。

在推荐系统中，不管是 cross validation 还是 holdout，如果不做任何干预，这两者都会带来 **数据穿越问题**
，非常致命。穿越问题主要发生在线下模型训练环节，比如在今天的训练数据中却含有明天的数据，这种使用到未来信息的问题被称为穿越问题。

那么为什么在很多别的系统中，这两种验证方式不会有数据穿越问题呢？因为推荐系统与数据的日期是有强关联的，用户的行为与时间息息相关，利用用户的未来信息去“预测”过去的行为当然会“准确”很多。数据穿越问题不仅难以发现，而且会给开发者造成
**模型质量很好** 的假象。所以一般情况下验证集可以选择训练集的 **最后一天数据** ，如果担心一天数据不够代表性，可以按照如下策略来生成验证集：

假设训练集有 M 天数据，则每 N 天训练集，对应 1 天验证集，以此类推，如下图所示：

![](https://images.gitbook.cn/e4d015e0-7f47-11eb-a60e-3fe7fd8eeb93)

上图中，训练集共有 21 天数据，第 22 天作为测试集，第 1 到 5 天作为训练数据，第 6 天作为验证数据，然后第 6 到 10
天可以作为训练数据，第 11 天作为验证数据...... 最后使用 4 份验证集的产生的 4 个离线指标（AUC
等）的平均值作为验证指标，据此选择最优的超参。最优超参选择完毕后，将对应的最优模型对测试集进行评估，据此作为离线最终指标。

数据穿越不仅会发生在数据集划分时，同样也会发生在特征上，特征的穿越会更加难以发现，在构造特征时需要特别的小心，尤其是行为序列特征，这种特征的内容是用户的历史行为，一旦发生穿越，那么历史行为会与数据的
Label（是否点击/购买） 具有强关联，所以一定要小心、小心、小心。

### GAUC

AUC 高，线上效果差的另外一个很重要的原因就是 AUC
本身——它衡量的是模型的全局排序能力。然而个性化推荐中的排序是千人千面的，排序的好坏应该在同一个用户下去评判，不同用户之间的排序结果并不能比较，全局的排序能力高并不能真实反映个性化排序的能力。

假设有 6 条数据和两个模型，具体 Label 和预测值如下：

用户 | 物品 | 点击 | 模型 1 | 模型 2  
---|---|---|---|---  
USER1 | ITEM1 | 1 | 0.9 | 0.8  
USER1 | ITEM2 | 0 | 0.3 | 0.4  
USER1 | ITEM3 | 0 | 0.1 | 0.3  
USER2 | ITEM1 | 1 | 0.8 | 0.3  
USER2 | ITEM2 | 0 | 0.3 | 0.2  
USER2 | ITEM3 | 0 | 0.1 | 0.1  
  
模型 1 的 AUC 为 1.0，模型 2 的 AUC 为 0.8125，单纯从 AUC 上来看模型 1 显然优于模型
2，但是如果观察的稍微仔细一点就会发现，只考虑 USER1 时，模型 2 的排序结果 AUC 为 1，同理，只考虑 USER2 时，模型 2 的排序结果
AUC 也为 1，这说明实际上模型 1 和 2 的排序能力是一样的，那么全局 AUC 给出的结论（模型 1 优于模型 2）就会指引出一个错误的方向。

GAUC$^1$（Grouped AUC）正是用来应对这种情况，GAUC 首先计算出每个用户下的 AUC，然后对所有用户的 AUC 进行加权平均，得到最终的
GAUC，计算公式如下：

$$GAUC=\frac{\sum_{i=1}^nw_i*AUC_i}{\sum_{i=1}^nw_i} \tag{1}$$

式中，$i$ 表示用户，$w_i$ 表示用户 $i$ 的权重，$AUC_i$ 表示用户 $i$ 的数据计算出的 $AUC$. 通常 $w_i$ 可以设为用户
$i$ 的曝光次数或者点击次数。

因此，在实际应用中，AUC 高线上效果好时，固然皆大欢喜，但是当 AUC 高线上效果差时，说明 AUC 已经不能真实反映模型的排序质量，此时可以计算一下
GAUC，如果 GAUC 差，那么说明 GAUC 和线上效果比较一致，如果 GAUC 也很高，那么就要再看看接下来的方法了。

### AUC 分析

微软在 2013 年发表了一篇专门阐述此类问题的论文$^2$，这也是目前笔者见到过的分析 **线上线下不一致** 问题的最好的论文，非常值得深究。

强烈建议所有参与点击率/转化率预估等排序任务的读者一定要仔仔细细、一字不落且理解透彻这篇 paper，实用性极强，对于问题分析也给出了一个很好的思路。

AUC 的缺点一就是对预测值不敏感，如下所示：

    
    
    label = [1, 1, 0, 0, 0]
    prediction1 = [0.9, 0.9, 0.1, 0.1, 0.1]
    prediction2 = [0.09, 0.09, 0.01, 0.01, 0.01]
    assert AUC(label, prediction1) == AUC(label, prediction2)
    

可以看出，虽然两组预测值差别巨大，但是对应的 AUC 却是一样的，所以为了弥补这个问题，就需要其他指标进行辅助，比如 MAE （平均绝对误差，Mean
Absolute Error）和 PE（预测误差，Prediction Error）。

这两个指标的计算公式如下：

$$\begin{aligned}MAE &= \frac{1}{n}\sum_{i=1}^{n}|p_i-c_i| \\\PE &=
\frac{avg(p)}{\gamma}-1\end{aligned} \tag{2}$$

式中，$p_i$、$c_i$ 分别是第 $i$ 个样本的预测值和 $label$，$p$ 是平均预测概率，$\gamma$ 是真实概率。

AUC 的缺点二就是它对整体数据进行评估，然而现实中往往是这种情况：

  1. 需要对 A 场景的点击率预估建模
  2. 线下用到了 ABCDEFG 等场景的数据进行训练和评估，得到模型 M
  3. 线上将 M 用在 A 场景

这就带来了问题，因为此时的评估不是针对 A 场景的，所以一般情况下对于模型的评估，要遵守：

  1. 在什么场景上线就用什么场景的数据进行 **评估**
  2. 评估数据能不采样就不采样

即使这样，依然不能解决 AUC 的缺点二，我们希望能够对 AUC 观察的更 **局部** 一点。

相信大家还记得 AUC 的梯形法计算方式——根据预测值区间去计算 AUC.
如果知道预测值区间内的细节，那么对于算法开发者来说是大有裨益的，论文也给了我们某些启发。

如下图所示$^2$ 便是启发之一，图中画出了 **预测概率（pClick）** 与 **预测误差（PE）**
的关系图，展示出每个预测区间内部的预测误差情况，能够明显看出在 model-2 在低预测区间和高预测区间都存在高估（预测概率大于真实概率），导致的结果就是
model-2 的离线 AUC 比 model-1 好很多，但是线上指标却差很多。

低区间的高估危害比高区间的高估危害大的多，看论文之前，你能想到原因吗？

当然，论文里还有很多其他的特别详实的内容，等待着读者去探索，这是一篇绝对不会让读者失望的论文。

![](https://images.gitbook.cn/7209a660-7f48-11eb-a60e-3fe7fd8eeb93)

### 小结

线上线下不一致在推荐系统中是特别常见的问题之一，本文也只是探讨了以 AUC
为指标时的情况，还有很多别的指标（NDCG、PRECISION、RECALL、ACCURACY
等等）也会发生不一致的问题，一旦发生，就必须仔细对待，否则会严重影响模型的方向和线上的收益。

AUC 的主要问题是对具体的预测值不敏感以及对整体数据进行评估，很多时候这会蒙蔽人类的双眼，误以为模型质量很好。所以需要一些辅助指标，比如
MAE、PE、LOSS 等等，来辅助 AUC 对模型的预测值进行评估，还需要一些手段来观察 AUC
内部的细节，特别是模型在各个预测区间内的高估和低估情况，对于日常工作中分析问题特别重要也特别有用。

线上线下不一致的可能原因实在太多，没有办法一一罗列，也不可能全都弄清楚，有的甚至笔者都没有遇到过，希望读者们在实践中遇到问题并且解决之后，一定要把经验记录在案以备下次避免踩坑。

下一节的主题是推荐系统中另外一个永远也回避不了的难题——冷启动，包括用户冷启动和物品冷启动。

咱们，下篇见。

注释 1：[Deep Interest Network for Click-Through Rate
Prediction](https://arxiv.org/abs/1706.06978)

注释 2：[Predictive Model Performance: Offline and Online
Evaluations](https://dl.acm.org/doi/10.1145/2487575.2488215)

