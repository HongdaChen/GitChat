> Coming up with features is difficult, time-consuming, requires expert
> knowledge. “Applied machine learning” is basically feature engineering.—
> Andrew Ng.

在 **召回部分** ，我们并没有用太多的笔墨来介绍 **特征** ，但是这不代表它不重要，恰恰相反， **特征工程**
能力是一项必须掌握的技能，甚至说是算法工程师最需要掌握的技能也不为过。

特征是原始数据和模型之间的桥梁，如下图所示：

![](https://images.gitbook.cn/42407540-4856-11eb-b57c-c78410229605)

原始数据的格式丰富多样，有音频、视频、图片、文字等，但是大部分模型的输入只接受数值。如何对原始数据进行处理，将其转化为模型接受的类型，这不仅仅是技术问题，更需要的是对具体业务的理解，因为不同的领域对数据的处理千差万别，比如图像算法工程师处理图像，NLP
工程师处理文本等等。

本篇将专注于推荐系统领域，详细介绍常用的特征工程技术。

虽然深度学习已经大大减少了特征工程的工作量，但是我们依然需要了解数据，在各自的业务领域熟知不同的数据在不同的模型上如何处理。因为本章的主题是推荐系统中的特征工程，实践中基本上都是通过深度学习来建模，所以本章的内容主要关注深度模型建模时的特征处理。

### 特征类型

特征按照类型可以划分成：

  * 类别特征（Categorical）
  * 数值特征（Numerical）
  * 序列特征（Sequential）

#### **类别特征**

类别特征，顾名思义，就是用来表示一个类别的特征。比如性别就是一个类别特征，它的取值有男、女和未知。国家也是一个类别特征，它的取值可以有中国、古巴、越南、老挝等。这类特征没有数学意义，不能对其进行类似加减乘除的操作。

类别特征又可以细分成 **无序类别特征** 和 **有序类别特征** 。

**无序类别特征**

无序类别特征，指的是类别之间没有什么可比性，比如商品颜色特征，红、黄、蓝等就属于这类特征。

**有序类别特征**

相反，有序类别特征，指的是类别具有了某种程度上的可比性，比如商品质量特征，差、良、优就属于这类特征，它们的值具有了一定意义的顺序性，但是这种顺序性只能定性，没法定量，只能说“优”比”良“好，具体好多少，就不得而知了。

#### **数值特征**

这类特征本身已经具有了数学意义，可对其进行数值运算，比如商品的重量、价格，商品的曝光次数、点击次数等等。这类特征虽然可以直接作为特征输入模型，但是并不意味着不用对其处理了，不同的算法模型对于数值特征的要求也不一样，比如树模型对数值的取值范围没有要求，但是线性模型一般要求数值落在[0,
1] 之间。我们在后面会看到数值特征的处理手法。

#### **序列特征**

这类特征不再是 **单** 值，而是 **多值** ，可以理解成特征取值是一个数组，数组里的元素可以是类别/连续型。比如，用户在过去一个月内点击的商品
ID，这就是一个序列特征，这也是推荐系统中最常用的序列特征之一。对于序列特征的处理有时候会很大程度上影响模型质量的好坏。

接下来，我们来看看推荐系统中一般建模时需要的原始特征该如何选取。

### 原始特征选取

推荐系统中时时刻刻都演绎着关于人（用户）和物（商品）在某个时刻（时间）某个地点（比如首页猜你喜欢场景）发生某件事情（用户产生了点击/加车/购买行为等）的故事。因此，对于原始特征的挖掘一般来自于以下五个方面：

  * 用户画像
  * 物品画像
  * 上下文/环境
  * 用户的历史/实时行为
  * 上述四种特征的交叉

下表列出了一些常用的特征。这里总结的特征虽然主要适用于类似地鼠商城这样的电商推荐系统，但是对于视频、文章等推荐场景，大体上的特征也不会超过这个范围，稍微注意下融会贯通即可。

序号 | 类别 | 名称 | 备注  
---|---|---|---  
1 | 用户画像 | 用户 ID |  
2 | 用户画像 | 年龄 | 见说明 1  
3 | 用户画像 | 性别 | 见说明 1  
4 | 用户画像 | 会员等级 |  
5 | 物品画像 | 物品 ID |  
6 | 物品画像 | 物品所在店铺 ID |  
7 | 物品画像 | 物品一/二/三级类目 | 见说明 2  
8 | 物品画像 | 物品价格 |  
9 | 上下文/环境 | 场景 ID | 见说明 3  
10 | 上下文/环境 | 用户使用的手机型号/浏览器版本号 |  
11 | 上下文/环境 | 用户使用的手机品牌/浏览器名称 |  
12 | 上下文/环境 | 用户接入系统的网络类型 | 4G/5G/WiFi……  
13 | 上下文/环境 | 用户所在省份 |  
14 | 上下文/环境 | 用户所在城市 |  
15 | 用户行为 | 用户过去 15 天内点击的物品 ID | 见说明 4  
16 | 用户行为 | 用户过去 30 天内加车的物品 ID |  
17 | 用户行为 | 用户过去 30 天内收藏的物品 ID |  
18 | 用户行为 | 用户过去 60 天内下单的物品 ID |  
19 | 用户行为 | 用户过去 15 天内点击的店铺 ID |  
20 | 用户行为 | 用户过去 30 天内加车的店铺 ID |  
21 | 用户行为 | 用户过去 30 天内收藏的店铺 ID |  
22 | 用户行为 | 用户过去 60 天内下单的店铺 ID |  
23 | 用户行为 | 用户过去 15 天内点击的一/二/三级类目 ID |  
24 | 用户行为 | 用户过去 30 天内加车的一/二/三级类目 ID |  
25 | 用户行为 | 用户过去 30 天内收藏的一/二/三级类目 ID |  
26 | 用户行为 | 用户过去 60 天内下单的一/二/三级类目 ID |  
| 交叉特征 | 上述四种特征的交叉特征…… |  
  
  1. 一般对于用户的性别和年龄是直接获取不到的（当然能直接获取到最好，电商一般会通过用户注册的身份证号获取性别和年龄信息），所以这里的性别和年龄一般需要另外的任务预测出来。
  2. 所谓的一/二/三级类目，是说物品其实是有具体的类目层级的，比如《算法导论》这本书，其一级类目是 **图书** ，二级类目是 **计算机科学** ，三级类目是 **编程语言与程序设计** 。类似的层级关系也可以拓展到其他领域，比如音乐推荐，《后来》这首歌，它的层级关系可能是 **华语/女声/情歌** 等等。
  3. 比如首页猜你喜欢场景有 ID、物品详情页也有其 ID、购物车页也有其 ID，一般情况下这些 ID 是不会变化的。
  4. 这类特征不同于一般的单值特征（比如用户 ID 就是一个单值特征），它是一个数组/列表，用来描述用户过去一段时间内的行为物品序列，比如 [IID2, IID1, IID3] 表示用户过去一段时间内点击了物品 ID2、物品 ID1、物品 ID3。

### 特征工程

获取到原始的数据之后，就需要着手处理这些数据，将其转变成可输入模型的特征，按照特征的类型不同其处理方式也会有所差异。

这里的特征工程都是基于深度学习建模来做的，不再赘述传统算法模型的特征工程。所谓 **万物皆可 embedding** ，深度学习模型的输入几乎都是
embedding，我们要做的就是以一些合理的手法将各种各样的特征转换为 embedding。

#### **类别特征**

类别特征的特征工程在这个时代已经简单了很多，不管是特征的值很多（比如用户 ID）还是很少（比如用户性别）均可以先 hash 再得到 embedding。

![](https://images.gitbook.cn/8bb95b60-4856-11eb-a5aa-e9f4ccbc8f49)

以 **用户 ID** 为例，用户 ID 的类型一般是字符串类型，比如：

> uid:3141259

  1. 对其进行 hash 再求模，得到一个 [1, V] 范围内（0 一般用于异常情况）的一个整数值 H。
  2. 以 H 为行号，去 $V \times D$ 的矩阵里抽取第 H 行的元素，得到 $1 \times D$ 的向量。
  3. 此向量即为用户 ID“uid:3141259”的 embedding。

其中 $V \times D$ 矩阵称为 **用户 ID embedding 矩阵** 。

这里需要注意 V 和 D 的取值。

**V 的大小**

V 是 hash 桶数：

  * V 太小， hash 冲突会增多，导致大量不同的特征映射到同一个桶里，造成特征区分度不够，不利于模型学习。
  * V 太大，对内存的要求大大提高，比如 V 是 10000000，D 是 300，则矩阵 $V \times D$ 占用的内存大小很容易就超过 10G 了。

假设原始特征的个数为 k（比如 10,000,000 个用户 ID），则在 hash 桶数为 V 的前提下，hash 冲突的概率为$^1$：

$$P = 1-e^{\frac{-k\times(k-1)}{V}}$$

如果 k 很小，则很容易设置一个不大的 V 使得 P 很小，这种情况很好办。 如果 k 很大，比如 10000000，那么即使 V 也设置
10000000，P 还是几乎等于 1，这种情况下可以尝试几组不同的 V 值，观察内存使用情况和模型的指标变化，权衡之后，选择最优的 V。

**D 的大小**

D 是特征的 embedding size ，通常按照下式$^2$ 进行设置就可以了

$$D = 2^{\lceil log_2k^{0.25} \rceil} \ \ \ \ // 为了让 D 是 2 的幂次方$$

比如 k 为 10000000，则 D 为 64。当然这只是 **经验法则** ，也可以根据需要自行调整 D 的大小。

#### **数值特征**

数值特征在作为深度模型的输入时，通常需要做归一化处理，一般的处理思路如下$^{3,4}$：

  1. 假设数值特征为 x，在训练数据集中统计其 n 个分位数，得到 n-1 个区间；
  2. 将特征 x 进行分段，根据具体的数值映射到其中一个区间 i 上（$i \in [0,n-1]$）；
  3. 得到归一化特征 $\tilde{x}=\frac{i}{n-1}$；
  4. 为了捕捉数值特征的高次项和低次项，再生成两个新特征 $\tilde{x}^2$ 和 $\tilde{x}^{0.5}$‘’

对数值特征取分位数的好处是能够极大降低异常数据的影响，提高数据质量和模型的鲁棒性（robustness）。

比如年龄特征，可以按照以下区间分段：

  * 0: [0 - 17]
  * 1: [18 - 24]
  * 2: [25 - 35]
  * 3: [36 - 44]
  * 4: [45 - 54]
  * 5: [55 - 64]
  * 6: [65 - 79]
  * 7: [80 - $\infty$]

如果数据集中出现了年龄为 999 的异常数据，则会落入第 7 段，并不会造成太大的影响。

当然，上述处理方式是将数值特征依然作为数值特征来使用的。如果将数值特征变换为类别特征，则可以按照上一小节的类别特征处理方式得到特征
embedding，这种处理方式更普遍一些。操作起来也很简单，在 **第 2 步** 将 x 映射到区间 i 上之后，将 i
作为类别特征对待即可（也就没有了 **第 3、4 步** 了）。

#### **序列特征**

此类特征一般表示的是用户的历史行为，以 **用户过去 15 天内点击的物品 ID** 特征为例，讲解这种特征该如何处理。

假设物品 ID 的 embedding 矩阵为 $W_{V \times D}$，某用户过去 15 天内点击的物品 ID 为 [ID1, ID 1, ID
2, ID 3]。则我们可以对此序列进行 POOLING$^{4,5}$、ATTENTION$^6$ 等操作。

**POOLING**

POOLING 主要有两种方式：SUM 和 AVERAGE。

**SUM POOLING**

就是将序列中的 4 个 ID 分别进行 EMBEDDING 操作，得到 4 个向量，对这 4 个向量按元素对位相加，得到 **1**
个全新的向量，将此向量作为模型的输入。

$input \ embedding = embedding1 + embedding2+embedding3+embedding4$

如下图所示（省略了 hash 的过程）

![](https://images.gitbook.cn/edf51a70-4857-11eb-8e68-bb4362622133)

**AVERAGE POOLING**

顾名思义，就是将上述的求和操作变成求平均操作，得到的向量作为模型的输入。

$input \ embedding = \frac{embedding1 + embedding2+embedding3+embedding4}{4}$

![](https://images.gitbook.cn/0fae3890-4858-11eb-a007-1b095f083930)

在实践中，优先尝试 SUM POOLING。

**ATTENTION** $^6$

POOLING 处理方式是将序列中的每个元素当作同等重要来对待，可以看成每个元素的权重都是 1。

但是显然序列中的点击物品，越靠近现在越重要，比如 1 个月前点击的物品重要性当然不如昨天点击的物品。所以更加合理的处理方式应该是这样的：

![](https://images.gitbook.cn/25f07690-4858-11eb-b57c-c78410229605)

$input \ embedding = weight1 \times embedding1 + weight2 \times
embedding2+weight3 \times embedding3+weight4 \times embedding4 $

上式中的 weight，正是 ATTENTION 机制 发挥作用的地方——它可以学习出各个 weight 的大小。

关于 ATTENTION 机制，其背后的原理有兴趣的同学一定要看看它最初的 paper$^8$。

### 小结

本文讲述了机器学习中最重要的部分——特征，其重要性再怎么强调都不为过。

我们对特征进行了分类，并解释了各自的区别，列出了推荐系统中常用的特征，基本上在建模时，这些特征几乎都要被包含在其中的。

对于类别特征，这也是在日常工作中处理最多的一种特征，在深度学习时代，embedding 几乎成为了标准的类别特征处理手段。我们也给出了类别特征的 hash
桶数 V 以及 embedding size D 的如何选择。

数值特征转化为类别特征是实践中很常见的操作，当然也可以依然将其作为数值特征输入到模型中，要注意数值特征作为深度模型输入时需要归一化。

最后关于序列特征，这是一类比较复杂的特征，也是极为重要的一类特征，处理得当的话能够很好的捕获用户兴趣爱好从而使推荐结果更为精准。序列特征除了介绍的
POOLING 和 ATTENTION 之外，还有一些别的处理手段$^7$，不管怎样，其共同目的都是尽量深挖用户的历史/实时兴趣爱好。

仅仅特征工程就可以写一本书了，所以这篇文章只是冰山一角，有一些很好的关于特征工程的书籍$^{9,10}$，也希望同学们有时间一定要去好好研究。

下一篇，我们从简单的模型入手，看看最为普通的 LR，从而引入 LOSS 函数、正则化等概念。

  * 注释 1：[hash 冲突概率的计算](https://preshing.com/20110504/hash-collision-probabilities/)
  * 注释 2：[embedding size 设置](https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html?m=1)
  * 注释 3：[Wide and Deep](https://arxiv.org/abs/1606.07792)
  * 注释 4：[Youtube Recommendations](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/45530.pdf)
  * 注释 5：[Sampling-Bias-Corrected Neural Modeling for Large Corpus Item Recommendations](https://research.google/pubs/pub48840/)
  * 注释 6：[Deep Interest Network for Click-Through Rate Prediction](https://arxiv.org/abs/1706.06978)
  * 注释 7：[Behavior Sequence Transformer for E-commerce Recommendation in Alibaba](https://arxiv.org/abs/1905.06874)
  * 注释 8：[Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)
  * 注释 9：<http://www.feat.engineering/>
  * 注释 10：Feature Engineering for Machine Learning - Principles and Techniques for Data Scientists

