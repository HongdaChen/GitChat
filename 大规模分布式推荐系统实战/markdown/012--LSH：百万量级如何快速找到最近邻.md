随着地鼠商城上架的商品越来越多，已经从当初的几千个飞速上涨到百万级别之后，Word2Vec
从数据源到相似商品表这其中最耗时的一步已经不是模型训练而是相似商品的计算，根据上一篇的计算方法，计算全量商品的相似度时间复杂度为 $O(M^2)$，M
是商品个数。在这个时代，数据量越来越大，搜索广告推荐等领域的实体（广告、商品等）也越来越多， **找相似**
这种需求成为了一种“刚需”，ANN（近似最近邻查找）就用来专门解决这个问题。相对于暴力穷举（Brute
Force，BF）在高时间复杂度下得到的准确的相似结果（Ground Truth），ANN 算法在牺牲一点精度的情况下可以快速得到相似结果。

如果某个场景能够允许 90~95% 的准确性（即允许 ANN 给出的 topN 相似结果中有 90~95% 的结果确实在 Ground Truth 的
topN 中），那么 ANN 是找相似的首选技术。

地鼠商城中的诸多推荐场景，当然满足上述要求，于是我们用众多 ANN 技术中的一种解决了 Word2Vec 找相似缓慢的问题，这便是 LSH（Local
Sensitive Hash，局部敏感哈希）。

### Hash

Hash 简单来说就是将任意长度的输入，通过 Hash 函数/算法转换成一个固定长度的输出，输出的值称为 Hash
值或者是桶号。通过定义可以知道，不同的输入有可能会得到相同的输出，这被称为 Hash 冲突。

比如，如下就是一个简单的 Hash 函数，将字符串转换成一个 $[0, mod)$ 之间的数：

    
    
    int simpleHash(String input, int mod) {
        int hash = 0;
        for (int i = 0; i < input.length(); ++i) hash += input.charAt(i);
        return (hash % mod);
    } 
    

下面这个 Hash 函数就稍微复杂点：

    
    
    int anotherSimpleHash(String input) {
        int hash = 0;
        for (int i = 0; i < input.length(); ++i) hash = 31 * hash + input.charAt(i);
        return hash;
    }
    

可以看出第二个比第一个 Hash 函数发生冲突的可能性小。一般情况下我们是希望数据经过 Hash 后尽量不要发生冲突，但是 LSH 却又强烈的依赖这种冲突。

LSH 的基本思想如下：

> 如果两份数据的 Hash 值相同，那么这两份数据有很大的概率是近邻，换句话说，如果 Hash
> 值不同，那么就有很大的概率不是近邻——由此，当我们要查询某份数据的最近邻时，只需要将该数据进行 Hash 得到 Hash 值，然后只遍历与该 Hash
> 值相同的数据，即可找到其近邻。也就是说，通过不断 Hash 操作，将原始较大的数据集切分成了多个较小的子集，每个子集内的数据都具有相同的 Hash
> 值，在查找近邻时，只考虑子集内的数据进行计算，这样计算量大大减少了。

如下图所示，将所有数据分别进行 Hash 后，看到 d4 和 d5 落到了同一个 Hash 桶里，那么根据 LSH 的基本思想，我们认为 d4 和 d5
很大可能是近邻，具有高相似性。这样在计算 d4 的最近邻时，只需要在 8 号桶内的数据查找就可以了，计算量骤减。

![](https://images.gitbook.cn/d7f9f370-27dd-11eb-be76-5555cd71111f)

以上我们说了这么多，那么 LSH 到底是怎么做的呢？Hash
函数得到的都是随机值，怎么能保证相似度高的数据以高概率落在同一个数据桶内呢？一直在说计算量大大减少，那么能不能定量给出到底减少了多少？相比于最原始的需要
$O(N)$ 的时间复杂度去计算得到一份数据的最近邻，LSH 的时间复杂度怎么算呢？而且有 Hash 就难免会有 Hash 冲突，这不可避免地会为 LSH
带来错误率（这也是为什么 LSH 是 **近似** 最近邻搜索算法），那这个错误率能不能也可以量化，好让我们作为算法人员心里有个数呢？

带着这一系列疑问，咱们来对 LSH 抽丝剥茧。

### LSH 算法

#### **具体案例**

我们先看看简单的二维空间。

> 需求：在 $xy$ 平面坐标系中，有 ABCDE 五个点，对于点 A，求它的近似最近邻点。

既然是为了讲解 LSH，那么就不提 **遍历一遍求最相似的点** 这种解决办法了。我们从另外一个角度来考虑这个问题。

如果两个点靠的很近，那么它们的 $x$ 坐标应该很接近，有了，先把这些点在 $x$ 轴上的投影画出来。

![](https://images.gitbook.cn/e7a9c250-27dd-11eb-97c1-1314be3cdf10)

可以看到 B 在 $x$ 轴上与 A 最接近。

好，那 B 就作为 A 的最近邻点候选之一。

同理，如果两个点靠的很近，那么它们的 $y$ 坐标也应该很接近，再把这些点在 $y$ 轴上的投影画出来。

![](https://images.gitbook.cn/efc2d170-27dd-11eb-9825-558323e50dbd)

这回 C 在 $y$ 轴上与 A 最接近。C 也是 A 的最近邻点候选之一。

现在为了得到 A 的最近邻点，分别使用 ABC 三个点的坐标计算 AB 和 AC 的距离，可以看到 AC 比 AB 靠得更近，C 就作为 A 的最近邻点了！

明白了这个道理，咱们再来瞧瞧 LSH 是怎么做的。

> 需求不变，还是求点 A 的最近邻点。

LSH 不会使用 $x$ 轴和 $y$ 轴，而是随机生成 k 条直线，每条直线将二维空间一分为二，一个空间中的点全标记为 1，另一个空间中的点全标记为
0，如下图所示（k = 3）：

![](https://images.gitbook.cn/ff8bf4b0-27dd-11eb-9e6e-492b4066adc9)

可见，A 点经过红黄绿 三条直线的划分后，得到了一个新的签名（signature）：1 0 1。

同理，可以得到 5 个点的签名:

点 | 签名  
---|---  
A | 1 0 1  
B | 1 1 0  
C | 0 0 1  
D | 1 0 1  
E | 0 0 0  
  
然后，LSH 将具有同样签名的点存放在一起，计算近邻点时只考虑同签名的点，对于点 A 来说，就是点 D。

问题来了，明明 C 是 A 的最近邻点，可以却因为随机直线划分时没有将 A 和 C 划分在一起导致只能选择 D 点，这样生成的结果错误率未免也太高了。

为了应对这个问题，LSH 算法的解决方案是：再随机第二组 k 条直线，依旧是每条直线将二维空间一分为二，重复上述步骤，如下图所示：

![](https://images.gitbook.cn/1006dbc0-27de-11eb-badb-4943f989e399)

这一轮 A 点经过红黄绿三条直线的划分后，得到了第二个签名：0 0 0。

同理，得到 5 个点的新签名：

点 | 签名  
---|---  
A | 0 0 0  
B | 1 1 0  
C | 0 0 0  
D | 1 1 1  
E | 1 0 0  
  
经过这一轮之后可以看到，与点 A 相同签名的点为 C。而上一轮与点 A 相同签名的点为 D，于是在计算 A 点的近邻点时，会将 C 和 D
均作为候选参与计算，最终得到了最近邻 C 点。

#### **算法原理**

经过以上具体例子的说明，相信聪明的你已经可以总结出 LSH 算法的基本原理了，可谓简单之至。

LSH 算法：假设数据共有 N 个点，每个点的维度为 D，希望生成的签名长度为 k。

1\. for p from 1 to N：

  * 将 D 维的点 p 乘以 D 行 k 列 的矩阵 W，得到一个 k 维的向量（这一步就是个向量映射的操作，由 D 维映射到了 k 维）；
  * k 维向量中的每一位，大于 0 则置为 1，否则置为 0；
  * 经过第 b 步处理后的向量作为 p 的签名（signature） end for。

2\. 将上述 for 循环重复 L 轮，即可得到所有点的 L 个签名，每个签名长度为 k。

3\. 将拥有同样签名（只要两个点的 L 个签名中任意一个一样，那么就可以说它们拥有同样的签名）的点存储在一起。

4\. 在计算近邻点时，只需遍历相同签名的点，计算相似度（距离）即可。

这里简单说明以下第 3 点，两个点的签名一样，并不是要求它们 L 个签名完全一样，只要有一个一样即可。比如上一小节中的点 A 和点 C，L（L 为
2）个签名中第二个是一样的，那么就可以认为它们签名相同，存储在一起。

#### **复杂度分析**

可以看到，这个算法如此直接、简单明了。那么，作为一个喜欢打破砂锅问到底的 Programmer 来说，这么浅显的知识显然满足不了优秀的你/我。

在这里不禁要问，对于一个点 p，要找到它的最近邻点 q，LSH 算法比暴力穷举好多少呢？它的查找时间复杂度是多少呢？

> 假设：数据点个数 N，每个点维度 D，签名长度 k，签名个数 L。

带着上述假设，我们来分析 LSH 算法的时间复杂度。

不做任何数据结构的优化，暴力穷举查找点 p 的最近邻点 q 的时间复杂度为 O(N)。

首先我们只考虑一个签名的情况：因为 L 个签名彼此互相独立，所以只计算在一个签名下的时间复杂度后，再乘以 L 就可以了。

要查找点 p 的最近邻 q，需要经过如下步骤：

  * 计算点 p 的签名：将 p 的 D 维向量映射成 k 维度，得到签名 sp，时间复杂度 $T1 = Dk$（D 维向量乘以 D 行 k 列的矩阵 W）。
  * 签名 sp 下平均点的个数 $n = N / 2^k$（每个维度取值为 1 或 0，那么 k 维可以取不同值的个数为 $2^k$，N 个点平均落在每个 k 维签名里的个数为 $N / 2^k$）。
  * 对这 n 个点进行遍历，根据某个相似度公式计算得到最近邻 q，此时计算的是签名内部的点的相似度，都是 D 维的点，所以遍历 n 个点的时间复杂度为 $T2 = Dn = DN/2^k$。
  * 综合时间复杂度 $T = T1+T2 = Dk + DN/2^k$。

上述只是当签名个数为 1 时的用时，那么为 L 时：

$$T = L(Dk + DN/2^k) = LDk + LDN/2^k$$

当取 $k = log(N)$ 时

$$T = LDlog(N) + LDN/2^{logN} = LDlog(N) + LD = O(log(N))$$

哇，LSH 将近邻搜索的查找时间从 $O(N)$ 降低到了 $O(log(N))$，厉害厉害......

然而？

### 下篇预告

对于需要通过向量来查找最近邻的任务来说，LSH 作为 ANN 算法家族的一员，较早地扛起了这个任务。

可以看到，LSH 基本上是 **大事化小**
的态度——将整个数据集按照签名（signature）分门别类，进而只在各自的签名下去计算近邻，大大减少了计算量。通过时间复杂度分析也可以看到，LSH
的平均时间复杂度能达到 $O(logN)$ 这么优秀的水平，不得不感慨前人的智慧。

但是，总有个但是，LSH 快是快了，但是有多准呢？我怎么知道它的准确性呢，万一速度上来了精度下去了也没有什么用呀。

下一篇，咱们来分析分析 LSH 的错误率以及其再实际中的应用和不足。

下篇见。

