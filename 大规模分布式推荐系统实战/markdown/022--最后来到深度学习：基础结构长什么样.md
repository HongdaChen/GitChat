经过了逻辑回归、FM，再到 LTR，终于来到了深度学习这个在当下已经成为标配的建模方式（在后面我们会看到如何将 LTR 与深度学习结合）。

地鼠商城在将深度学习应用到推荐系统的过程中，吃了很多的“苦”，当然最终的结果让我们又获得了很多的“甜”。这其中涉及到数据处理、训练框架、模型服务、任务调度等等一系列不可规避的问题需要解决，从零搭建一套这样的系统，绝非一两个人在短期内可以完成的，本篇以及接下来的内容会先从数据处理和训练框架这两个方面入手，详解如何从原始数据开始到最终产出可用的深度模型。至于模型服务和任务调度会留到第三部分再思考具体的解决方案。

我们讨论的模型均是推荐系统的排序模型，有监督且一般情况下是二分类。

本篇以及后续内容均有 TensorFlow 的代码，希望同学们提前掌握其基本用法。

TensorFlow 版本：1.15。

![](https://images.gitbook.cn/cc36f4b0-58e9-11eb-9c81-71e9be275b25)

图 1 深度模型基本结构

自底向上，为输入到输出的数据流向，其中 $x$ 是原始输入，经**预处理**送入全连接层，$W$ 是矩阵，表示各层的参数，$y$ 是最终的输出。

具体来说，$x_1$，$x_2$，...，$x_n$ 经过：

  1. 特征工程后，变成模型可以识别的数值输入（ **第 19 节** 中已经具体阐述过不同特征的处理方式）
  2. 查询 embedding 矩阵，得到一个/多个 embedding 向量
  3. 将这些向量连接在一起，作为全连接层的输入

由上图可知，模型需要“学习”的参数为：

  * 所有 embedding matrix 中的元素值
  * 所有 $W$ 矩阵中的元素值

每一层全连接隐藏层（fc layer）的细节如下所示：

![](https://images.gitbook.cn/ef002340-58e9-11eb-8ca0-ad2e0df56a32)

图 2 隐藏层

上一层的输出 $H_{L-1}$ 作为下一层的输入 $X_L$，$X_L$ 经过第 $L$ 层的参数 $W_L$ 进行基本的矩阵线性运算后得到
$Z_L$，至此与一般的线性模型并无区别，但是紧接着对 $Z_L$ 再施加一个函数（称为激活函数，一般情况下是非线性的），得到
$H_L$，将此结果作为下一层的输入 $X_{L+1}$。图中每一层中的圆称为神经元（neuron）或者隐藏节点（hidden unit）。

用数学符号来翻译上述描述，就得到了公式$(1)$。

$$\begin{aligned}Z_{W,b}(X) &= XW + b \\\H &= \phi(Z)\end{aligned} \tag{1}$$

  * $X$ 是每一层的输入，形式为向量，它可能是多个向量首尾连接组成的一个向量，如上图中 $x_{L,1},x_{L,2}...,x_{L,5}$ 这 5 个向量拼接起来组成了 $L$ 层的输入。
  * $W$ 是每一层的参数，是形状为**当前层的输入维度 **$\times$ **当前层的输出维度** 的矩阵，当前层的输入维度，其实也就是上一层的输出维度。
  * $b$ 是每一层的 bias 参数，它是可选的。每个隐藏节点都会有一个 bias 参数，其为一个 **数值** 。每一层的多个隐藏节点的 bias 参数形成了当前层的 bias **向量** 。
  * $Z$ 是线性矩阵线性运算的结果。
  * $H$ 是对 $Z$ 施加激活函数后的结果，既是当前层的输出，又是下一层的输入。

可以看到，其实每一层的处理都是相似的，也都很好理解。

在掌握了深度模型的基础结构以及内部的一些细节之后，就需要考虑如何用 Tensorflow 来实现简单的模型搭建工作。

根据图 1 中的数据流向，我们先来将各式各样的特征转变为 embedding。

如果对特征处理很熟悉了，请忽略这条提示，否则最好先熟悉下第 19 篇的内容哦。

### Feature Column

这一小节会介绍 Tensorflow 中一系列特别好用的特征处理函数，均来自于 **tf.feature_column**
包，这些函数的目录层级如下图所示：

![](https://images.gitbook.cn/1f46fe70-58ea-11eb-9723-f7cdd0e57a3f)

图 3 feature column 函数

为了直观的说明各个函数的使用方法和场景，假定某数据集中有以下特征：

名称 | 格式 | 示例 | 备注  
---|---|---|---  
user_id | 字符串 | "uid012" | 用户 ID  
age | 整型 | 18 | 异常值: 999  
gender | 字符串 | "0" | 取值 "0", "1", "未知"  
item_id | 字符串 | "item012" | 物品 ID  
clicked_items_15d | 字符串列表 | ["item012", "item345"] | 15 天内点击的物品  
  
这些特征覆盖了 类别特征、数值特征以及序列特征。

我们最终的目的是将所有特征都转变为 embedding。

    
    
    import tensorflow as tf
    import tensorflow.feature_column as tfc
    import math
    
    sess=tf.Session()
    
    def print_column(features, columns):
        inputs = tfc.input_layer(features, columns)
        sess.run(tf.global_variables_initializer())
        print(sess.run(inputs))
    
    def embedding_size(category_num):
        return int(2 ** math.ceil(math.log2(category_num ** 0.25)))
    

#### **类别特征**

**1\. user_id**

此类 **类别特别多** 的特征的处理方法先 hash，再 embedding。

    
    
    categorical_column_with_hash_bucket, embedding_column
    

代码如下所示：

    
    
    features = {'user_id': ['uid012']}
    user_num = 100 # 用户数
    buckets = 10 # hash 桶数
    # (1). hash 值 对 buckets 求 mod,注意这里的参数 key,很重要,必须与 features 中的 key 保持一致
    user_hash = tfc.categorical_column_with_hash_bucket(
        key='user_id', 
        hash_bucket_size=buckets, 
        dtype=tf.string)
    user_embedding_size = embedding_size(user_num)
    # (2). hash 值对应的 embedding
    user_embedding = tfc.embedding_column(user_hash, user_embedding_size)
    # shape: (1, 4)
    print_column(features, [user_embedding])
    

注释说明：

  * (1) 处内部调用了 **tf.string _to_ hash _bucket_ fast** 将 string 进行 hash。
  * (2) 处内部生成了一个 buckets $\times$ user_embedding_size 的 embedding 矩阵，此矩阵由 TensorFlow 生成并管理，但是后面会看到，有时候我们需要使用这个矩阵，就需要手动生成了。

**2\. gender 和 item_id**

处理方式和 **user_id** 相同。

可能大家会对 gender 特征有疑惑：此特征只有 2、3 个值，有必要 hash 吗，one-hot 就行了吧？虽然也可以对类别较少的特征进行 one-
hot 处理，但是实践中还是先 hash 再 embedding 操作。

#### **数值特征**

**age**

年龄特征一般做分段处理，再 embedding。

    
    
    numeric_column, bucketized_column, embedding_column
    

假设分段如下：

  * 0：[0 - 17]
  * 1：[18 - 24]
  * 2：[25 - 35]
  * 3：[36 - 44]
  * 4：[45 - 54]
  * 5： [55 - 64]
  * 6：[65 - 79]
  * 7：[80 - $\infty$]

    
    
    features = {'age': [18]}
    # 1. 读入数值
    raw_age = tfc.numeric_column(
        key='age', 
        dtype=tf.int64)
    boundaries = [18, 25, 36, 45, 55, 65, 80]
    # 2. 分段
    bucketized_age = tfc.bucketized_column(
        source_column=raw_age,
        boundaries=boundaries)
    age_embedding_size = embedding_size(len(boundaries) + 1)
    # 3. 段对应的 embedding
    age_embedding = tfc.embedding_column(
        bucketized_age,
        age_embedding_size)
    # shape: (1, 2)
    print_column(features, [age_embedding])
    

#### **序列特征**

**clicked _items_ 15d**

这类特征比较特殊，为序列特征，一般用户历史行为特征均为此类。

它特殊在序列内部元素的 embedding 其实是 特征 **item _id_** 对应的 embedding，也就是说 **clickeditems
_15d_** 与 **itemid** 的 embedding 矩阵是共享的。

TensorFlow 很贴心的为我们提供了 **shared _embedding_ columns** 这个 API，专门用来处理 embedding
共享问题。

    
    
    features = {'item_id': ['item012'], 'clicked_items_15d': [['item012', 'item345']]}
    # 1. 需要 share 的 feature column 定义除了 key 之外，其他参数必须完全一样
    item_id = tfc.categorical_column_with_hash_bucket(
        key='item_id',                                        
        hash_bucket_size=10, 
        dtype=tf.string)
    clicked_items_15d = tfc.categorical_column_with_hash_bucket(
        key='clicked_items_15d', 
        hash_bucket_size=10,
        dtype=tf.string)
    
    # 2. embedding size 设为 2
    item_id_embedding, clicked_embedding = tf.feature_column.shared_embedding_columns(
        [item_id, clicked_items_15d], 
        dimension=2)
    # shape: (1, 2) ?
    print_column(features, [clicked_embedding])
    

嗯？为什么输出的 embedding 的形状是 $(1, 2)$？直观上来说 clicked\\_items\\_15d 中有 2 个
item\\_id，每个 item_id 对应一个长度为 2 的 embedding，最后输出的形状应该是 $(2,2)$ 才对呀！

原来 shared_embedding_columns 会将序列特征做一个聚合操作（combine)，比如序列中有 N 个元素，每个元素对应的
embedding 长度为 D，则最后的输出形状应该为 $(N,D)$，但是 shared\\_embedding_columns 会这 N 个 D
维的向量做聚合操作，将其变为 $(1,D)$。

假设未聚合的 embedding 数据为 $[[1,2],[3,4]]$，此时 $N=2, D=2$。

聚合操作当前支持：

  * mean：默认聚合操作，求均值，$[[1,2],[3,4]] => ([1,2]+[3,4])/N = [[2,3]]$
  * sum：求和，$[[1,2],[3,4]]=>([1,2]+[3,4])=[[4,6]]$
  * sqrtn：求和除以根号 N，$[[1,2],[3,4]] => ([1,2]+[3,4])/\sqrt{N} = [[2.83,4.24]]$

但是一般情况下，聚合并非想要的结果，这样简单粗暴的聚合会丢失很多的信息，尤其是历史行为之间的信息。有没有办法让序列特征输出的 embedding 形状就是
$(N,D)$ 呢？有，但是稍微麻烦一点，不过这对于熟悉 Feature Column 的底层实现是大有好处的。

直接贴上代码片段，一看便知。

    
    
    features = {
        'item_id': ['item012'], 
        'clicked_items_15d': [['item012', 'item345']]
    }
    
    # 1. 手动计算各 item_id 的 hash 值
    item_id_hash = tf.string_to_hash_bucket_fast(
        features['item_id'], 
        num_buckets=10)
    
    clicked_hash = tf.string_to_hash_bucket_fast(
        features['clicked_items_15d'],
        num_buckets=10)
    
    # 2. 手动生成 item_id 对应的 embedding 矩阵
    item_id_embedding_matrix = tf.get_variable(
        'embedding_matrix', 
        shape=(10, 2))
    
    # 3. 手动查询各 item_id 对应的 embedding 向量
    item_id_embedding = tf.nn.embedding_lookup(
        item_id_embedding_matrix, 
        item_id_hash)
    clicked_embedding = tf.nn.embedding_lookup(
        item_id_embedding_matrix, 
        clicked_hash)
    
    initializer = tf.global_variables_initializer()
    sess.run(initializer)
    # 4. shape: (1, 2)
    print(sess.run(item_id_embedding))
    
    # 5. shape: (2, 2)
    print(sess.run(clicked_embedding))
    

注意到此代码片段里没有用到任何 TensorFlow 自带的 feature column
函数，虽然代码是多了点，但是让我们或多或少明白了这些自带的函数内部实现机制，何乐而不为。

#### **input_layer**

print_column 中调用了 feature column 的 input_layer 方法，签名如下：

    
    
    def input_layer(features, # 1
                    feature_columns, # 2
                    weight_collections=None,
                    trainable=True,
                    cols_to_vars=None,
                    cols_to_output_tensors=None)
    

一般只传入 **features** 和 **feature_columns** 参数，实现数据的转换。具体处理逻辑如下

  * features 提供具体的特征数据，格式为字典，内容：$\\{key_1: value_1, key_2: value_2, ..., key_n: value_n\\}$。

  * feature_columns 提供具体数据的处理函数，格式为列表，内容：$[numeric\\_column,categorical\\_column,...]$，每个 feature column 函数都有一个参数 key。

  * 有了 **数据** ，有了 **数据的处理方法** ，input_layer 根据 feature_columns 中每个 feature column 函数的参数 key，去 features 查找具有相同 key 的数据：

    * 查不到就报错。
    * 查到了，把数据取出来通过 feature column 函数进行数据处理。

### 建模

掌握了特征处理方法之后，搭网络建模型就是很简单的事情了。

使用 TensorFlow estimator API 来搭建模型网络。

步骤如下：

  1. 读数据
  2. 构建模型结构
  3. 数据喂入模型

    
    
    # -*- coding: utf-8 -*-
    import tensorflow as tf # 1.15
    import math
    import sys
    

#### **读数据**

    
    
    def input_fn():
        data = [
            {
                'label': 0, 
                 'user_id': 'uid012',
                 'age': 18,
                 'item_id': 'item012', 
                 'clicked_items_15d': ['item012', 'item345']
            },
            {
                'label': 0,
                'user_id': 'uid012',
                'age': 18, 
                'item_id': 'item012', 
                'clicked_items_15d': ['item012', 'item345']
            },
            {
                'label': 1, 
                'user_id': 'uid345', 
                'age': 18,
                'item_id': 'item012',
                'clicked_items_15d': ['item012', 'item345']
            }
        ]
    
        def generator():
            for instance in data:
                label = instance['label']
                features = {k: instance[k] for k in instance if k != 'label'}
                yield features, label # 1
    
        dataset = tf.data.Dataset.from_generator(
            generator,
            output_types=(
                {
                    'user_id': tf.string,   
                     'age': tf.int64, 
                     'item_id': tf.string, 
                     'clicked_items_15d': tf.string
                }, # features
                tf.int64 # label
            )
        ) # 2
    
        dataset = dataset.shuffle(buffer_size=10000)
        dataset = dataset.repeat(100)
        dataset = dataset.batch(32)
    
        return dataset
    

注释说明

注释 1 与 注释 2：yield 后面的数据格式与 output_types 是一一对应的，这对于后面理解 padded_batch 很有帮助。

#### **构建模型结构**

TensorFlow 要求构建模型的函数签名为：

    
    
    def model_fn(features, labels, mode, params)
    # features: input_fn 中 yield 返回结果中的第一项
    #   labels: input_fn 中 yield 返回结果中的第二项
    #     mode: 由 TensorFlow 传入，表明当前是 TRAIN，还是 EVAL，还是 PREDICT
    #   params: 由开发者传入，一般是一些超参配置，例如 learning_rate 等
    

按照此要求，编写一个最简单的 DNN。

    
    
    # 定义 feature column
    def embedding_size(category_num):
        return int(2 ** math.ceil(math.log2(category_num ** 0.25)))
    
    # user
    user_hash = tf.feature_column.categorical_column_with_hash_bucket(
        key='user_id', 
        hash_bucket_size=10,
        dtype=tf.string)
    user_embedding_size = embedding_size(100)
    user_embedding = tf.feature_column.embedding_column(
        user_hash, 
        user_embedding_size)
    
    # age
    raw_age = tf.feature_column.numeric_column(
        key='age', 
        dtype=tf.int64)
    boundaries = [18, 25, 36, 45, 55, 65, 80]
    bucketized_age = tf.feature_column.bucketized_column(
        source_column=raw_age, 
        boundaries=boundaries)
    age_embedding_size = embedding_size(len(boundaries) + 1)
    age_embedding = tf.feature_column.embedding_column(
        bucketized_age, 
        age_embedding_size)
    
    # item_id, clicked_items_15d
    item_id = tf.feature_column.categorical_column_with_hash_bucket(
        key='item_id', 
        hash_bucket_size=10, 
        dtype=tf.string)
    clicked_items_15d = tf.feature_column.categorical_column_with_hash_bucket(
        key='clicked_items_15d',
        hash_bucket_size=10,
        dtype=tf.string)
    item_id_embedding, clicked_embedding = tf.feature_column.shared_embedding_columns(
        [item_id, clicked_items_15d], 
        dimension=2)
    

然后开始搭建模型：

    
    
    def deep_layers(layer, hidden_units, activation=None, name=''):
        layers = len(hidden_units)
        for i in range(layers - 1):
            num_units = hidden_units[i]
            layer = tf.layers.dense(
                layer, 
                units=num_units,
                activation=tf.nn.relu,
                kernel_initializer= \
                tf.contrib.layers.variance_scaling_initializer(),
                name=name + '_hidden_{}'.format(str(num_units) + '_' + str(i)))
        num_units = hidden_units[layers - 1]
        layer = tf.layers.dense(
            layer, 
            units=num_units,
            activation=activation,
            kernel_initializer=tf.glorot_uniform_initializer(),
            name=name + '_hidden_{}'.format(str(num_units) + '_output'))
        return layer
    
    
    def model_fn(features, labels, mode, params):
        learning_rate = params['learning_rate']
        fc_layers = params['fc_layers']
        inputs = tf.feature_column.input_layer(
            features, 
            [user_embedding,
             age_embedding,
             item_id_embedding,
             clicked_embedding
            ])
        logits = deep_layers(inputs, fc_layers, name='fc')
        probabilities = tf.sigmoid(logits)
    
        # 验证时输出离线指标
        if mode == tf.estimator.ModeKeys.EVAL:
            metrics = {
                'auc': tf.metrics.auc(labels=labels,
                                      predictions=probabilities,
                                      num_thresholds=1000)
            }
            return tf.estimator.EstimatorSpec(
                mode, 
                loss=loss, 
                eval_metric_ops=metrics)
        # 训练时梯度下降更新参数
        elif mode == tf.estimator.ModeKeys.TRAIN:
            global_step = tf.train.get_global_step()
            optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)
            train_op = optimizer.minimize(loss=loss, global_step=global_step)
            return tf.estimator.EstimatorSpec(
                mode, 
                loss=loss,
                train_op=train_op)
    

#### **数据喂入模型**

    
    
    def train_and_eval(net):
        train_spec = tf.estimator.TrainSpec(input_fn=lambda: input_fn())
        eval_spec = tf.estimator.EvalSpec(
            input_fn=lambda: input_fn(),
            steps=10, # 每次验证 10 个 batch_size 的数据
            throttle_secs=180 # 至少每隔 180s 验证一次
        )
        tf.estimator.train_and_evaluate(net, train_spec, eval_spec)
    
    def main(unused):
        run_config = tf.estimator.RunConfig(**{
            'save_summary_steps': 10,
            'save_checkpoints_steps': 100,
            'keep_checkpoint_max': 5,
            'log_step_count_steps': 10
        })
    
        model = tf.estimator.Estimator(
            model_fn=model_fn,
            model_dir='./checkpoints',
            config=run_config,
            params={
                'learning_rate': 0.01,
                'fc_layers': [8, 4, 1]
            }
        )
    
        train_and_eval(model)
    
    if __name__ == '__main__':
        tf.logging.set_verbosity(tf.logging.INFO)
        tf.app.run(main=main, argv=[sys.argv[0]])
    

TADA，一个 DNN 模型就这样被创造出来了。按照步骤来一点难度也没有，无非是 TensorFlow API 的熟悉程度而已，对不对？

完整的代码详见 [GIT](https://github.com/recsys-4-everyone/dist-
recsys/tree/main/chapter%2022)。

### 小结

这一节我们看到了常规的深度模型结构，使用 TensorFlow Feature column API 实现了特征工程，最后使用 estimator API
搭建了一个简单的模型。

但是我们搭建的模型瑕疵还很多，比如：

  * 数据是假的，一般工程中使用分布式存储海量数据，然后 TensorFlow 去读取这些数据。
  * 模型没有导出，上述代码只有模型训练，没有模型导出。
  * 网络简单。
  * ……

下一篇，我们将介绍如何：

  * 用 Spark 来生成海量的训练数据，数据格式为 TFRecord。
  * 实现两个常用的排序模型: Wide and Deep$^2$ 和 DIN$^3$。
  * 导出模型并使用 TensorFlow Serving 让模型实现线上预测的功能。

咱们，下篇见。

  * 注释 1：[Feature Column Google 入门教程](https://developers.googleblog.com/2017/11/introducing-tensorflow-feature-columns.html?m=1)
  * 注释 2：[Wide and Deep](https://arxiv.org/abs/1606.07792)
  * 注释 3：[Deep Interest Network for Click-Through Rate Prediction](https://arxiv.org/abs/1706.06978)

