这一天，推荐小组的组员大 T 跑到我的座位旁边来，发生了以下对话：

> T：阿星，为什么我们的推荐系统这么 LOW？
>
> 我：怎么讲？
>
> T：我们到现在只有协同过滤、关联规则这种老古董、老掉牙的算法，难道不 LOW 吗？
>
> 我：……
>
> 我：咱们做推荐系统的目标是什么？
>
> T：提升咱们线上的推荐质量、为商城带来更多的收入。
>
> 我：那现在这个目标达到了吗？
>
> T：暂时来看，是达到了。
>
> 我：那为什么嫌算法 LOW？老算法新算法，能达到目标的就是好算法，再说了，高大上的算法也不一定有效果。
>
> T：我就是感觉平时学的那些深度学习、强化学习啥的到现在都没用上。
>
> 我：算法终究是为了业务服务的，没有业务支撑，算法就是个花架子，作为一名从业者你要清楚这一点。
>
> T：……
>
> 我：不过我也明白你的想法，毕竟一名优秀的算法工程师总喜欢去尝试各种各样的 IDEA，既然你提到了深度学习，那么我们就首先从基础做起，先试验一下
> Word2Vec（词向量）在推荐系统召回部分的作用。
>
> T：真的么？那太好了！我再回去温习温习！
>
> （蹬蹬蹬蹬蹬……T 一脸满足倏然离去……）

接下来这几篇，咱们就来好好地讲述词向量的原理、公式推导、实现技巧以及场景应用等诸多必须掌握的要点。同时词向量也伴随着深度学习的不断发展几乎成了所有深度模型中必不可少的一部分。掌握了词向量基础，对以后
Deep Learning 的学习有着极大的裨益，所以还请对深度学习感兴趣的读者务必要好好地学习这项技术。

我们先以一个例子来将词向量的概念具体化。

### 学霸与学渣

现在我们有 5 位同学：A、B、C、D 和 E。同时有 5 门课程：语文、数学、英语、物理和化学。

在某一次考试之后，5 位同学的分数如下表所示：

姓名 | 语文（150） | 数学（150） | 英语（150） | 物理（100） | 化学（100）  
---|---|---|---|---|---  
A | 120 | 120 | 120 | 80 | 80  
B | 120 | 120 | 112 | 90 | 85  
C | 90 | 105 | 90 | 75 | 60  
D | 105 | 75 | 90 | 70 | 60  
E | 105 | 60 | 105 | 65 | 60  
  
用向量表示为：

    
    
    A: [120, 120, 120, 80, 80]
    
    B: [120, 120, 112, 90, 85]
    
    C: [90, 105, 90, 75, 60]
    
    D: [105, 75, 90, 70, 60]
    
    E: [105, 60, 105, 65, 60]
    

先将各向量归一化，得到如下归一化后的向量（语文、数学和英语除以 150，物理化学除以 100）：

    
    
    A: [0.80, 0.80, 0.80, 0.80, 0.80]
    
    B: [0.80, 0.80, 0.75, 0.90, 0.80]
    
    C: [0.60, 0.70, 0.60, 0.75, 0.60]
    
    D: [0.70, 0.50, 0.60, 0.70, 0.60]
    
    E: [0.70, 0.40, 0.70, 0.65, 0.60]
    

得到了向量之后，通常情况下我们会去计算相似度得分——cosine 得分，比如：

    
    
    cosine_similarity(A, B) = cosine_similarity([0.80, 0.80, 0.80, 0.80, 0.80], [0.80, 0.80, 0.75, 0.90, 0.80]) = 0.998
    
    cosine_similarity(A, E) = cosine_similarity([0.80, 0.80, 0.80, 0.80, 0.80], [0.70, 0.40, 0.70, 0.65, 0.60]) = 0.983
    

俗话说人以群分，这么看来 A 和 B 是一类人，而 D 与 E 又是另一类人。

但是显然，评判两个人的相似，并不能只看上述 5
门课程的分数，我们还可以考虑政治、历史、生物、地理等，那么向量的维度就会变高。可见，维度越高，向量的表现力就越强，但是要求的计算力就越高。

通过这一小节，我们知道：

  * 可以用向量来表示任何实体（人或者物）
  * 利用向量可以计算实体两两之间的相似度，而且特别方便

这一小节中我们知道向量的每一维度代表什么意思：第一维代表语文分数、第二维代表数学分数……但是在词向量的世界中，我们几乎不可能知道每一维度的具体含义。

### 预训练的词向量

Word2Vec 技术就是将一个单词变成向量的过程，那么这种转变有什么好处呢？

我们先来看看知名的 GloVe 模型$^1$ 训练出来的某些单词的向量表示：

    
    
    import gensim.downloader as api
    wv = api.load("glove-wiki-gigaword-100")
    king = wv['king']
    

KING：

    
    
    array([-0.32307 , -0.87616 ,  0.21977 ,  0.25268 ,  0.22976 ,  0.7388  ,
           -0.37954 , -0.35307 , -0.84369 , -1.1113  , -0.30266 ,  0.33178 ,
           -0.25113 ,  0.30448 , -0.077491, -0.89815 ,  0.092496, -1.1407  ,
           -0.58324 ,  0.66869 , -0.23122 , -0.95855 ,  0.28262 , -0.078848,
            0.75315 ,  0.26584 ,  0.3422  , -0.33949 ,  0.95608 ,  0.065641,
            0.45747 ,  0.39835 ,  0.57965 ,  0.39267 , -0.21851 ,  0.58795 ,
           -0.55999 ,  0.63368 , -0.043983, -0.68731 , -0.37841 ,  0.38026 ,
            0.61641 , -0.88269 , -0.12346 , -0.37928 , -0.38318 ,  0.23868 ,
            0.6685  , -0.43321 , -0.11065 ,  0.081723,  1.1569  ,  0.78958 ,
           -0.21223 , -2.3211  , -0.67806 ,  0.44561 ,  0.65707 ,  0.1045  ,
            0.46217 ,  0.19912 ,  0.25802 ,  0.057194,  0.53443 , -0.43133 ,
           -0.34311 ,  0.59789 , -0.58417 ,  0.068995,  0.23944 , -0.85181 ,
            0.30379 , -0.34177 , -0.25746 , -0.031101, -0.16285 ,  0.45169 ,
           -0.91627 ,  0.64521 ,  0.73281 , -0.22752 ,  0.30226 ,  0.044801,
           -0.83741 ,  0.55006 , -0.52506 , -1.7357  ,  0.4751  , -0.70487 ,
            0.056939, -0.7132  ,  0.089623,  0.41394 , -1.3363  , -0.61915 ,
           -0.33089 , -0.52881 ,  0.16483 , -0.98878]
    

单纯的数字可能不会让人有什么感觉，但是如果展示成图呢？是不是有令人惊讶的发现？

**KING**

![](https://images.gitbook.cn/825dfe30-275a-11eb-b804-ef8651c2512a)

**MAN**

![](https://images.gitbook.cn/9c0f9960-275a-11eb-b804-ef8651c2512a)

**WOMAN**

![](https://images.gitbook.cn/a4a32fb0-275a-11eb-b804-ef8651c2512a)

是的，你会发现词向量捕获了单词与单词之间某种有意义的关联，虽然说不上来具体哪些关联，但是至少从语义上来说，人类觉得相似的单词，词向量之间的距离也很接近，也就是说相似程度很高。这就是词向量的厉害之处，我们没法像上一节解释词向量每个维度具体的含义，但是它确实可以挖掘出很有趣、通常也是很有用的信息。

我们再来看看更有趣的：

    
    
    wv.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)
    # output: [('queen', 0.7698541283607483)]
    

瞧，词向量居然会有这样的能力：

> 'king' + 'woman' - 'man' = 'queen' !

我们再看看 QUEEN 的图表示：

**QUEEN**

![](https://images.gitbook.cn/bec56700-275a-11eb-90f6-fbd19bda6e6e)

以及： **KING + WOMAN - MAN**

![](https://images.gitbook.cn/c765a780-275a-11eb-9825-558323e50dbd)

是不是特别的相似？多么神奇的算法呀。居然能找出如此奇妙的关系。

但是等等，这些好像都是在用别人预训练好的词向量，我们自己能训练一个质量相当的词向量模型吗？

当然可以！

### 训练过程

Word2Vec 的训练一般有两种方法，CBOW 和 Skip-Gram，前者是通过
周围的词来预测中间词，后者整好是反过来的，通过中间的词来预测周围的词。在[这篇著名的
paper](https://arxiv.org/pdf/1301.3781.pdf) 中，作者得到的结论就是 Skip-Gram 准确度比 CBOW
好，尤其是生僻字上表现更佳，所以我们主要讲解 Skip-Gram 的训练过程，想了解 CBOW 的同学可以自行谷歌，不过原理都是差不多的。

假设我们的训练集（语料库）就这么一句话：

    
    
    无论精神多么独立的人，感情却总是在寻找一种依附，寻找一种归宿。
    

那么，整个训练过程的步骤就是这个样子。

#### **生成词汇表**

在原始语料库的基础上，需要分词、去除停用词、标点符号等，得到如下词汇表，并给每一个词编好序号。

分词、去除停用词等实现依赖不同的分词器：

词 | 序号  
---|---  
无论 | 0  
精神 | 1  
多么 | 2  
独立 | 3  
人 | 4  
感情 | 5  
总是 | 6  
依附 | 7  
归宿 | 8  
寻找 | 9  
  
可见，词汇表中已经去除了类似“的”、“在”、“一种”这样的词，并且已经去重（“寻找”在语料库中出现了两次）。词的序号一般也是按照词出现的次数升序排列后得到的。

#### **生成训练数据**

有了词汇表之后，接下来就需要把原始语料库转换为计算机可以识别的训练数据，既然是用中间词预测周围词，那么显然模型的输入是中间词，输出则是周围词。

    
    
    无论精神多么独立的人，感情却总是在寻找一种依附，寻找一种归宿。
    

可以转换成训练数据如下（这里假设只看前后 1 个词，这里的 1 也被称为 窗口长度 window size）：

输入 | 输出  
---|---  
无论 | 精神  
精神 | 无论  
精神 | 多么  
多么 | 精神  
多么 | 独立  
独立 | 多么  
独立 | 人  
人 | 独立  
人 | 感情  
感情 | 人  
感情 | 总是  
总是 | 感情  
总是 | 寻找  
寻找 | 总是  
寻找 | 依附  
依附 | 寻找  
依附 | 寻找  
寻找 | 依附  
寻找 | 归宿  
归宿 | 寻找  
  
#### **训练过程**

我们来看看 Word2Vec 训练的模型结构和训练步骤。

那么

![](https://images.gitbook.cn/58477f80-275b-11eb-be76-5555cd71111f)

假设词汇表长度为 V，隐藏层的长度为 h，我们可以看到：

  * 输入的并不是原始词，而是其在词汇表中对应的序号，取值范围是 0 到 V-1。
  * 根据序号，得到一个 one-hot 向量，长度为 V，在序号处值为 1，其他位置均是 0。
  * 根据 one-hot 向量与 输入到隐藏层的矩阵 W1 相乘，W1 的形状为 V * h，得到隐藏层的向量表示，长度为 h。
  * 再将隐藏层的向量与 W2 相乘，W2 的形状为 h * V，经过一层 softmax 得到了最终的输出，长度为 V。
  * 输出的个数有 V 个，每个都是一个概率表示，意味：当输入是 input 时，输出是 output 的概率。从上面的例子中即可解释为：
    * 第一个输出概率意思是：当输入是“人”时，输出是“无论”的概率；
    * 第二个输出概率意思是：当输入是“人”时，输出是“精神”的概率，以此类推。
  * 真实 label 是 ”独立“ y_true = 1，再拿到输出中“独立”的概率 y_pred：
    * 是不是就可以计算 loss 了？
    * 有了 loss，就可以计算梯度、更新参数了。

一次训练完毕，是不是 so easy？

对，Word2Vec 的训练就是这么简单，没有复杂的结构，但是效果却出奇的好。

但是，总有一个“但是”，聪明的你发现了什么问题了吗？

#### **训练过程优化**

**两个问题：**

  * 我们看到 Word2Vec 结构最后一层的输出单元竟然有 V 个，加入词汇表有百万/千万级，那这个模型还怎么训练啊？最后一层 softmax 的分母岂不是要把计算机算出脑梗塞？
  * 虽然我们提前剔除了类似“的”、“啊等这样的词，但是依然依然有高频词出现，比如中文的“什么”、“不知道”等，而出现次数越多的词，其携带的信息量反而是越少的，我们希望能让频次低的词也能参与到足够的训练中来？

这两个问题的答案都出现再了[Word2Vec 原始 paper](http://papers.nips.cc/paper/5021-distributed-
representations-of-words-and-phrases-and-their-compositionality.pdf)
中，有兴趣的同学可以好好研究以下这篇经典的 paper。

**解决方案如下。**

**1\. Negative Sampling**

原始的 softmax 由下式计算得到：

$$p = \frac{exp^{v_{wo}^t v_{wi}}}{\sum_{n=1}^{V}exp^{v_{wi'}^tv_{wi}}}$$

其中 $v_{wi}$ 是训练样本的输入，$v_{wo}^t$ 是训练样本的输出，$v_{wi'}^t$ 是词汇表中任意一个单词的向量。这也是原始
Word2Vec 不可扩展的根本原因，分母的计算量太大了——O(V)。而 paper 的作者给出了很好的解决方案——既然 O(V) 太大，我就索性只算
2~5 个（神之所以为神，是因为他敢想人所不敢想）。结果就变成了这样：

$$p = \frac{exp^{v_{wo}^t v_{wi}}}{exp^{v_{wo}^t v_{wi}} +
\sum_{n=1}^{5}exp^{v_{wi'}^tv_{wi}}}$$

也就是说，最后的 softmax 层不再将所有 V-1 个单词都作为负例，而只随机采样少数（paper 中给出的数字是
2~5）个词作为负例，参与到一次的参数更新中来。这大大提高了 Word2Vec 的可扩展性，而作者也证明了模型的质量并没有下降。

**2\. Sub-Sampling**

第二个问题可以通过二次采样的方式来解决，也就是每次生成一条样本时，有概率的丢弃某个词，而这个概率与词在语料库中的次数有关，出现的次数越大，被丢弃的概率就越大。

paper 中给出的概率公式为：

$$P(w_i) = 1 - \sqrt\frac{t}{f(w_i)}$$

其中 $P(w_i)$ 是 单词 $w_i$ 被丢弃的概率，$f(w_i)$ 是单词 $w_i$ 在语料库中出现的次数，$t$ 是一个超参数，一般设置为
$10^{-5}$。

比如上一小节中：

    
    
    输入: 无论   输出: 精神
    

这么一条样本，如果通过概率丢弃的方式将“精神”丢掉 ，那么这条样本就随之被丢弃。

### 小结

讲完这些之后，大 T 意犹未尽，显得很满足，却又有那么一点不满足。

> 大 T：这些都很好理解，看完 paper 就懂了，但……
>
> 我：但是怎么运用在电商推荐系统里呢，对吧？
>
> 大 T：是的，而且怎么实现呢……（挠头）
>
> 我：别着急，先把这一节的东西消化掉，咱们下一篇再来解决你心中的疑惑，包括在电商中的运用和实现，你会发现代码真只是 paper 的翻译。
>
> 大 T：好!

下一篇的内容会偏理论一点，从背后的原理出发彻底搞清楚 Word2Vec 的工作原理及其在 Spark 中的实现。

咱们，下篇见。

* * *

注释 1：GloVe 是训练词向量的技术之一，[GitHub 地址](https://github.com/stanfordnlp/GloVe)

