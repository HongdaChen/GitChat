目前为止，我们已经学习了诸多召回算法，有传统的协同过滤、关联规则，也有稍微复杂点的词向量以及时下特别流行的深度模型。我们掌握了它们的原理、如何实现以及在线上如何
serving，但是我们忽略了一个至关重要的一步——线下评测。

线下评测的重要性不言而喻，一个算法模型在上线前，如果能够找到一个或者几个指标来衡量这个算法的优劣，不仅可以避免直接上线带来的效益损失风险，还能够让我们在线下对多个模型进行比对从而选择一个最优的模型。

今天咱们就来好好看看一些常见的用来评估召回模型质量的指标。

### 精准率和召回率

这两个概念初学者很容易混淆，觉得不好记。其实弄清楚之后，就再也不会忘记了。

假设我们将所有样本分成 **正** （Positive）和 **负** （Negative）。

#### **精准率**

精准率（Precision）是针对 **预测结果** 而言，指的是预测为正的样本中有多少实际上也确实是正样本。

预测结果为正时，会出现两种情况：

  * 一是预测对了，确实是正样本，称为 True Positive，TP。
  * 二是预测错了，本来是负样本的，被预测成了正样本，称为 False Positive，FP。

由此根据精确率的概念，精确率的计算公式为：

$$P = \frac{TP}{TP+FP}$$

#### **召回率**

相对地，召回率（Recall）是针对 **真实样本** 而言，指的是真实样本中正样本有多少被预测对了。

真实样本为正时，也会出现两种情况：

  * 一是预测对了，还是 True Positive，TP。
  * 二是预测错了，本来是正样本的，被预测成了负样本，称为 False Negative，FN。

由此根据召回率的概念，得到召回率的计算公式为：

$$R = \frac{TP}{TP+FN}$$

#### **混淆矩阵**

根据精准率和召回率的定义，我们可以画出如下矩阵帮助理解这两个概念，称为混淆矩阵（confusion matrix）

![](https://images.gitbook.cn/0485be70-42e5-11eb-a007-1b095f083930)

从上往下的视角是针对 **真实样本** 而言，从左到右的视角是针对 **预测结果** 而言。

  * 第一行表示 **预测** 为 **正** 的样本
  * 第二行表示 **预测** 为 **负** 的样本
  * 第一列表示 **真实** 为 **正** 的样本
  * 第二列表示 **真实** 为 **负** 的样本

#### **例子**

举个推荐系统中的具体例子来说明一下这两个指标的计算。

假设用户对 10 件商品感兴趣，召回模型给该用户推荐了 40 件，其中有 2 件确实是用户感兴趣的，则：

  * TP = 2
  * FP = 40 - 2 = 38
  * FN = 10 - 2 = 8

$$\begin{aligned}P &= \frac{TP}{TP+FP} = \frac{2}{40} = 5\% \\\R &=
\frac{TP}{TP+FN} = \frac{2}{10} = 20\%\end{aligned}$$

弄清楚精确率和召回率之后，我们再来看看另外一个推荐系统中常见的问题。

还是假设用户对 10 件商品感兴趣，模型 1 和 2 都对该用户推荐了 40 件商品，且都只有 2 件确实是用户感兴趣的，但是：

  * 模型 1 将这 2 件 TP 排在了 40 件商品的第 1、2 位
  * 模型 2 将这 2 件 TP 排在了 40 件商品的第 39、40 位

请问模型 1 和 2 谁更好一点？

显然是模型 1，虽然精确率和召回率都一样，但是模型 1 的排序性能更好，因为它将 TP 排的更靠前了。

不过这只是我们的直观感受，如何能够用具体的数值来量化这个排序的能力呢？

### NDCG

NDCG 是一种专门用来衡量算法模型排序能力的指标。

这里先说下相关性（relevance） 这个概念——也就是说推荐列表里的物品，用户越有兴趣，表示该物品的 relevance 越高。

对于地鼠商城这样一个电商场景，假设购买行为的 relevance 是 3，加车行为的 relevance 是 2，点击行为的 relevance 是 1。

那么如果模型 M1 给用户推荐了如下商品：

> [A，B，C，D，E]

而用户对这 5 个推荐商品产生了 [A 购买，B 点击，C 加车，D 购买，E 加车] 行为，则对应的 relevance 为：

> [3, 1, 2, 3, 2]

了解了 relevance 之后，还需要再来认识一下 CG 和 DCG。

#### **CG**

CG 的全称是 Cumulative Gain，计算公式也特别的简单：

$$Cumulative \ Gain = \sum_{i=1}^n relevance_i \ \ //i 是物品在列表中的位置$$

则上述用户的消费行为对应的 CG 值为：

$$CG = 3 + 1 + 2 + 3 + 2 = 11$$

不过 CG 的缺点实在太大了，比如现在又有一个模型 M2，也给用户推荐了同样的商品，但是顺序却是这样的：

> [B, C, E, A, D]

该推荐结果对应的 relevance 为：

> [1, 2, 2, 3, 3]

其 CG 也等于 11。但是我们明显能看出来 M1 比 M2 排的好，因为它把 相关性（relevance ）比较高的商品排在了前面。

要解决这个问题，似乎需要考虑每个商品所在结果中的位置，DCG 应运而生。

#### **DCG**

DCG 的全称是 Discounted Cumulative Gain，计算公式也特别的简单：

$$Discounted \ Cumulative \ Gain = \sum_{i=1}^n \frac{2^{relevance_i}
-1}{log_2(i+1)} \ \ //i 是物品在列表中的位置，注意是从 1 开始$$

可见 DCG 考虑了位置信息。我们再来计算一下上述两个相同 CG 的 relevance 序列对应的 DCG。

$$DCG_{[3,1,2,3,2]} = \frac{2^3-1}{log_2(1+1)} + \frac{2^1-1}{log_2(2+1)} +
\frac{2^2-1}{log_2(3+1)} + \frac{2^3-1}{log_2(4+1)} + \frac{2^2-1}{log_2(5+1)}
= 13.31 \\\DCG_{[1,2,2,3,3]} = \frac{2^1-1}{log_2(1+1)} +
\frac{2^2-1}{log_2(2+1)} + \frac{2^2-1}{log_2(3+1)} + \frac{2^3-1}{log_2(4+1)}
+ \frac{2^3-1}{log_2(5+1)} = 10.12$$

瞧，DCG 这个指标说明了第一个 relevance 更好。

一般情况下，我们都习惯于指标的值落在 [0 - 1] 之间，因为这更符合人类的直观感受，而 DCG 的值就不满足了，于是 NDCG 出现了。

#### **NDCG**

NDCG 的全称是 Normalized Discounted Cumulative Gain，从名字上也能看出来，归一化的 DCG。

NDCG 的计算公式为 ：

$$NDCG = \frac{DCG}{iDCG}$$

其中的 iDCG 是 ideal DCG，是最优的 DCG，iDCG 也很容易计算，按照 relevance 倒排之后算出来的 DCG 就是 iDCG 啦。

继续以 [3, 1, 2, 3, 2] 这个 relevance 为例，其 DCG = 13.31。

$$\begin{aligned}DCG_{[3,1,2,3,2]} &= 13.31\\\iDCG_{[3,1,2,3,2]} &=
DCG_{[3,3,2,2,1]} \\\&= \frac{2^3-1}{log_2(1+1)} + \frac{2^3-1}{log_2(2+1)} +
\frac{2^2-1}{log_2(3+1)} + \frac{2^2-1}{log_2(4+1)} + \frac{2^1-1}{log_2(5+1)}
\\\& = 14.60 \\\NDCG_{[3,1,2,3,2]} &=
\frac{DCG_{[3,1,2,3,2]}}{iDCG_{[3,1,2,3,2]}} \\\& = \frac{13.31}{14.60} \\\& =
0.91\end{aligned}$$

在推荐系统中，我们会有很多个类似 [3, 1, 2, 3, 2] 这样的根据某个模型 M 得到的用户推荐列表 relevance，只要对这些
relevance 计算出来的 NDCG 取平均，即可得到模型 M 的排序能力。

掌握了以上 精准率、召回率和 NDCG 之后，我们来看看工作中是如何运用这些指标来衡量算法模型的质量。

### 应用

现在需要对至今为止学过的召回算法做离线评估。假定协同过滤、关联规则或者词向量生成的商品相似/关联表（又可称为 item2item，i2i）如下所示：

item1 | item2 | score  
---|---|---  
物品 ID1 | 物品 ID2 | 得分  
  
score 越大表示物品之间越相似/关联。

还记得这个相似/关联表的线上服务逻辑吗？

回顾一下：

  1. 用户请求到来
  2. 系统根据用户 ID 获取其最近的行为商品列表
  3. 根据行为商品列表去商品相似/关联表中查询相似/关联商品
  4. 去重、过滤等一些列操作后返回推荐列表

还需要一份真实的用户行为数据表，如下所示：

user | item | relevance | timestamp  
---|---|---|---  
用户 ID | 物品 ID | 相关性 | 时间戳  
  
relevance 可以采用用户行为来表示，比如点击行为时 relevance 值为 1，加车行为时 relevance 值为 2，等等。timestamp
是该行为发生的时间戳。

有了这两份数据，就可以就可以评估算法的各项指标啦。具体评估逻辑如下（线下的评估处理逻辑最好和线上服务逻辑保持一致哦）：

  1. 使用 N 天的数据训练出模型 M
  2. 使用 第 N + 1 天的用户行为数据表作为测试数据

以单个用户为例（多用户时只不过是所有单个用户各自计算完成后取平均而已），假设用户 U 在第 N + 1 天按照时间顺序有行为如下：

user | item | relevance | timestamp  
---|---|---|---  
U | itemU1 | 2 | 1605024000  
U | itemU2 | 1 | 1605024010  
U | itemU3 | 3 | 1605024030  
  
开始计算用户 U 的推荐指标：

1\. 获取第一行数据，得到行为商品 itemU1，查询 **商品相似/关联表** ，得到推荐列表 recsU1。

查看 **itemU2** 在 recsU1 的命中情况以及排名情况，以此可以计算 TP、FP、NDCG 等指标。

> 这里要注意是，根据 itemU1 得到的推荐列表 recsU1 去校验 itemU2 的命中情况。（为什么呢？）

2\. 获取第二行数据，得到行为商品 itemU2，查询 **商品相似/关联表** ，得到推荐列表 recsU2。查看 **itemU3** 在
recsU2 的命中情况以及排名情况。

3\. 用户 U 的评估结果结束，根据上面 1、2 步得到的信息，汇总得到精准率、召回率以及 NDCG 等。

> 具体评估代码逻辑见 [Git](https://github.com/recsys-4-everyone/dist-
> recsys/tree/main/recall_metrics)。

怎么样，不复杂吧？同学们对照着这个逻辑再去看看[代码](https://github.com/recsys-4-everyone/dist-
recsys/tree/main/recall_metrics)，相信很快就能够在自己的日常工作中用起来了。

> 深度学习的离线指标一般是
> AUC（二分类问题），这个指标我们留到讲解排序模型离线评测时再好好研究研究，它在面试中出现的频率特别之高，因此值得我们对其原理和实现仔细剖析。

### 小结

地鼠商城推荐系统的召回部分到此就实现了从无到有，零的突破。可喜可贺，可歌可泣。

从此要在上新的召回算法之前，必须要先经过离线评估，得到一个评估指标且该离线指标比线上算法的离线指标好，才具有上线的资格。

> 使用新算法和线上算法在同一份测试数据上评估得到的指标才具有可比性，而且要注意两种算法的训练数据都不能与测试数据有任何重合的部分。

本篇讲解了召回系统中常用的指标——Precision、Recall、NDCG 等，其实还有很多的离线指标，感兴趣的同学可以自行谷歌，选择适合各自场景的指标。

阐述完这些指标的原理之后，又描述了在实际的工作中是如何使用这些指标去进行评估的，在这里我们对深度学习的评估留了一个空白，这部分会在排序算法部分再做详细说明。

下一篇，咱们来聊聊，算法通过了离线评估，上线后 AB 实验的种种问题。包括但不限于：

  * AB 实验波动太大怎么办？
  * AB 实验需要跑多少天，需要多少流量才能保证结果置信呢？
  * 线下评估“芝麻开花节节高”，线上指标就“飞流直下三千尺”，这怎么分析呀？

咱们下篇见。

