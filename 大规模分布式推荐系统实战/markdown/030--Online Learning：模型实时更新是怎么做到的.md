在正式进入本篇的主题之前，我们先来做个简单的加减乘除运算。

场景为点击率预估，假设每日整体曝光数据量为 10 亿，训练时负采样率为 0.1，使用训练数据周期为 30 天，总的训练数据量大概为 30
亿。再假设训练环境为单机，batch size 为 256，每秒可以训练 50 个 batch，一共训练 2 轮（epoch 为
2）。那么训练完毕大概需要多长时间呢？

5.4 天！一周工作日还要加上半天周末。

换句话说，至少需要 5 天的时间，模型才会更新一次，如果训练过程中不小心任务失败，时间间隔会更长。

因此，当数据量已经大到一定程度，模型训练时长已经没有办法满足实际应用时，就必须想办法解决这个问题，否则对于业务的收益有极大的负向影响。

本章会探讨如何缓解或者解决数据量过大带来的模型更新慢的问题，并不是所有的推荐业务都要求模型必须实时或者秒级更新，有的场景可能一天更新一次已经绰绰有余（特别是用户行为不那么丰富的场景），有的场景可能需要实时更新。接下来会针对不同的更新频率要求做出不同的策略建议供读者参考，希望能在这个问题上给大家带去一点启发。

### 离线更新

如果对于模型的更新频率要求不是那么的高，一天一次也可以接受的话，那么采用 **全量模型更新 + 增量模型更新**
策略基本能够满足需求。所谓的全量模型更新是指模型从零开训，会使用较多的数据。增量更新，是指在已有模型的基础上，仅仅拟合新数据，这种更新方式一般使用的数据较少。那么在实际应用中是如何把全量更新与增量更新结合起来呢？这里主要介绍两种更新策略。

> 全量模型称为大模型，增量模型称为小模型。

#### **一大 N 小**

顾名思义，这种策略就是首先训练全量模型，后续不再训练全量模型，而是在原有的模型上只做增量更新，如下图所示。

![](https://images.gitbook.cn/7a31d740-a1a3-11eb-8c5d-e927c72eece1)

这个也很好理解，实际应用中有不少场景是这么去做的，优点就是占用资源少，第一次全量模型训练完毕后，后续的模型训练只会使用少量的数据参与训练，大大减少了训练时间，提高了模型迭代的频率，使模型可以更快的捕捉到数据分布的变化。

当然凡事都有两面性，训练数据的减少，使得增量模型更好的拟合 **新数据**
，但是对全局数据的拟合程度很难保证，也就是说模型的收敛可能局限在新数据的最优点，而不是全局最优的。还有一个更可能存在的问题就是万一某次增量更新因为各种各样的问题导致模型质量下降（模型跑偏了，比如大促、活动等），后续的增量模型可能都受到影响。如果能够定期对增量模型进行“纠偏”，那就再好不过了，因此更多时候我们会选择下面这种方式。

#### **大小交替**

这种方式也不难理解，为了让增量模型不至于训跑偏了，需要定期更新全量模型，然后每次增量时都基于最新的全量模型进行更新，如下图所示。

![](https://images.gitbook.cn/f90f73a0-a1a4-11eb-b8fb-65c86085beb3)

以上图为例，每 7 天全量更新模型，使用 30 天的训练数据从头训练。同时以最新的全量更新模型为基础进行增量更新，也就是说全量更新模型只负责产出
checkpoints，增量更新模型使用这个 checkpoints
进行训练，训练完成后将模型导出，为线上提供预测服务。通过这种方式，不会担心模型的跑偏，同时也能够保证模型的更新频率。

当然，在算法的世界里凡事无绝对，究竟哪种方式更好，还是需要通过 AB 测试验证出来，适合自己的才是最好的。

如果上述增量更新策略的实时性依然达不到业务的要求，比如对于物品上新下架特别频繁或者数据分布变化比较大的场景（特别是在大促时期），那么就要考虑通过下面这种方式对模型进行更新了。

### 在线学习

深入到在线学习的细节之前，我们先回想一下离线更新模型时从数据到模型服务整个流程是什么样子呢？大致如下图所示。

![](https://images.gitbook.cn/06ec7e50-a1a5-11eb-89eb-d324b3dc8a30)

具体流程为：

  1. 用户向推荐服务发起请求
  2. 推荐服务根据用户信息、上下文信息以及物品信息等从特征库中抽取模型可用的特征
  3. 将特征送入在线推理服务进行预估
  4. 推荐服务得到预测值
  5. 根据预测值进行排序，返回推荐结果给用户
  6. 如果用户进行滑动或者翻页操作，则产生曝光事件，终端会向服务器上报曝光埋点数据，携带本次 PV ID，离线数据处理任务对此上报的数据进行处理，生成一张曝光表
  7. 如果用户发生了点击行为，则产生点击事件，终端同样会向服务器上报点击埋点信息，也会携带本次 PV ID，同理会生成一张点击表
  8. 对曝光表和点击表进行关联和特征工程，就生成了训练数据，喂入模型
  9. 训练完成后模型推送到线上提供推理服务

可以看到，离线训练时训练数据是通过批处理离线数据得到的。

在线学习，也就是 Online Learning，不同于离线训练时使用的是 **一批一批** 的数据，采用 Online Learning
方式进行训练的模型使用的是 **一条一条** 的数据，也就是每来一条数据就进行模型更新，从而实现模型的实时更新。

那么在线学习时，从数据生成到模型服务的流程又是怎样的呢？如下图所示。

![](https://images.gitbook.cn/159eb350-a1a5-11eb-a6d9-e9904e63f59e)

可以发现，在线学习与离线更新最大的差别出现在右上角的训练数据生成部分——在线学习需要试试生成训练样本。

样本由 features 和 label 组成，那么这两部分又如何生成呢？我们以一个例子来说明。

1\. 用户 U，打开地鼠商城 App，向推荐服务发起请求，且请求时携带请求 ID REQ_ID_123456，此请求 ID 由 App
自动生成且全局唯一。

2\. 请求达到服务器，假设用户落入实验组 E，由模型 M 提供服务，服务器根据用户 U 的行为召回到物品 1、物品 2 和物品 3。

根据模型 M 的配置，服务器获取 M 所需特征，假设如下所示。

**用户特征**

| 用户 ID | 年龄 | 性别  
---|---|---|---  
用户 U | UID123 | 20 | 1  
  
**物品特征**

| 物品 ID | 品牌 | 价格  
---|---|---|---  
物品 1 | ITEM1 | BRAND1 | PRICE1  
物品 2 | ITEM2 | BRAND2 | PRICE2  
物品 3 | ITEM3 | BRAND3 | PRICE3  
  
为了演示，忽略上下文、用户行为等特征。

3\. 将上述特征喂入模型。

4\. 模型服务返回预测值如下：

| 预测值  
---|---  
物品 1 | 0.3  
物品 2 | 0.5  
物品 3 | 0.2  
  
5\. 服务器将排序后的推荐结果 物品 2、物品 1、物品 3 返回给用户，同时还要将第 2 步中生成的特征与请求 ID REQ_ID_123456
发送给实时数据处理服务（比如 Kafka），发送的内容格式如下（简化）：

请求 ID | 用户 ID | 年龄 | 性别 | 物品 ID | 品牌 | 价格 | 预测值  
---|---|---|---|---|---|---|---  
REQ_ID_123456 | UID123 | 20 | 1 | ITEM1 | BRAND1 | PRICE1 | 0.3  
REQ_ID_123456 | UID123 | 20 | 1 | ITEM2 | BRAND2 | PRICE2 | 0.5  
REQ_ID_123456 | UID123 | 20 | 1 | ITEM3 | BRAND3 | PRICE3 | 0.2  
  
可以看到，此时 **有了特征，但是没有 Label** 。

6\. 用户看见 App 上展示的内容，向下滑动的过程中，触发埋点，假设用户看了前 2
个物品，埋点会将用户看到的内容上报给数据处理服务，上报格式如下（简化）：

请求 ID | 物品 ID | 事件  
---|---|---  
REQ_ID_123456 | ITEM2 | 曝光  
REQ_ID_123456 | ITEM1 | 曝光  
  
7\. 假如用户点击了物品 1，则埋点又会上报这个事件给数据处理服务，上报格式如下（简化）：

请求 ID | 物品 ID | 事件  
---|---|---  
REQ_ID_123456 | ITEM1 | 点击  
  
8\. 由第 6 步和第 7 步，可以生成 Label（是否点击），但是没有特征，正好与第 5 步的内容结合起来，就可以得到 **特征** 和
**Label** 。这其中 请求 ID 起到了串联数据的作用，保证特征和 Label 可以准确无误的关联起来。

9\. 训练样本有了，就可以进行模型训练了，实时地对模型参数进行更新，从而完成了一次 **学习** ，将更新后的模型实时地推送到线上对外提供推理服务。

由上述步骤可以看到，在线学习可以做到实时更新模型，能够很好的保证模型的时效性，及时的捕捉到线上数据的变化，特别是在数据分布发生剧烈变化时，它的优势尤其明显（大促、抢购等）。

但是，这种时效性是需要代价的，在线学习引入了实时数据处理，使得整个系统的复杂度上升了一个台阶，出现问题后定位和解决的难度加大。因此，在线学习并不像看上去的那么美好，维护和使用的成本都比较高，究竟能不能为业务带来令人满意的收益，还需要各位读者在实际运用的过程中权衡利弊。

敏锐的读者可能会发现，特征是先生成的，Label
还要等待用户行为（点还是不点）才能确定，那么这不是会有延迟吗？没法做到实时更新模型吧？正确！在线学习有一个很大的问题需要解决——正样本延迟问题。

#### **正样本延迟**

正样本延迟问题，又叫延迟反馈问题，普遍存在于推荐系统中。

由上述描述可以知道，特征早已经准备好，唯独缺少 Label，只有等用户点击到来，才知道样本是正样本。这个 **等** 字就带来了一个问题——等多久？

**1\. 等待 T 时间**

也就是说当特征准备好之后，我们需要等待一段时间 T，如果在 T 时间内用户点击行为到来，则 Label 为 1，否则 Label 为 0. 超参 T
的设置可以分析历史数据中曝光与点击时间差的分布，取 95 或者 99 分位点对应的时间差。

假设有 n 条负样本，m 条正样本，采用这种样本拼接方式时，模型见到的数据量为 $m + rn$，其中 $r$ 为采样率，模型的预测平均概率为
$\frac{m}{m+rn}$，而真实数据的平均概率为 $\frac{m}{m+n}$，因此预测概率需要校准，校准公式就留给聪明的读者自行推导。

这种方式比较容易理解，只是需要等待 T 时间来确定 Label，所以模型并不是实时更新，而是会延迟 T。

那么可不可以不等呢？可以！

**2\. 不等待**

也就是不等待，特征生成时直接当成负样本喂入模型参与训练，延迟基本为 0。当真正的正样本到来时，再参与一次训练。因此模型见到的数据量为
$m+r(m+n)$，因为正样本会多发送一遍，模型的预测平均概率为 $\frac{m}{m+r(m+n)}$，真实数据的平均概率为
$\frac{m}{m+n}$，因此预测概率需要校准，校准公式为：

$$\begin{align}calibrated\\_pctr=\frac{r \times pctr}{1-pctr}\end{align}
\tag{1}$$

式中，$pctr$ 为模型预测概率，$calibrated\\_pctr$ 为校准概率。

#### **模型训练**

既然正样本延迟是存在的，那么可不可以让模型感知到这个问题并在建模时加以利用呢？在这里推荐一篇很好的 Paper，专门研究正样本延迟问题——
_Modeling Delayed Feedback in Display Advertising_ $^1$。

在这篇 Paper 中，作者将概率预估任务拆成了两个建模任务 M1 和 M2，M1 用来预估事件发生的概率是多少，M2
用来预估事件发生还需要多久。两个模型联合训练（Joint Training），正样本延迟的信息可以很好的被模型捕捉到。

非常推荐读者们好好研究一下此 Paper，虽然它是 2014 年发表的，彼时深度学习还没有那么流行，但是其对于正样本延迟问题的解决思路非常值得借鉴和研究。

在转化率预估任务中，点击未购买为负样本，点击购买为正样本，曝光和点击以请求 ID
关联，那么就会有一个问题，曝光未点击的就一定是负样本吗？假如用户看了一件物品，两天后才点击。

Paper 中提到训练完成后，M1 用来为线上提供预测服务，M2 直接丢弃，但是这样未免太过可惜，所以一般 M2 也会被留作他用——比如可以用于预估“等待
T 时间”提到的等待时长 T。

深度模型在进行在线学习时，一般会采用 Ftrl$^2$、Adagrad 等自适应优化器。

### 小结

当训练数据越来越多时，如何减少训练时间会变成一个很棘手的问题，动辄训练一周的模型实在难以满足业务的需求。

因此增量模型更新就成为了解决这个问题的法宝，一般实际应用中会采用全量模型与增量模型混合使用的策略，既能够满足模型更新频率的要求，又可以保证模型不会跑偏。

在线学习是一种很吸引人的训练模型策略，优点是更新快速，能够及时捕获到线上数据的分布变化。缺点也很明显，系统架构的复杂性远比离线更新高的多，正样本延迟问题不仅存在于在线学习中，在离线模型训练中同样存在，Paper$^1$
是用来解决此类问题的经典文献，还望读者能够仔细研读。

一般的实时性包括两方面：模型实时和特征实时。而前者的重要性一般来说是不如后者的，带来的收益一般也不如后者。所谓的特征实时，当用户在终端产生行为时，服务器要能够立刻感知到，而不是等到几个小时或者第二天才去更新用户的历史行为轨迹。在特征实时性没有做好之前，其实是不太建议尝试在线学习的。它的维护和调试的成本比较的高，对于系统的稳定性要求也比较高，因此在尝试它之前，最好先估计一下能够带来的业务价值，值不值得去采用在线学习。

至此，排序系统部分基本上到达了终点，接下来我们将会探讨另外一个很重要的主题——分布式训练！

咱们，下篇见。

  * 注释 1：[Modeling Delayed Feedback in Display Advertising](https://dl.acm.org/doi/10.1145/2623330.2623634)
  * 注释 2：[Ad Click Prediction: a View from the Trenches](https://research.google/pubs/pub41159/)

