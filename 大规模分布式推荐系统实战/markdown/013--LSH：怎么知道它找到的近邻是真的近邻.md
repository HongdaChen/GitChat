在更进一步之前，我们先回顾一下上篇的内容。上篇中，我们对 LSH 算法的基本思想和时间复杂度都好好地刨根问底了一番。

  * 基本思想：大事化小、小事化了——将整个数据集按照 Hash 值划分到多个桶中，桶内的数据点认为是近邻点。
  * 时间复杂度：平均算法复杂度为 $LDK + \frac{LN}{2^k} = O(logN) if k = logN$。

上篇末尾，遗留了几个问题：LSH 确实是大大缩短了近邻的搜索时间，这速度是解决了，可是精度呢？用很短的时间给出无用的结果也是不能接受的呀，该怎么评估 LSH
的精度呢？

这篇文章咱们就来看看 LSH 的错误率计算、准确率召回率计算以及如何在实际中使用 LSH，我们会用到前几篇中 Word2Vec 训练出来的商品向量，通过
LSH 算法找出每个商品的最相似的 topN。

### 错误率分析

首先沿用上文中的一些符号：

  * N：点的个数
  * D：点的向量长度
  * L：签名个数
  * k：签名长度

![](https://images.gitbook.cn/ae993190-281d-11eb-b5c5-e14bfa43e9db)

LSH 本身是一个 ANN 搜索（Approximate Nearest Neighbor Search）算法，功能是找出 **点的最近邻** ，这里的
Approximate（近似）就决定了此类算法不可能 100% 保证给出的最近邻是真的最近邻。

这里的错误率分为两种：

  * False Negative Error：本来点 a 和点 b 应该是近邻，但是 LSH 不认为是近邻。
  * False Positive Error：本来点 a 和点 b 不应该是近邻，但是 LSH 认为是近邻。

下面逐一分析这两种情况。

#### **False Negative Error**

> 本来点 a 和点 b 应该是近邻，但是 LSH 不认为是近邻。

LSH 不认为是近邻，那么就意味着：

点 a 由 L 个签名，点 b 有 L 个签名，点 a 的 L 个签名与 点 b 的 L 个签名没有一个一样的（因为只要 L 个签名中任意一个一样，那么
LSH 就会把 点 a 和 点 b 放在同一个桶中，所以必须一个都不一样才行）。每个签名有 k bit 组成，所以只要两个签名任意有一 bit
不同，那么这两个签名就不一样。

文字描述太多晦涩，下面用符号来讲述 False Negative Error 的计算:

$False \ Negative \ Error $ $= P(a 和 b 的 L 个 hash 值一个都不一样)$
$=P(hashcode(a)!=hashcode(b))^L \ \ // 假设 L 个 hash 值之间互相独立 $ $=P(k 个 bit
中至少有一个不一样)^L $ $=(1-P(k 个 bit 完全一样))^L $ $=(1-P(某一个 bit 位值一样)^k)^L \ \ //假设
hash 值内部的 k bit 彼此互相独立$ $=(1-p^k)^L,p=P(点 a 和点 b 相同位置 bit 位值一样)$

#### **False Positive Error**

> 本来点 a 和点 b 不应该是近邻，但是 LSH 认为是近邻。

有了上面的基础，这个错误率就好计算了。

$False \ Positive \ Error $ $= P(a 和 b 的 L 个 hash 值至少有一个一样) $ $=1 - P(a 和 b 的
L 个 hash 值一个都不一样) $ $=1 - False \ Negative \ Error $ $=1 - (1-p^k)^L,p=P(点 a
和点 b 相同位置 bit 位 值一样)$

#### **p**

现在所有的焦点都聚焦在 $p$ 的计算上了。

p 是 a 和 b 相同位置 bit 位值一样的概率，这个概率怎么计算呢？

我们知道 LSH 的每个签名都是 k bit 位，每个 bit 位取值 0 或 1，相当于每一位都将空间一分为二，其中一个子空间中的值全为 1，另一个全为
0，如下图所示，黄色的线即切割线，出现在黄线两侧的点对应的 bit 位肯定不一样。

![](https://images.gitbook.cn/5dce1950-281e-11eb-badb-4943f989e399)

所以点 a 和点 b 要想在黄线切割空间时产生的 bit 位一样，就必须出现在同侧。也就是说：

$$\begin{align}p &= P(a 和 b 经过空间切割后产生的 bit 位值一样) \\\&=P(a 和 b 出现在线同侧)
\\\&=1-P(a 和 b 出现在线的两侧) \\\&=1-\frac{a 和 b 的夹角}{\frac{\pi}{2}}\end{align}$$

至此，LSH 的错误率分析就结束啦。

### 召回率分析

> 召回率：假设 LSH 给出 top N 个近邻，其中有 n 个确实是 top N 近邻，那么召回率为 n / N，一般认为召回率达到 95% 以上的
> ANN 算法才算合格。

前面都在针对 LSH **犯错** 的分析，所谓奖惩分明方可更好地调动算法的“积极性”，这一节就来看看 LSH **算对** 的分析。

要算出 LSH 的召回率，必须要知道每个商品 GROUNG TRUTH 的相似商品，也即精确的 topN 相似商品，而这只有暴力穷举才能得到，这样假设有 M
个商品，暴力穷举的时间复杂度大约等于 $O(M^2)$，当 M
达到百万或者更高量级时，暴力穷举根本没有办法在合理的时间范围内结束，所以在这种情况下，一般的处理方法是随机抽 S 个商品，只计算这 S 个商品的精确 top
N 相似商品（S 的量级远远小于 M，一般设为 10000），这样就把时间复杂度控制在了 $O(MS)$，用这 S 个商品的结果去评估算法的性能。

计算逻辑如下。

**参数**

  * topN：每个商品需要计算 topN 个最相似商品
  * S：随机抽取 S 个商品，用这 S 个商品的性能指标来衡量 LSH 的优劣
  * M：商品个数

**评估逻辑**

1\. 暴力穷举

    
    
        map_truth<item, sim_items> = empty map
        for sample from 1 to S:
            heap = topN size min heap
            for candidate from 1 to M:
                sim = some_similarity_calculation(sample, candidate)
                if sim > heap.min:
                    heap.delete_min()
                    heap.insert((candidate, sim))
            map_truth[sample] = heap   
         end for
    

得到 S 个商品对应的真实近邻 sim_items。

2\. LSH

    
    
    map_lsh<item, sim_items> = lsh(S samples, topN)
    

调用 LSH 算法，传入 S 个商品以及 topN，直接算出结果。

3\. 召回率计算

    
    
        recall_rate = 0.0
        for sample from 1 to S:
            sim_truths = map_truth.get(sample)
            sim_lsh = map_lsh(sample)
            recall_rate += length(sim_lsh intersect sim_truth) / topN // 交集长度除以 topN
        recall_rate = recall_rate / S
    

经过以上步骤便得到了 LSH 的召回率，一般的 ANN 算法在评估召回率时基本上都是采用这种方式的，很简单吧。

### 应用

当然我们不会自己写 LSH 的代码，Spark 官方已经有了实现。本文中采用的 LSH 实现是 LinkedIn
开源的代码，地址在[这里](https://github.com/linkedin/scanns)，没有别的原因，读者们可以采用自己熟悉的任何实现。

利用 LSH 计算相似度的代码如下：

    
    
    import org.apache.spark.ml.embedding._
    
    // 前几篇介绍的 Word2Vec 生成向量
    val word2Vec = new Word2Vec()
          .setInputCol("sentence")   // 数据表中对应句子的列名
          .setVectorSize(vectorSize)
          .setMaxIter(maxIter)
          .setNegative(5)
          .setHS(0)
          .setSample(1E-3)
          .setMaxSentenceLength(1000)
          .setNumPartitions(200)
          .setSeed(77L)
          .setStepSize(0.025)
          .setWindowSize(windowSize)
          .setMinCount(minCount)
    
    val model = word2Vec.fit(document) // document 就是数据集，只有一列，名为 "sentence"
    val embedding = model.getVectors.rdd.map(r => {
        val word = r.getAs[Long]("word") // 这里的商品 ID 类型为 Long
        val vector = r.getAs[org.apache.spark.ml.linalg.Vector]("vector").toDense
        (word, vector)
    }) // 生成每个商品的向量
    
    /* 该 LSH 出手了 */
    import com.linkedin.nn.algorithm.CosineSignRandomProjectionNNS
    
    val topN = 100
    val lsh = new CosineSignRandomProjectionNNS()
          .setNumHashes(numHashes) // 对应 LSH 算法中的签名个数 L
          .setSignatureLength(signatureLen) // 对应每个签名长度 k
          .setJoinParallelism(1000) // spark join 并行度，根据自己数据大小自行设置
          .setBucketLimit(bucketLimit) // 每个 hash 桶内保留的元素个数 (1)
          .setShouldSampleBuckets(true) // (2)
          .setNumOutputPartitions(100)
          .createModel(vectorSize) // 商品向量长度
    
    val similarities = lsh.getSelfAllNearestNeighbors(embedding, topN) 
    .map { case (item1, item2, distance) => (item1, item2, 1 - distance) } // 因为算出来的是距离，这里转换成相似度了
    .toDF("item1", "item2", "sim")
    

整个计算都很直观，没什么好展开的，这里主要说明以下标注的两行：

  * (1)：bucketLimit 控制了每个 hash 桶内的元素个数上限，本身商品向量并非均匀分布，所以可能某些桶内的元素特别多，这时候就只保留 bucketLimit 个，别的全都丢掉。
  * (2)：在 (1) 步中，全都丢掉，那么怎么丢呢？这里采用了 **水塘抽样** 的方式丢弃超过 bucketLimit 个的商品，本文就不展开讲述了，有兴趣的同学可以自行谷歌，是个很有趣的抽样方法。这里的处理也特别的巧妙。

### 小结

至此，LSH 从算法原理、时间复杂度分析、错误率分析、召回率计算等方面已经介绍完毕，可以看到该算法在 **不特别要求运行速度** 的前提下可以满足绝大多数
**离线** 场景的使用，在实际使用上来看，其效果还是可以的，当然 LSH 的错误率和召回率主要受这两个超参的影响：

  * 签名个数 L：L 越大，LSH 的召回率越高，相应的计算时间就越长。
  * 签名长度 k：k 越大，LSH 的召回率就越低，但是相应的计算时间就越短。

综上，可以看出需要在召回率和运行时间上找到一个平衡点，一般情况下可以固定 k 为 logN（N 为数据点的个数），然后调节
L，通过观察召回率是否达到自己的业务需求来找到合适的 L。

LSH 是简单易用的算法，但是随着时代的发展，各种 ANN 算法层出不穷，数据点的个数和维度也在疯狂地上涨，对于计算延迟的要求也愈加苛刻。LSH
的缺点也让它的使用愈来愈少：

  * 不稳定：LSH 的 L 个签名是通过对原始向量与某个矩阵相乘映射得到的，该矩阵基于某种概率模型生成，结果不稳定。
  * 占空间：LSH 需要大量的存储空间，随着 L 和 k 的变大，空间复杂度呈线性上升。
  * 还不够快：LSH 显然只能作为离线场景去使用，没有办法做到实时查询。

可以在[这里](https://github.com/erikbern/ann-benchmarks)看到当前流行的各种 ANN
算法的表现情况，可以看到这里已经没有了 ANN 的身影。

但是，尽管如此，LSH 还是可以在离线场景且数据点数不是很大的情况下发挥它的作用，请读者们好好掌握它。

不知道眼光锐利的你有没有发现，[还是这里](https://github.com/erikbern/ann-benchmarks)提到的诸多 ANN
算法中，有一个叫 **HNSW** 的 ANN 算法好像有点鹤立鸡群的味道，而且向量维度越大，它的优势越明显，这勾起了我的强烈兴趣。

不着急，以后需要的时候咱们再来吃掉它！

下一篇的内容，暂时卖个关子，但是绝对会比这个专栏到现在为止的所有文章更能吸引你。

咱们，下篇见！

