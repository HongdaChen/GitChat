现在我们回到 LR 模型本身。

### 回归模型做分类

从前面关于分类与回归的定义来看，分类模型和回归模型似乎是泾渭分明的。输出离散结果的就是用来做分类的，而输出连续结果的，就用来做回归。

我们前面讲的两个模型：线性回归的预测结果是一个连续值域上的任意值，而朴素贝叶斯分类模型的预测结果则是一个离散值。

但 LR 却是用来做分类的。它的模型函数为：

$h_\theta(x) = \frac{1}{1 + e^{-\theta^Tx }} $

设 $z = \theta^T x$，则

$h(z) = \frac{1}{1 + e^{-z }} $

在二维坐标中形成 S 形曲线：

![enter image description
here](http://images.gitbook.cn/def9a1f0-5285-11e8-804e-bb449981bb15)

上图中，$z$ 是自变量（横轴），最终计算出的因变量 $y$（纵轴），则是一个 [0,1] 区间之内的实数值。

一般而言，当 $y>0.5$ 时，$z$ 被归类为真（True）或阳性（Positive），否则当 $y <=0.5$ 时，$z$
被归类为假（False）或阴性（Negative）。

所以，在模型输出预测结果时，不必输出 $y$ 的具体取值，而是根据上述判别标准，输出1（真）或0（假）。

因此，LR 典型的应用是二分类问题上，也就是说，把所有的数据只分为两个类。

**注意：** 当然，这并不是说 LR 不能处理多分类问题，它当然可以处理，具体方法稍后讲。我们先来看 LR 本身。

看到此处，大家是不是会有点担心，如果大量的输入得到的结果都在 $y=0.5$ 附近，那岂不是很容易分错？

说得极端一点，如果所有的输入数据得出的结果都在 $y=0.5$ 附近，那岂不是没有什么分类意义了，和随机乱归类结果差不多？

这样的担心其实是不必要的。此模型函数在 $y=0.5$
附近非常敏感，自变量取值稍有不同，因变量取值就会有很大差异，所以不用担心出现大量因细微特征差异而被归错类的情况——这也正是逻辑回归的“神奇”之处。

### 逻辑回归的目标函数

有了模型函数，来看看逻辑回归的目标函数。

逻辑函数 $h(x)$ 是我们要通过训练得出来的最终结果。在最开始的时候，我们不知道其中的参数 $\theta$ 的取值，我们所有的只是若干的 $x$
和与其对应的 $y$（训练集合）。训练 LR 的过程，就是求 $\theta$ 的过程。

首先要设定一个目标：我们希望这个最终得出的 $\theta$ 达到一个什么样的效果——我们当然是希望得出来的这个
$\theta$，能够让训练数据中被归为阳性的数据预测结果都为阳，本来被分为阴性的预测结果都为阴。

而从公式本身的角度来看，$h(x)$ 实际上是 $x$ 为阳性的分布概率，所以，才会在 $h(x) > 0.5$ 时将 $x$ 归于阳性。也就是说
$h(x) = P(y=1)$。反之，样例是阴性的概率 $P(y=0) = 1 - h(x)$。

当我们把测试数据带入其中的时候，$P(y=1)$ 和 $P(y=0)$ 就都有了先决条件，它们为训练数据的 $x$ 所限定。因此：

$P(y=1|x) = h(x); P(y=0|x) = 1- h(x)$。

根据 **二项分布** 公式，可得出 $P(y|x) = h(x) ^y(1- h(x))^{(1-y)}$。

假设我们的训练集一共有 m 个数据，那么这 m 个数据的联合概率就是：

$L(\theta) = \prod_{i=1}^{m}P(y^{(i)}|x^{(i)};\theta) =
\prod_{i=1}^{m}(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{(1-y^{(i)})}$

我们求取 $\theta$ 的结果，就是让这个 $L(\theta)$ 达到最大。

还记得我们之前在朴素贝叶斯分类器中讲到的 **极大似然估计** 吗？其实此处 LR 目标函数的构建过程也是依据极大似然估计。

$L(\theta)$ 就是 LR 的 **似然函数** 。我们要让它达到最大，也就是对其进行“ **极大估计** ”。因此，求解 LR
目标函数的过程，就是对 LR 模型函数进行极大似然估计的过程。

为了好计算，我们对它求对数。得到 **对数似然函数** ：

$ l(\theta) = \log(L(\theta)) = \sum_{i=1}^{m}[y^{(i)}\log(h_\theta(x^{(i)}))
+ (1-y^{(i)})\log(1-h_\theta(x^{(i)}))]$

我们要求出让 $l(\theta)$ 能够得到最大值的 $\theta$。

$l(\theta)$ 其实可以作为 LR 的目标函数。前面讲过，我们需要目标函数是一个凸函数，具备最小值。因此我们设定：$J(\theta) =
-l(\theta) $。

$ J(\theta) = - \log(L(\theta)) =
-\sum_{i=1}^{m}[y^{(i)}\log(h_\theta(x^{(i)})) +
(1-y^{(i)})\log(1-h_\theta(x^{(i)}))]$

这样，求 $l(\theta)$ 的最大值就成了求 $J(\theta)$ 的最小值。$J(\theta)$ 又叫做 **负对数似然函数** 。它就是
**LR 的目标函数** 。

#### 优化算法

我们已经得到了 **LR 的目标函数 $J(\theta)$** ，并且 **优化目标是最小化它** 。

如何求解 $\theta$ 呢？具体方法其实有很多。此处我们仍然运用之前已经学习过的，最常见最基础的梯度下降算法。

基本步骤如下：

• 通过对 $J(\theta)$ 求导获得下降方向—— $J ' (\theta) $；

• 根据预设的步长 $\alpha$，更新参数 $\theta := \theta − \alpha J’(θ)$；

• 重复以上两步直到逼近最优值，满足终止条件。

![enter image description
here](http://images.gitbook.cn/f048fa90-5286-11e8-bcd6-e300dcfa6492)

既然知道了方法，我们就来计算一下。

已知：

$ J(\theta) = - \log(L(\theta)) =
-\sum_{i=1}^{m}[y^{(i)}\log(h_\theta(x^{(i)})) +
(1-y^{(i)})\log(1-h_\theta(x^{(i)}))]$

$J(\theta)$ 对 $\theta$ 求导：

$\frac{\partial{ J(\theta)}}{\partial{\theta}} =
-\sum_{i=1}^{m}[y^{(i)}\frac{h_\theta'(x^{(i)})}{h_\theta(x^{(i)})} -
(1-y^{(i)})\frac{h_\theta'(x^{(i)})}{(1-h_\theta(x^{(i)}))}] =
\sum_{i=1}^{m}[(-y^{(i)})\frac{h_\theta'(x^{(i)})}{h_\theta(x^{(i)})}+(1-y^{(i)})\frac{h_\theta'(x^{(i)})}{(1-h_\theta(x^{(i)}))}]
$

因为有：

$h'(z) = \frac{d(\frac{1}{1+e^{-z}})}{dz} = -(\frac{-e^{-z}}{(1 + e^{-z})^2})
= \frac{e^{-z}}{1+e^{-z}}\frac{1}{1+e^{-z}} = (1-
\frac{1}{1+e^{-z}})(\frac{1}{1+e^{-z}} ) = h(z)(1 - h(z))$

同时，运用链式法则，有：

$\frac{\partial{ h_\theta(x)}}{\partial{\theta}} = \frac{\partial{
h_\theta(x)}}{\partial{(\theta x)}} x = h_\theta(x)(1 - h_\theta(x))x $

将上式带入上面的 $J(\theta)$ 求导式子里，有：

$\frac{\partial{ J(\theta)}}{\partial{\theta}} =
\sum_{i=1}^{m}[(-y^{(i)})\frac{h_\theta(x^{(i)})(1-
h_\theta(x^{(i)}))x^{(i)}}{h_\theta(x^{(i)})} +
(1-y^{(i)})\frac{h_\theta(x^{(i)})(1-
h_\theta(x^{(i)}))x^{(i)}}{(1-h_\theta(x^{(i)}))}] = \sum_{i=1}^{m}[-y^{(i)} +
y^{(i)}h_\theta(x^{(i)}) + h_\theta(x^{(i)}) - y^{(i)}h_\theta(x^{(i)})
]x^{(i)} = \sum_{i=1}^{m}[ h_\theta(x^{(i)}) -y^{(i)}]x^{(i)}$

当 $x$ 为多维的时候（设 $x$ 有 $n$ 维），则在对 $z=\theta x$ 求导的时候，要对 $x$ 的每一个维度求导。

又因为 $\theta$ 和 $x$ 维度相同，所以当 $x$ 有 $n$ 维的时候，$\theta$ 同样是有 $n$ 维的。则 $J(\theta)$
的求导也变成了对 $\theta$ 的每一个维度求导：

$\frac{\partial{ J(\theta)}}{\partial{\theta_j}} = \sum_{i=1}^{m}[
h_\theta(x^{(i)}) -y^{(i)}]x_j^{(i)} ;\quad j = 1, 2, ..., n$

因此，优化算法伪代码为：

> Set initial value: $\theta_0, \alpha$
>
> while (not convergence)
>
> {
>
> $\qquad \theta_j := \theta_j + \alpha\sum_{i=1}^{m}( y^{(i)} -
> h_\theta(x^{(i)}))x_j^{(i)}$
>
> }

### 实例及代码实现

我们来看一个例子，比如某位老师想用学生上学期考试的成绩（Last Score）和本学期在学习上花费的时间（Hours Spent）来预期本学期的成绩：

![enter image description
here](http://images.gitbook.cn/8884fe80-5287-11e8-ae90-8538fe442b90)

面对这样一个需求，我们可能首先想到的是线性回归，毕竟，要做的是预测本次的成绩。那样的话，我们取 X = [“Last Score”, “Hours
Spent”]，y = “Score”。

用线性回归实现代码如下：

    
    
        from sklearn.linear_model import LogisticRegression
        from sklearn.linear_model import LinearRegression
        import pandas as pd
    
        # Importing dataset
        data = pd.read_csv('quiz.csv', delimiter=',')        
        used_features = ["Last Score", "Hours Spent"]
        X = data[used_features].values
        scores = data["Score"].values
    
        X_train = X[:11]
        X_test = X[11:]
    
        # Linear Regression - Regression
        y_train = scores[:11]
        y_test = scores[11:]
    
        regr = LinearRegression()
        regr.fit(X_train, y_train)
        y_predict = regr.predict(X_test)
    
        print(y_predict)
    

我们把前11个样本作为训练集，最后3个样本作为测试集。

这样训练出来之后，得到的预测结果为：[55.33375602 54.29040467 90.76185124]，也就说 id 为 12、13、14
的三个同学的预测分数为55、54和91。

第一个差别比较大，id 为12的同学，明明考及格了，却被预测为不及格。

这是为什么呢？大家注意 id 为4的同学，这是一位学霸，他只用了20小时在学习上，却考出了第一名的好成绩。

回想一下线性回归的目标函数，我们不难发现，所有训练样本对于目标的贡献是平均的，因此，4号同学这种超常学霸的出现，在数据量本身就小的情况下，有可能影响整个模型。

这还是幸亏我们有历史记录，知道上次考试的成绩，如果 X 只包含“Hours Spent”，学霸同学根本就会带偏大多数的预测结果（自变量只有“Hours
Spent”的线性回归模型会是什么样的？这个问题留给同学们自己去实践）。

那么我们看看用逻辑回归如何。用逻辑回归的时候，我们就不再是预测具体分数，而是预测这个学生本次能否及格了。

这样我们就需要对数据先做一下转换，把具体分数转变成是否合格，合格标志为1，不合格为0，然后再进行逻辑回归：

    
    
        from sklearn.linear_model import LogisticRegression
        from sklearn.linear_model import LinearRegression
        import pandas as pd
    
        # Importing dataset
        data = pd.read_csv('quiz.csv', delimiter=',')
    
        used_features = [ "Last Score", "Hours Spent"]
        X = data[used_features].values
        scores = data["Score"].values
    
        X_train = X[:11]
        X_test = X[11:]
    
        # Logistic Regression – Binary Classification
        passed = []
    
        for i in range(len(scores)):
            if(scores[i] >= 60):
                passed.append(1)
            else:
                passed.append(0)
    
        y_train = passed[:11]
        y_test = passed[11:]
    
        classifier = LogisticRegression(C=1e5)
        classifier.fit(X_train, y_train)
    
        y_predict = classifier.predict(X_test)
        print(y_predict)
    

这次的输出就是[1 0 1]，对12、13、14号同学能否通过本次考试的判断是正确的。

### LR 处理多分类问题

LR 是用来做二分类的，但是如果我们面对的是多分类问题：样本标签的枚举值多于2个，还能用 LR 吗？

当然是可以的。我们可以把二分类问题分成多次来做。

假设你一共有 n 个标签（类别），也就是说可能的分类一共有 n 个。那么就构造 n 个 LR 分类模型，第一个模型用来区分 `label_1`和 `non-
label _1`（即所有不属于 `label_1` 的都归属到一类），第二个模型用来区分 `label_2` 和 `non-label _2`……, 第
n 个模型用来区分 `label_n` 和 `non-label _n`。

使用的时候，每一个输入数据都被这 n 个模型同时预测。最后哪个模型得出了 Positive 结果，就是该数据最终的结果。

如果有多个模型都得出了 Positive，那也没有关系。因为 LR 是一个回归模型，它直接预测的输出不仅是一个标签，还包括该标签正确的概率。那么对比几个
Positive 结果的概率，选最高的一个就是了。

例如，有一个数据，第一和第二个模型都给出了 Positive 结果，不过 `label_1` 模型的预测值是0.95，而 `label_2`
的结果是0.78，那么当然是选高的，结果就是 `label_1`。

说起原理来好像挺麻烦，好在 sklearn 已经为我们处理了多分类问题，我们用 sklearn 来做多分类的时候，只是需要把 y
准备好，其他的，都和做二分类一样就可以了。

比如还是上面的例子，现在我们需要区分：学生的本次成绩是优秀（>=85），及格，还是不及格。我们就在处理 y 的时候给它设置三个值：0
（不及格）、1（及格）和2（优秀），然后再做 LR 分类就可以了。代码如下：

    
    
        from sklearn.linear_model import LogisticRegression
        from sklearn.linear_model import LinearRegression
        import pandas as pd
    
        # Importing dataset
        data = pd.read_csv('quiz.csv', delimiter=',')
    
        used_features = [ "Last Score", "Hours Spent"]
        X = data[used_features].values
        scores = data["Score"].values
    
        X_train = X[:11]
        X_test = X[11:]
    
        # Logistic Regression - Multiple Classification
        level = []
    
        for i in range(len(scores)):
            if(scores[i] >= 85):
                level.append(2)
            elif(scores[i] >= 60):
                level.append(1)
            else:
                level.append(0)
    
        y_train = level[:11]
        y_test = level[11:]
    
        classifier = LogisticRegression(C=1e5)
        classifier.fit(X_train, y_train)
    
        y_predict = classifier.predict(X_test)
        print(y_predict)
    

测试集的输出是：[1 0 2] —— 12号及格，13号不及格，14号优秀，还是蛮准的。

### 附录

quiz.csv 文件：

> Id,Last Score,Hours Spent,Score
>
> 1,90,117,89
>
> 2,85,109,78
>
> 3,75,113,82
>
> 4,98,20,95
>
> 5,62,116,61
>
> 6,36,34,32
>
> 7,87,120,88
>
> 8,89,132,92
>
> 9,60,83,52
>
> 10,72,92,65
>
> 11,73,112,71
>
> 12,56,143,62
>
> 13,57,97,52
>
> 14,91,119,93

