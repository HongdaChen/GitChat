### 从有监督学习到无监督学习

有监督学习和无监督学习，是机器学习两个大的类别。我们之前讲的都是有监督学习，毕竟有监督学习现阶段还是机器学习在实际应用中的主流。

关于有监督学习和无监督学习的区别，在第5课里有过讲解。在此，进一步说明下。

#### 有监督学习（Supervised Learning）

所谓 **有监督学习** ，即：

  * 训练数据同时拥有输入变量（$x$）和输出变量（$y$）；

  * 用一个算法把从输入到输出的映射关系——$y = f(x)$——学习出来；

  * 当我们拿到新的数据 $x'$ 后，就可以通过已经被学习出的 $f(\cdot)$，得到相应的 $y'$。

有监督学习就像在学校上课——老师给我们留作业，盯着我们做作业，再给我们判作业。

每道作业题（输入变量），都有正确答案（输出变量）；而整个算法运行的过程，就像有一个老师在监督着学生的每个解答，跟随指导，一旦出现错题立刻予以纠正——把有可能“跑偏”的参数给“拽”回来。

等到老师觉得学生已经掌握了现在的知识，就可以下课了——有监督学习在算法获得可接受的性能之后便停止。

![enter image description
here](https://images.gitbook.cn/e8ed0200-9ec6-11e8-b6f3-454e1d4b65e0)

#### 无监督学习（Unsupervised Learning）

**无监督学习** 和有监督学习相对，即：

  * 训练数据只有输入变量（$x$），并 **没有输出变量** ；

  * 无监督学习的目的是 **将这些训练数据潜在的结构或者分布找出来** ，以便于我们对这些数据有更多的了解。

无监督学习没有正确答案也没有老师，只有算法自己在数据中探索，去发现蕴含在数据之中的有趣结构。

比较起来，有监督学习可以类比人类学习已有知识；而无监督学习则更像是去探索新的课题。

![enter image description
here](https://images.gitbook.cn/f0789e30-9ec6-11e8-8324-45c28b509596)

#### 半监督学习（Semi-supervised Learning）

还有一种介于有监督和无监督之间的半监督学习，或者说是一种混合应用有监督和无监督学习的方法。

它所对应的场景是： **有一部分训练数据的输入变量（$x$）有对应的输出变量（$y$），另一些则没有** 。

有监督学习虽然有效，但标注数据（给训练数据的 $x$ 指定正确的 $y$）在目前还是一种劳动力密集型的人工劳动，所需投入巨大。

在现实当中，出现了一些问题，如果用有监督学习效果会比较好，可惜标注数据太少，大量数据都没有被人工标注过。

在这种情况下，我们可以尝试：

  * 首先，用无监督学习技术来发现和学习输入变量的结构；

  * 然后，用有监督学习对未标注数据的输出结果进行“猜测”；

  * 最后，再将带着猜测标签的数据作为训练数据训练有监督模型。

这就是 **半监督学习** 。

下图可见这三类学习算法的差异：

![enter image description
here](https://images.gitbook.cn/a57113b0-9ec4-11e8-b6f3-454e1d4b65e0)

### 发展趋势

单纯就机器学习而言，目前，无论是模型、算法的研究还是在实际问题上的应用，都以有监督学习为主流。

原因很简单：有监督学习的预测结果可控，优化目标明确，因此只要方法得当，数据质量好，一般模型质量也能比较好。

而无监督学习呢，最终能得出什么结果，可能建模的人自己都不知道；有了结果也不知道往哪个方向去调优；现有的数据好不容易调出了一个可以接受的结果，新数据进来，重新学习后的模型和之前大相径庭……

不过随着大数据时代的来临，各行各业各类数据存量和增量迅速攀升，无监督学习的重要性也随之悄然提升。

究其原因，还是那个最简单的因素： **成本** ——对有监督学习而言，没有标注数据，一切都是空谈，而标注工作需要投入大量人工成本：

  * 有些数据，虽然样本标注相对简单，但因为和业务结合紧密而随时需要调整标注原则；

  * 有些数据，需要的标注量极大，比如图片标注，一张人体或人脸图片就需要标出少则十几个多则几十个关键点；

  * 还有些数据，需要深厚的领域知识才有可能做出标注，比如医学图像的诊断等；

而所有的数据当被派遣给不同的标注人做标注后，又都面临着一致性的问题。

一面是大量易得的源数据，另一面是高昂的标注成本。这种客观的情况，也促进了半监督学习等中间地带方法的出现和应用。

当然，从实际的效用角度而言，真正应用于实际问题解决的模型还是以有监督学习为主。不过在当前大数据技术普及的背景之下，数据分析，机器学习，特别是深度学习方法的研究中，无监督学习越来越被重视。

从今天开始，我们就要进入无监督模型的学习。首先，我们来讲讲： **KNN** 。

其实 **KNN** 是一个 **有监督学习** 算法，为什么要放在无监督学习的第一课来讲呢？这个稍后再解释。

### KNN 算法

KNN（K-Nearest Neighbor，译作 K-近邻居）算法，是一种既可以用于分类，又可以用于回归的非参数统计方法。

KNN 是一种基于实例的学习，是所有机器学习算法中最简单的一个。

#### KNN 算法原理

KNN 算法的 **基本思想** 是：

  * 训练数据包括样本的特征向量（$x$）和标签（$y$）；

  * $k$ 是一个常数，由用户来定义；

  * 一个没有标签的样本进入算法后，首先找到与它距离最近的 $k$ 个样本，然后用它这 $k$ 个最近邻居的标签来确定它的标签。

![enter image description
here](https://images.gitbook.cn/f519ab50-9ecb-11e8-b6f3-454e1d4b65e0)

KNN 算法的 **步骤** 如下。

  1. 算距离：给定未知对象，计算它与训练集中的每个样本的距离——特征变量是连续的情况下，将欧氏距离作为距离度量；若特征是离散的，也可以用重叠度量或者其他指标作为距离，这要结合具体情况分析。

  2. 找近邻：找到与未知对象距离最近的 $k$ 个训练样本。

  3. 做分类/回归：在这 $k$ 个近邻中出现次数最多的类别作为未知对象的预测类别（多数表决法），或者是取 $k$ 个近邻的目标值平均数，作为未知对象的预测结果。

多数表决法有个问题：如果训练样本的类别分布不均衡，出现频率较多的样本将会主导预测结果。

这一问题的解决办法有多种，其中常见的一种是：不再简单计算 $k$ 个近邻中的多数，而是同时考虑 $k$ 个近邻的距离，$k$
近邻中每一个样本的类别（或目标值）都以距离的倒数为权值，最后求全体加权结果。

#### 有监督学习算法 KNN VS 无监督学习算法 KMeans

前面说了，KNN 是有监督学习模型。无论是做分类还是做回归，KNN 的每个训练样本都带有一个标签（目标值或类别）。

既然是有监督学习算法，我们为什么样要放在这一章讲呢？就是为了和 KMeans 做对比!

为什么要和 KMeans 做对比呢？

原因之一是：这两个算法虽然非常不同，但却经常被初学者搞混，可能是因为名字乍看有几分形似吧。

原因之二是：两者都有一个 $k$——这个 $k$ 还都是一个需要用户主动指定的常数。两个 $k$
虽然含义和作用不同，但在重要性程度和取值的“艺术性”上，却颇有些异曲同工之妙。

#### KNN 的 k

在 KNN 算法中，假设训练样本一共有 $m$ 个，当一个待预测样本进来的时候，它要与每一个训练样本进行距离计算，然后从中选出 $k$ 个最近的邻居，根据这
$k$ 个近邻标签确定自己的预测值。

此处的 $k$，是一个正整数。若 $k = 1$，则该对象的预测值直接由最近的一个样本确定。若 $k=m$，则整个训练集共同确定待测样本。

通常情况下，$k > 1$，但也不会太大，是一个较小的正整数。具体取何值最佳，则取决于训练数据和算法目标。

一般情况下， $k$ 值越大，受噪声的影响越小；但 $k$ 值越大，也越容易模糊类别之间的界限。

比如下面这个例子，用 KNN 做分类，黄色为 A 类，紫色为 B 类，红色的是待测样本。

![enter image description
here](https://images.gitbook.cn/929096f0-9ec7-11e8-8324-45c28b509596)

当我们取 $k=3$ 时，根据多数选举法，预测结果为 B；但当 $k=6$ 时，依然是根据多数选举法，预测结果就成为了 A。

可见，$k$ 的取值大小，直接影响着算法的结果。

当然，超参数的选择并不是 KNN 独有的问题，而是一个机器学习常见的共性问题。

专门有一系列超参数最优化方法（例如网格搜索法、随机搜索法、贝叶斯最优化法等），来帮助我们选择最佳的超参数。

因为 $k$ 是 KNN 算法唯一的超参数，因此，它对于 KNN 尤其重要。这一点和 KMeans 的 $k$ 参数之于 KMeans，颇为神似。

