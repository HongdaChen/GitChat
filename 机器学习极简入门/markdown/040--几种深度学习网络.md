### 神经网络的历史沿革

![enter image description
here](https://images.gitbook.cn/dfb6d380-a9d1-11e8-95af-1d569340e767)

#### 缘起

1943年，神经生理学家和神经元解剖学家 Warren McCulloch 和数学家 Walter Pitts
提出了神经元的数学描述和结构，并且证明了只要有足够的简单神经元，在它们互相连接并同步运行的情况下，可以模拟任何计算函数。

这样开创性的工作被认为是 NN 的起点。

#### 几度兴衰

1958年，计算机学家 Frank Rosenblatt 提出了一种具有三级结构的
NN，称为“感知机”（Perceptron）。它实际上是一种二元线性分类器，可以被看作一种单层 NN（参见下图）。

![enter image description
here](https://images.gitbook.cn/a5c16900-ac2a-11e8-b45b-2b163577bdfd)

Rosenblatt 还给出了相应的感知机学习算法。

尽管结构简单，感知机能够学习并解决相当复杂的问题，在60年代掀起了 NN 研究的第一次热潮。很多人都认为只要使用成千上万的神经元，他们就能解决一切问题。

这股热潮持续了10年，终于因为感知机的作用终归有限（比如它不能处理线性不可分问题），在实践中无法产生实际的价值，而导致了 NN 发展的第一次低潮期。

直到80年代，NN 的研究才开始复苏。

1986年，David Rumelhart、Geoffrey Hinton 和 Ronald Williams 将反向传播算法用于多层 NN 的训练，带来了
NN 的第二春。

然而，训练 NN，最开始都是随机初始化权值。当 NN 的层数稍多之后，随机的初始值很可能导致反复迭代仍不收敛——根本训练不出来可用的 NN。

进一步的研究和实际应用都受阻。

基于统计的学习模型有严格的理论基础，可以在数学上严格地被证明为是凸优化问题。特别是在 SVM/SVR 出现后，用统计学习模型执行复杂任务也能得到不错的结果。

而 NN
缺少数学理论支持——它的优化过程不是凸优化，根本不能从数学原理上证明最优解的存在；就算训练出了结果，也无法解释自己为什么有效；在实际运用的效果又不够好。

如此种种，NN 研究进入第二次低谷。此后十几年的时间里，大多数研究人员都放弃了 NN。

#### 从 NN 到 DNN

Hinton 却矢志不渝地坚持着对 NN 的研究。终于在2006年迎来了划时代的成果。这一年，Hinton 发表了经典论文[“Reducing the
Dimensionality of Data with Neural
Networks”](http://www.cs.toronto.edu/~hinton/science.pdf)。

这篇论文提出了预训练（Pre-training）的方法（可以简单地想象成是“一层一层”地训练），分层初始化，使得深层神经网络（Deep Neural
Network，DNN）的训练变得可能——训练 NN 不必再局限在很少的一两层，四五层甚至八九层都成为了可能。

由此，NN 重新回到大众的视线中，从此 NN 进入了 DNN 时代。

### 深度学习（Deep Learning）

#### 什么是深度学习

现在我们说的深度学习一词，其实在30多年前就已经被提出来了。Rina Dechter 在1986年的论文中就提到了“ Shallow
Learning”和“Deep Learning”。不过直到2000年，这个说法才被引入到 NN 领域。

现在我们说的 **深度学习** 指利用多层串联的非线性处理单元，进行特征提取和转化的机器学习算法。其结构中的不同层级对应于不同程度的数据抽象。

DNN 就是一种典型的深度学习模型。其他的，像 CNN、RNN、LSTM 等，都属于这一领域。

如今，深度学习被看作是通向人工智能的重要一步，也是人工智能实现技术中的热门。

#### 深度学习的爆发

说到深度学习的爆发，可谓天时地利人和。

  * 天时：互联网普及，数据井喷；大数据时代来临，获取、存储和处理数据的技术蓬勃发展。

  * 地利：GPU 被应用到深度学习模型的训练和推断中，极大地提升了运算能力。

  * 人和：Hinton 老先生的几篇论文，将研究人员的焦点吸引到了 DNN 等 DL 技术上。

几种因素叠加，使得深度学习技术在许多实践领域（例如语音识别、语音合成、手写识别、人脸识别、图片分类、情感分析等）大幅提高了自动化的准确率，从而引起了深度学习的大爆发。

### 不同种类的深度学习网络

下面我们将介绍几种深度学习领域中比较常用的网络结构。

这几种结构是深度学习中最基础最常见的一小部分内容，而且在此仅仅是告诉大家“有什么”，对于它们“是什么”只限于提及，不做展开介绍。

#### 卷积神经网络（Convolutional Neural Network, CNN）

CNN 是一种前馈神经网络，通常由一个或多个卷积层（Convolutional Layer）和全连接层（Fully Connected
Layer，对应经典的 NN）组成，此外也会包括池化层（Pooling Layer）。

CNN 的结构使得它易于利用输入数据的二维结构。

> 注意： **前馈神经网络** （Feedforward NN）指每个神经元只与前一层的神经元相连，数据从前向后单向传播的
> NN。其内部结构不会形成有向环（对比后面要讲到的 RNN/LSTM）。
>
> 它是最早被发明的简单 NN 类型，前面讲到的 NN、DNN 都是前馈神经网络。

每个卷积层由若干卷积单元组成——可以想象成经典 NN 的神经元，只不过激活函数变成了卷积运算。

卷积运算是有其严格的数学定义的。不过在 CNN 的应用中，卷积运算的形式是数学中卷积定义的一个特例，它的目的是提取输入的不同特征。

一般情况下，从直观角度来看，CNN 的卷积运算，就是下图这样：

![enter image description
here](https://images.gitbook.cn/77843400-ab7b-11e8-97f1-1df9b888ef68)

>
> 上图中左侧的蓝色大矩阵表示输入数据，在蓝色大矩阵上不断运动的绿色小矩阵叫做卷积核，每次卷积核运动到一个位置，它的每个元素就与其覆盖的输入数据对应元素相乘求积，然后再将整个卷积核内求积的结果累加，结果填注到右侧红色小矩阵中。
>
> 卷积核横向每次平移一列，纵向每次平移一行。最后将输入数据矩阵完全覆盖后，生成完整的红色小矩阵就是卷积运算的结果。

CNN 经常被用于处理图像，那么对应的输入数据就是一张图片的像素信息。

对于这样的输入数据，第一层卷积层可能只能提取一些低级的特征，如边缘、线条、角等，更多层的网络再从低级特征中迭代提取更复杂的特征。

![enter image description
here](https://images.gitbook.cn/5014b4f0-ace6-11e8-a984-b10ddfbee5a0)

CNN 结构相对简单，可以使用反向传播算法进行训练，这使它成为了一种颇具吸引力的深度学习网络模型。

除了图像处理，CNN 也会被应用到语音、文本处理等其他领域。

#### 循环神经网（Recurrent Neural Network，RNN）

RNN，循环神经网络，也有人将它翻译为 **递归神经网络** 。从这个名字就可以想到，它的结构中存在着“环”。

确实，RNN 和 NN/DNN 的数据单一方向传递不同。RNN 的神经元接受的输入除了“前辈”的输出，还有自身的状态信息，其状态信息在网络中循环传递。

RNN 的结构用图形勾画出来，是下图这样的：

![enter image description
here](https://images.gitbook.cn/4abdb990-acc6-11e8-a354-77de0f9f71bc)

图 1

> 注意：图中的 $A$ 并不是一个神经元，而是一个神经网络块，可以简单理解为神经网络的一个隐层。

RNN
的这种结构，使得它很适合应用于序列数据的处理，比如文本、语音、视频等。这类数据的样本间存在顺序关系（往往是时序关系），每个样本和它之前的样本存在关联。

RNN 把所处理的数据序列视作时间序列，在每一个时刻 $t$，每个 RNN 的神经元接受两个输入：当前时刻的输入样本 $x_t$，和上一时刻自身的输出
$h_{t-1}$。

$t$ 时刻的输出 $h_t = F_{\theta}(h_{t-1}, x_t)$。

输出值 $h_{t}$，又是本神经元下一个时刻的两个输入之一（另一个输入是 $x_{t+1}$)。

**图1** 经过进一步简化，将隐层的自连接重叠，就成了下图：

![enter image description
here](https://images.gitbook.cn/5ef40580-acc7-11e8-a354-77de0f9f71bc)

图2

上图展示的是最简单的 RNN 结构，此外 RNN 还存在着很多变种，比如双向 RNN（Bidirectional RNN），深度双向 RNN（Deep
Bidirectional RNN）等。

RNN 的作用最早体现在手写识别上，后来在语音和文本处理中也做出了巨大的贡献，近年来也不乏将其应用于图像处理的尝试。

#### 长短时记忆（Long Short Term Memory，LSTM）

LSTM 可以被简单理解为是一种神经元更加复杂的 RNN，处理时间序列中当间隔和延迟较长时，LSTM 通常比 RNN 效果好。

相较于构造简单的 RNN 神经元，LSTM 的神经元要复杂得多，每个神经元接受的输入除了当前时刻样本输入，上一个时刻的输出，还有一个元胞状态（Cell
State），LSTM 神经元结构请参见下图：

![enter image description
here](https://images.gitbook.cn/07434680-acf3-11e8-b90c-4d816df0b568)

LSTM 神经元中有三个门。

  * 遗忘门（Forget Gate)：接受 $x_t$ 和 $h_{t-1}$ 为输入，输出一个$0$到$1$之间的值，用于决定在多大程度上保留上一个时刻的元胞状态 $c_{t-1}$，$1$表示全保留，$0$表示全抛弃。

$f_t=\sigma(W_fx_t+U_fh_{t-1})$

  * 输入门（Input Gate）: 用于决定将哪些信息存储在这个时刻的元胞状态 $c_t$ 中。

$i_t=\sigma(W_ix_t+U_ih_{t-1}) $

$\tilde{c}_t=\tanh(W_cx_t+U_ch_{t-1})$

$c_t=f_t * c_{t-1} + i_t * \tilde{c}_t$

  * 输出门（Output Gate）：用于决定输出哪些信息。

$o_t=\sigma(W_ox_t+U_oh_{t-1})$

$y_t = h_t=o_t *\tanh(c_t)$

所以，虽然从连接上看，LSTM 和 RNN 颇为相似，但两者的神经元却相差巨大，我们可以看一下下面两个结构图的对比：

![enter image description
here](https://images.gitbook.cn/11f57510-ad09-11e8-b90c-4d816df0b568)

LSTM 的结构图

![enter image description
here](https://images.gitbook.cn/ce097f80-ad09-11e8-b90c-4d816df0b568)

RNN 的结构图

> 注意：如果把 LSTM 的遗忘门强行置0，输入门置1，输出门置1，则 LSTM 就变成了标准 RNN。

可见 LSTM 比 RNN 复杂得多，要训练的参数也多得多。

但是，LSTM 在很大程度上缓解了一个在 RNN 训练中非常突出的问题： **梯度消失/爆炸（Gradient Vanishing/Exploding）**
。这个问题不是 RNN 独有的，深度学习模型都有可能遇到，但是对于 RNN 而言，特别严重。

梯度消失和梯度爆炸虽然表现出来的结果正好相反，但出现的 **原因** 却是一样的。

因为神经网络的训练中用到反向传播算法，而这个算法是基于梯度下降的——在目标的负梯度方向上对参数进行调整。如此一来就要对激活函数求梯度。

又因为 RNN 存在循环结构，因此激活函数的梯度会乘上多次，这就导致：

  * 如果梯度小于1，那么随着层数增多，梯度更新信息将会以指数形式衰减，即发生了 **梯度消失（Gradient Vanishing）** ；
  * 如果梯度大于1，那么随着层数增多，梯度更新将以指数形式膨胀，即发生 **梯度爆炸（Gradient Exploding）** 。

因为三个门，尤其是遗忘门的存在，LSTM 在训练时能够控制梯度的收敛性，从而梯度消失/爆炸的问题得以缓解，同时也能够保持长期的记忆性。

果然，LSTM 在语音处理、机器翻译、图像说明、手写生成、图像生成等领域都表现出了不俗的战绩。

> 注意：深度学习领域专门应对梯度消失/爆炸问题的方法和手段有很多，比如引入 ReLu 等激活函数替代 Sigmoid 函数等。而 LSTM
> 的主要意义在于对 RNN 的改进，并非专门用来解决梯度消失/爆炸问题。

