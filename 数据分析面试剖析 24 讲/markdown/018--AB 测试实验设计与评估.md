A/B 测试已经成为互联网领域最常见的定量试验与数据收集方式，也是产品、运营和数据分析师的必备能力。对于互联网公司来说，A/B
测试是一种有效的精细化运营手段，过去很多依靠经验的粗放式策略管理，通过 A/B 测试改变为可量化的精准决策。

A/B
测试是一种通过已有客观指标，通过对比不同分组方案来衡量哪种效果最佳的方法。它的优势在于能够在“真实的线上环境中”，通过部分或者少量用户验证不同的方案。

例如，在对产品进行 A/B 测试时，我们可以为同一个优化目标（提升支付率）制定两个方案，让一部分用户使用 A 方案，另一部分用户使用 B
方案，统计并对比不同方案的支付率等指标，以判断不同方案的优劣并进行决策，从而提升支付转化率。

### 为什么要进行 A/B 测试？

对于互联网公司来说，经常通过实验的方法对比指标，进而来衡量和验证某一些策略是否更优，以此来提高用户体验和提升产品收益。

使用 A/B 测试，优势如下：

  * 可以利用小样本的抽样方法，评估对整体的影响，进而节省时间和成本，这样可以让更多的想法或者策略得以快速地验证；
  * 通过实验对比指标，能够找到产品问题的真正原因，建立以数据驱动、可持续优化的闭环流程；
  * 通过 A/B 测试，还可以降低新产品、新功能或新模块的发布风险，为产品的创新发展提供保障。

A/B 测试如此好，到底哪些场景适用呢？从统计学的角度来说，A/B 测试主要是针对当前产品验证哪个方案更好；从产品生命周期来说，A/B 测试主要用来优化迭代
1~100 的产品，而很难用于 0~1 产品的创新发明。

所以，当一个产品刚刚上线或者冷启动，用户数量很少时，是没有必要做 A/B
测试的，如果要进行某些验证，最好的办法就是直接和种子用户进行电话沟通或者面对面沟通，深入了解他们的感受。

乔布斯曾经说过：“消费者并不知道自己需要什么，直到我们拿出自己的产品，他们就会发现，这是他们需要的产品”。从这个意义上来说，A/B
测试还是要回归产品本身，寻找产品的核心价值，清楚用户为什么使用这个产品，以及怎么样可以让用户更快、更乐意体验到产品的核心价值。

### A/B 测试的统计学原理

#### 大数定理

大数定理被称为概率论史上的第一个极限定理，该定理认为，在大量重复的试验中，如果试验条件不变，重复多次的随机事件的频率近似接近它的概率，即偶然中包含着某种必然。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210106142333328.png?x-oss-
process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzMwNjQz,size_16,color_FFFFFF,t_70#pic_center)

这个定理是 A/B 测试能够成立的基础理论，可以认为，当抽样数据越来越大，重复次数越来越多，试验验证的指标的均值就会逐步接近于真实值。

#### 中心极限定理

中心极限定理指：给定足够大的样本量，无论变量在总体中的分布如何，变量均值的抽样分布都将近似于正态分布。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210106142356902.jpg?x-oss-
process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzMwNjQz,size_16,color_FFFFFF,t_70#pic_center)

通俗地说也就是：给定一个任意分布的总体，从这个总体中抽取 n 个样本，随机抽取 m 次，计算这 m
次的样本的平均值，则这些平均值的分布是正态分布，并且这些平均值的均值近似等于总体均值，平均值的方差为总体方差除以 n。

中心极限定理是概率论中最重要的一类定理，它支撑着和置信区间相关的 T 检验和假设检验的计算公式和相关理论。如果没有这个定理，之后的推导公式都是不成立的。

事实上，对于中心极限定理，在不同的场景下都可以对 A/B 测试的指标置信区间判定起到一定作用。

对于属于正态分布的指标数据，我们可以很快捷地对它进行下一步假设检验，并推算出对应的置信区间；而对于那些不属于正态分布的数据，根据中心极限定理，在样本容量很大时，总体参数的抽样分布是趋向于正态分布的，最终都可以依据正态分布的检验公式对它进行下一步分析。

那知道了样本均值总是正态分布之后，我们可以做什么呢？

  * 可以用均值的正态分布来分配置信区间
  * 可以进行 T 检验（即两个样本均值之间是否存在差异） 
  * 可以进行方差分析（即 3 个或更多样本的均值之间是否存在差异）

#### P 值的含义

根据百科定义：p
值是指在一个概率模型中，统计摘要（如两组样本均值差）与实际观测数据相同，或甚至更大这一事件发生的概率。换言之，是检验假设零假设成立或表现更严重的可能性。p
值若与选定显著性水平（0.05 或 0.01）相比更小，则零假设会被否定而不可接受。然而这并不直接表明原假设正确。p
值是一个服从正态分布的随机变量，在实际使用中因样本等各种因素存在不确定性。产生的结果可能会带来争议。

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210106142531841.png?x-oss-
process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzMwNjQz,size_16,color_FFFFFF,t_70#pic_center)

那 P 值多小才足够呢？这取决于你的期望。在许多社会实验领域，比如心理学，任何 P
值＜0.05（5%）都被看作具有统计显著性，即观察到的差异不是随机因素引发的结果。换种说法，即观察到的数据的效应存在
5%的概率是由数据中随机产生的干扰。在其他领域，比如物理学，仅当 P 值＜0.0000003 时才被看作具有统计显著性。

### A/B 测试的流程、

一个完整的 A/B Test 主要包括如下步骤：

  1. 业务分析：根据当前业务，分析优化点和改进点；
  2. 做出假设：根据改进点，做出试验假设，并提出可行性的优化建议；
  3. 梳理指标：梳理出能够衡量版本或者实验组时间差异的指标；
  4. 设计与开发：设计优化版本的原型并完成开发；
  5. 确定测试时长：确定测试进行的时长；
  6. 确定分流方案：确定每个测试版本的分流比例及其他分流细节；
  7. 采集并分析数据：通过埋点等方式，收集实验数据，进行有效性和效果判断；
  8. 给出结论：①确定发布的版本号；②调整分流比例继续测试；③优化迭代方案重新开发，回到步骤 1。

### A/B 测试的案例

下面通过一个项目实例来进行 A/B 测试的实战练习，在这个项目中，以一个电子商务网站运营为测试背景，假设已知设计新网页可以提高用户转化率，试验的目标是通过
A/B 测试来决定是否应该使用新页面，亦或是保留原来网页或延长测试时间进一步来观测。

本次使用的数据集名为“ab_data.csv”，包含用户 ID、时间戳、组别（实验组、对照组）、访问页面的类型、转化情况（0-否，1-是）。

引入包：

    
    
    import pandas as pd
    import numpy as np
    import random
    import matplotlib.pyplot as plt
    %matplotlib inline
    

加载数据：

    
    
    df = pd.read_csv('ab_data.csv')
    df.head()
    

| user_id | timestamp | group | landing_page | converted  
---|---|---|---|---|---  
0 | 851104 | 2017-01-21 22:11:48.556739 | control | old_page | 0  
1 | 804228 | 2017-01-12 08:01:45.159739 | control | old_page | 0  
2 | 661590 | 2017-01-11 16:55:06.154213 | treatment | new_page | 0  
3 | 853541 | 2017-01-08 18:28:03.143765 | treatment | new_page | 0  
4 | 864975 | 2017-01-21 01:52:26.210827 | control | old_page | 1  
  
看看数据量有多大：

    
    
    len(df)
    
    
    
    294478
    

看看有多少个用户：

    
    
    len(df.user_id.unique())
    
    
    
    290584
    

是否存在缺失值：

    
    
    df.isnull().sum()
    
    
    
        user_id         0
        timestamp       0
        group           0
        landing_page    0
        converted       0
        dtype: int64
    

由于存在脏数据， 使得 treatment 与 new_page 不一致，control 与 old_page
不一致的情况，需要把不一致的数据剔除掉，生成一个合格的数据集 df2：

    
    
    diff_idx = df[((df['group'] == 'treatment') == (df['landing_page'] == 'new_page')) == False].index
    df2 = df.drop(diff_idx).reset_index()
    

查看下提出不一致的数据之后，数据量多大：

    
    
    len(df2)
    
    
    
    290585
    

检查 df2 中是否有重复的数据：

    
    
    df2.user_id.nunique()
    
    
    
    290584
    

可以看到 290585-290584 = 1，也就是 df2 中存在一条重复的数据，对 df2 去重。

    
    
    df2 = df2.drop_duplicates(['user_id'])
    

先计算一个用户收到新页面的概率：

    
    
    new_page = round((df2['landing_page'] == 'new_page').mean(),4)
    new_page
    
    
    
    0.5001
    

第一步，计算单个用户的转化率。

    
    
    converted_total = round(df2.converted.mean(),4)
    converted_total
    
    
    
    0.1196
    

第二步，假设一个用户处于 control 组中，计算其转化率。

    
    
    converted_control = round(df2[df2['group'] == 'control'].converted.mean(),4)
    converted_control
    
    
    
    0.1204
    

第三步，假设一个用户处于 treatment 组中，计算其转化率。

    
    
    converted_treatment = round(df2[df2['group'] == 'treatment'].converted.mean(),4)
    converted_treatment
    
    
    
    0.1188
    

从上面的计算，我们可以得出一个结论，在用户收到新旧页面概率相同的情况下，对照组和实验组实际转化率几乎相同（对照组 0.123 略大于实验组
0.120，近似相等）。但是以上数据并不具有显著性证明哪一个页面可以带来更多的转化。

**下面使用 A/B 测试来进行显著性检验。**

注意，由于每个事件都具有时间戳，则可以连续进行每次试验的假设检验。

然而，问题的难点在于，一个页面被认为比另一页页面的效果好得多的时候你就要停止检验吗？还是需要在一定时间内持续发生？你需要将检验运行多长时间来决定哪个页面比另一个页面更好？

第一，现在要考虑的是，需要根据提供的所有数据做出决定。如果假定旧的页面效果更好，除非新的页面在类型 I 错误率为 5%
的情况下才能证明效果更好，那么，你的零假设和备择假设是什么？ 你可以根据单词或旧页面与新页面的转化率 p_old 与 p_new 来陈述你的假设。

零假设：p_old <= p_new，即 p_old - p_new <= 0。

备择假设：p_old > p_new，即 p_old - p_new > 0。

第二，假定在零假设中，不管是新页面还是旧页面，p_old 与 p_new 都具有等于 转化 成功率的“真”成功率，也就是说 p_old 与 p_new
是相等的。此外，假设它们都等于 ab_data.csv 中的转化率，新旧页面都是如此。

每个页面的样本大小要与 ab_data.csv 中的页面大小相同。执行两次页面之间 转化 差异的抽样分布，计算零假设中 10000 次迭代计算的估计值。

在零假设中，p_new 的 convert rate（转化率）是：

    
    
    p_new = round(df2.converted.mean(),4)
    p_new
    
    
    
    0.1196
    

在零假设中，p_old 的 convert rate（转化率）是：

    
    
    p_old = round(df2.converted.mean(),4)
    p_old
    
    
    
    0.1196
    

查看下访问的新页面个数：

    
    
    new_page = len(df2[df2['landing_page'] == 'new_page'])
    new_page
    
    
    
    145310
    

查看下访问的旧页面个数：

    
    
    old_page = len(df2[df2['landing_page'] == 'old_page'])
    old_page
    
    
    
    145274
    

在零假设中，使用 p_new 的转化率模拟新页面 new_page 的转化情况：

    
    
    new_page_converted = np.random.choice(2, size=new_page, p=[1-p_new, p_new])
    

同理，在零假设中，使用 p_old 的转化率模拟旧页面 old_page 的转化情况：

    
    
    old_page_converted = np.random.choice(2, size=old_page, p=[1-p_old, p_old])
    

计算模拟新页面 new_page 的平均转化率与模拟旧页面 old_page 的转化率之间的差异：

    
    
    diff = new_page_converted.mean() - old_page_converted.mean()
    diff
    
    
    
    -5.721105864667231e-05
    

下面，利用理论部分的大数定理和中心极限定理，对零假设中的新旧页面模拟的转化率进行 20000 次的模拟，并将之间的平均差异 diffs 绘制直方图：

    
    
    # 计算过程需要等待几分钟
    diffs = []
    for i in range(10000):
        new_page_converted = np.random.choice(2, size=new_page, p=[1-p_new, p_new])
        old_page_converted = np.random.choice(2, size=old_page, p=[1-p_old, p_old])
        diffs.append(new_page_converted.mean() - old_page_converted.mean())
    
    
    
    # 先计算数据集 ab_data.csv 中对照组和实验组之间的实际转化率之差
    treatment_control_diff = converted_treatment - converted_control
    # 绘制直方图
    plt.hist(diffs)
    plt.axvline(treatment_control_diff, color='red');
    

![在这里插入图片描述](https://img-blog.csdnimg.cn/20210106165443853.png?x-oss-
process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L3FxXzM2MzMwNjQz,size_16,color_FFFFFF,t_70#pic_center)

通过上面的图，我们可以模拟的结果几乎呈正态分布，且大多数值最终结果落在 0 附近。

下面计算 P 值，也就是计算在 diffs 中，大于数据集 ab_data.csv 的实际转化率 treatment_control_diff
的占比是多少？

    
    
    p = (np.array(diffs) > treatment_control_diff).mean()
    p
    
    
    
    0.9128
    

那我们得到结论：在零假设的抽样分布下，新旧页面转化率的差值中，能够取到 treatment_control_diff
的可能性（即零假设为真，观察到统计量的概率）为 91%，远大于我们设定的一类错误阈值 5%。

也就是说，单纯的进行新旧页面变化，转化率并没有出现明显的提升。

除此之外，还可以考虑使用回归分析进行验证，还可以考虑地区差异等。

### 总结

本篇简单介绍了 A/B 测试使用到的主要数学理论和基本测试流程，最后通过一个开源案例介绍了 A/B 测试的应用，然而 A/B
测试的相关内容还有很多，核心要掌握假设检验，其次，推荐学习参考资料的课程，并通过实践来进行练习。

参考资料：

  * [源代码](https://github.com/GitBookCn/soyoger/tree/main/codes/%E4%B8%9A%E5%8A%A1%E4%B8%93%E9%A2%98%E7%AF%87%EF%BC%9AAB%E6%B5%8B%E8%AF%95%E5%AE%9E%E9%AA%8C%E8%AE%BE%E8%AE%A1%E4%B8%8E%E8%AF%84%E4%BC%B0)
  * [Udacity A/B Testing](https://www.udacity.com/course/ab-testing--ud257)

