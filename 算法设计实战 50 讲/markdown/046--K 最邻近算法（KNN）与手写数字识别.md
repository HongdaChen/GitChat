> K 最近邻（KNN，K-Nearest
> Neighbor）算法是一类在数据挖掘领域常用的分类算法，该算法看似神秘，其实原理很简单，算法实现也很简单。KNN
> 算法在类别决策时，只参考极少量的相邻样本，也就是说主要靠周围有限的邻近样本，而不是靠判别类域的方法来确定所属类别，因此对于类域的交叉或重叠较多的待分样本集来说，KNN
> 方法较其他方法更为适合。这一课我们用 KNN 算法来实现一个简单的手写数字识别程序，看看到底有多简单。

### KNN 算法原理

K 最邻近算法（KNN）是一个理论上比较成熟的方法，也是最简单的机器学习算法之一，所谓 K 最邻近（K-Nearest Neighbor），意思是 K
个最相近的邻居。KNN 算法的核心思想是每个样本都可以用与它在特征空间中最接近的 K 个邻居来代表，如果这 K
个最相邻的邻居样本大多数属于某一个类别，那么该样本也属于这个类别。

![enter image description
here](https://images.gitbook.cn/3bedf080-0fd9-11e9-97a1-b5c0cb989076)

图（1）KNN 算法的决策示意图

图（1）是用来解释 KNN 的典型示意图，图中的一个实线圆和一个虚线圆表示 K 的范围。当范围比较小时（实线圆范围，K = 3），绿色小圆最邻近的 3
个样本中有 2 个属于红色类别，1 个属于蓝色类别，根据 KNN 的理论，此时决策绿色小圆表示的样本应该属于红色类别。但是若令 K =
5，范围扩大到虚线圆的范围时，此时绿色小圆最邻近的样本有 5 个，3 个属于蓝色类别，2 个属于红色类别，此时决策绿色小圆表示的样本应该属于蓝色类别。

这个示意图解释了两个问题，一个是给未知样本分类的规则，另一个是 KNN 算法的一个关键参数 K 的意义。K 个最近的邻居，那么这个 K
怎么选合适，也是一个问题。

#### 算法工作原理

KNN
算法原理很简单，但是要让算法工作起来，还需要解决最关键的模型问题，首先是训练样本如何表达，其次是样本之间的距离怎么表达。对于算法问题，只要一提到距离，大家首先应该想到的就是向量，将样本数据转化为向量是此类问题常用的思想，比如计算文本相似度的余弦定理，就是对文本分词，将文本数据组织成词的向量。对于
KNN 算法来说也不例外，只要想办法将样本数据转化成向量，第二个问题就迎刃而解了，因为计算向量距离的方法太多了，比如曼哈顿距离、欧式几何距离等。

将问题的样本数据转化为向量的方法有很多，因问题的不同而异，如果样本数据有很多个属性，那这些属性就是天然的向量维度，直接将这些属性对应到向量的各个维度上就可以了。如果样本的信息无明显分组，可考虑用一些特殊的方法将信息离散化，然后抽取关键信息点组成向量的各个维度，最后再将样本数据对齐到向量的各个维度上。对文本数据进行分词，就是常见的向量化处理方法。

在介绍 A * 算法的时候，已经说过曼哈顿距离和欧氏几何距离了，这里回顾一下这两个距离的计算公式。对于任意 $n$ 维向量 $p$ 和
$q$，欧氏几何距离可以表示为：

$$ D _{Euclidean}(p,q) = \sqrt{\sum_ {i = 1}^{n}(px _{i} - qx_ {i})^{2}} $$

同样，曼哈顿距离可以表示为：

$$ D _{Manhattan}(p,q) = \sum_ {i = 1}^{n}\left |px _{i} - qx_ {i}\right | $$

解决了数据建模问题和距离计算方法问题，KNN 算法的实现就非常简单了，因不同人的实现而异，但大致都有这几个步骤：

  * 准备好已经分好类的训练数据，以合理的方式组织这些数据
  * 计算测试数据与各个训练数据之间的距离
  * 按照距离的递增关系进行排序，然后取距离最小的前 K 个数据
  * 统计前 K 个数据所属的类别出现的频率，取出现频率最高的那个分类作为测试数据的预测分类

#### 来个例子，增加点感性认识

某网站要对本站的文章按照科技文章和人文文章进行分类，进而统计两类典型词汇，一类是科技类文章典型词汇，如公式、定理、程序、证明、参数、算法等，另一类是人文类词汇，如家庭、爱情、照顾、婚姻等。工作人员事先整理了一些科技类文章和人文类文章，得到了以下统计样本：

文章 | 科技词汇出现次数 | 人文词汇出现次数 | 分类  
---|---|---|---  
人肉吹气推进技术在宇航科学中的应用前景 | 83 | 9 | 科技文章  
当抓把洲爱上欻捏苏 | 2 | 122 | 人文文章  
论豆浆和咖啡的拓扑一致性 | 196 | 9 | 科技文章  
利用引力波技术降低离婚率的研究报告 | 165 | 23 | 科技文章  
横跨 800 米的异地恋 | 6 | 151 | 人文文章  
将 2 进行到底 | 15 | 137 | 人文文章  
  
现有一篇文章，其中科技词汇出现了 46 次，人文词汇出现了 18 次，这篇文章应该归为哪一类呢？

首先可以看出来，这个问题可以根据样本的属性进行向量化，每个样本有 2
个属性，分别是科技词汇出现次数和人文词汇出现次数；接着是用曼哈顿距离分别计算这篇文章与以上 6 篇文章的距离，按照从小到大的顺序排列如下：

分类 | 文章 | 与未知文章的距离  
---|---|---  
科技文章 | 人肉吹气推进技术在宇航科学中的应用前景 | 46  
科技文章 | 利用引力波技术降低离婚率的研究报告 | 124  
人文文章 | 当抓把洲爱上欻捏苏 | 148  
人文文章 | 将 2 进行到底 | 150  
科技文章 | 论豆浆和咖啡的拓扑一致性 | 162  
人文文章 | 横跨 800 米的异地恋 | 173  
  
假设 K = 3，我们按照顺序取前三个数据，有两个科技文章分类、一个人文文章分类，因此这篇文章被分类为科技文章。

### 手写数字识别程序设计

跟我们课程中的所有例子一样，这里做个 KNN
算法的演示程序，主要目的是演示算法的原理和实现，供研究算法之用。程序首先读取训练数据集合中的文件数据，转化成向量，然后用测试数据集合中的数据逐个进行测试（测试数据也是事先分好类的），以检查分类算法识别的准确性，经过验证，发现识别率还是很高的。当设
K = 13 时，如图（2）所示，只有数字 8 的识别正确率是 90%，其他数字的识别正确率都超过了 95%，还行。

![enter image description
here](https://images.gitbook.cn/505505e0-0fd9-11e9-b42e-390d5c3452ad)

图（2）演示程序的识别率结果

#### 数字文件的格式

一般来说，这种数字识别程序都是直接对位图文件进行处理和识别的，基本思路就是将彩色的图像文件转化为黑白二值图像（1 表示黑色，0
表示白色，或者反过来表示），然后再读取图像文件中的点阵信息，将每个点的信息转换到设计好的数据模型中。为了简化演示程序的实现，让大家把注意力集中在 KNN
算法上，我们的演示程序没有直接处理图像，而是使用了已经整理好的测试数据，这套测试数据在很多地方都有人用，32 × 32 的图像文件被转化成 32 行，每行
32 列的文本格式，用字符 0 和 1 代表图像上的白点和黑点，下面就是数字 8 的文件内容：

    
    
    00000000111000011111100000000000
    00000001111001111111111100000000
    00000001111111111111111100000000
    00000000111111111111111110000000
    00000000111111111111111111000000
    00000001111111110000001111000000
    00000011111111110000001111000000
    00000001111111100000001110000000
    00000001111110000000011110000000
    00000001111110000000111110000000
    00000000111110000000111100000000
    00000000111111000011111000000000
    00000000111111000011111000000000
    00000000011111001111110000000000
    00000000000111111111100000000000
    00000000000111111111000000000000
    00000000000111111111000000000000
    00000000000011111110000000000000
    00000000000011111100000000000000
    00000000000111111100000000000000
    00000000001111111110000000000000
    00000000001111111110000000000000
    00000000011111011110000000000000
    00000000111110011110000000000000
    00000000111100001110000000000000
    00000000111000001111000000000000
    00000001111000001111000000000000
    00000001111000001111000000000000
    00000001111111111110000000000000
    00000001111111111110000000000000
    00000000111111111111000000000000
    00000000001111100000000000000000
    

这种转化后的文本格式处理起来非常方便，直接用字符串处理方式逐行读入文件即可。网上的这套测试数据分两部分，一部分是训练数据，在 traindata
目录中，另一部分是测试数据，在 testdata 目录中，文件名有规律，比如 0_113.txt，就表示数字 0 的第 113
个测试文件。从文件名可以知道这个数据文件的分类，程序处理的时候直接根据文件名确认这个文件中的数据应该分类为 2 还是 5 就可以了，很方便。

#### 样本和数据集的处理

首先考虑怎么将文件中的数据转化成向量形式，既然每个数字的文件信息有 32 × 32 个点信息，我们可以考虑用一个 1024 （32 × 32 =
1024）维的向量来表示每个数字文件，转化的方式也很简单，就是将 32 行的 0 和 1 依次拼接起来，就得到一个 1024 维的向量。

SampleVec 是每个样本数据的数据模型，cat 是这个样本的已知分类，用 0 ~ 9 数字分别表示识别的数字分类（非巧合，也是 0 ~ 9）。vec
是向量，因为数据就是 0 和 1，因此就用 char 类型，减少一点内存使用。

    
    
    const int BMP_WIDTH = 32;
    const int BMP_HEIGHT = 32;
    
    typedef struct
    {
        int cat;
        char vec[BMP_WIDTH * BMP_HEIGHT];
    }SampleVec;
    

将文件读入并转化成 SampleVec 的算法很简单，就是逐行读入文本格式的 0 和 1 字符串，然后转化成数字 0 和 1，依次拼接到 vec
中。注意文本格式的数字 0 ~ 9 对应的 ASCII 码是 0x30 ~ 0x39，要转成数字 0 ~
9，需要做个转换，转换的方法在前面基础部分已经介绍过了，这里再看一次吧，就是这一行：sline[i] - '0'。

    
    
    bool AppendToVec(SampleVec& vec, int row, std::string& sline)
    {
        if (sline.length() != BMP_WIDTH)
        {
            return false;
        }
    
        char *pvs = vec.vec + row * BMP_WIDTH;
        for (std::size_t i = 0; i < sline.length(); i++)
        {
            *pvs++ = sline[i] - '0';
        }
    
        return true;
    }
    
    bool LoadFileToVec(const std::string fileName, SampleVec& vec)
    {
        std::ifstream file(fileName, std::ios::in);
        if (!file)
        {
            return false;
        }
        int row = 0;
        std::string sline;
        while (std::getline(file, sline))
        {
            if (!AppendToVec(vec, row, sline))
            {
                break;
            }
            row++;
        }
    
        return (row == BMP_HEIGHT);
    }
    

枚举目录中的数据文件，逐个转化成 SampleVec，并用 std::vector 组织数据集合，filePath
参数是测试文件的路径，训练数据和测试数据都可以用 LoadDataSet() 函数加载。遍历文件我用了 std::filesystem，这是 C++ 17
新增的内容，不过 VS 2015 不支持 C++ 17，但是它好心地提供了一个体验版，我用了一个别名：

    
    
    namespace std_fs = std::experimental::filesystem;
    

如果你的编译器支持 C++ 17，可以这样改一下：

    
    
    namespace std_fs = std::filesystem;
    

C 语言的话，要用 findfirst 和 findnext，其实也不麻烦，看看下面的例子就明白了：

    
    
        finddata_t fd;
        int pf = findfirst("e:/*.*",&fd);
        while (!findnext(pf,&fd))
        {
            printf("%s\n",fd.name);
        }
        findclose(pf);
    

GetCategoryFromFileName() 函数从文件名中提取数字分类，就是找到 _，然后前一个字符就是分类，转成数字直接使用即可。

    
    
    std::pair<bool, int> GetCategoryFromFileName(const std::string fileName)
    {
        std::size_t pos = fileName.find('_');
        if (pos == std::string::npos)
        {
            return {false, 0};
        }
    
        int cat = fileName[pos - 1] - '0';
    
        return { true, cat };
    }
    
    bool LoadDataSet(const std::string filePath, std::vector<SampleVec>& dataSet)
    {
        for (auto& p : std_fs::directory_iterator(filePath))
        {
            std::string fileName = p.path().generic_string();
            std::pair<bool, int> catrtn = GetCategoryFromFileName(fileName);
            if (!catrtn.first)
            {
                return false;
            }
    
            SampleVec vec = { catrtn.second };
            if (!LoadFileToVec(fileName, vec))
            {
                return false;
            }
    
            dataSet.emplace_back(vec); //better than push_back()
        }
    
        return true;
    }
    

#### 训练和测试数据

这里就是 KNN 算法的核心了，我很想写详细一点，但是真的没什么可写的，因为这个算法太简单了。前面那些代码都是做些准备工作，这个 Classify()
函数才是 KNN 算法的核心，但是，也就是十几行代码而已，dataTrain 是从训练数据目录中加载的训练数据集，vec 是待分类的样本。第一个 for
循环计算 vec 与所有训练数据的曼哈顿距离，存到 CatResult 数组 cr 中，cr 数组初始化为长度和 dataTrain
一样，因为它们确实一样长，每个训练数据都要算一个曼哈顿距离。

计算完成后，对 cr 数组按照距离从小到大排序，std::sort() 用 lessCrPred 给出的比较方式对这个数据进行了排序，接下来是从 cr
数组中取前 k 个数据，并统计这 k
个数据中每种分类出现的次数。这里又用到了数组下标的技巧，希望大家能看出来。最后，GetMaxCountCategory() 函数遍历一下 count
数组，从中找出出现次数最多的分类，并将这个结果作为 Classify() 函数的分类结果。

    
    
    typedef struct
    {
        double distance;
        int cat;
    }CatResult;
    
    int Classify(const std::vector<SampleVec>& dataTrain, const SampleVec& vec)
    {
        int idx = 0;
        std::vector<CatResult> cr(dataTrain.size());
        for (auto& vt : dataTrain)
        {
            cr[idx].cat = vt.cat;
            cr[idx++].distance = ManhattanDustance(vt, vec);
        }
    
        auto lessCrPred = [](const CatResult& cr1, const CatResult& cr2)->bool { return (cr1.distance < cr2.distance); };
        std::sort(cr.begin(), cr.end(), lessCrPred); 
    
        std::vector<int> count(NUM_COUNT, 0); 
        for (int i = 0; i < K; i++) 
        {
            count[cr[i].cat]++;
        }
    
        return GetMaxCountCategory(count);
    }
    

图（2）显示的输出结果，是用的测试数据集，对测试数据集中的每个样本依次进行 Classify() 分类，并统计结果得到的正确率数据。

### 总结

KNN 算法的优点是简单、易于实现，样本数据只需要整理分类，不需要训练。分类精度高，对样本中的异常值不敏感（异常数据样本在排序后进不到前 K
个决策样本中），对于多分类问题（一个样本可以属于多个分类），KNN 也有很好的适应性。

KNN
算法的缺点是测试样本分类时计算量大（需要和所有训练样本都计算一下距离），样本空间开销大。当样本数量不平衡时，比如某类样本容量很大，其他类样本容量很小的时候，前
K 个样本邻居中该类样本数量就会占多数，样本容量小的分类样本很难进入前 K 个样本。

最后，大名鼎鼎的 K 最近邻（KNN）算法，其实就是将样本转化成向量，然后计算距离，就是这么简单。

[请单击这里下载源码](https://github.com/inte2000/play_with_algo)

### 答疑与交流

> **为了方便与作者交流与学习，GitChat 编辑团队组织了一个《算法应该怎么玩》读者交流群，添加小助手-
> 伽利略微信：「GitChatty6」，回复关键字「259」给小助手-伽利略获取入群资格。**

