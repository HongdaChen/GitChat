> 在以各种“XX学习”为代表的人工智能技术普及之前，游戏里常见的角色 AI
> 都是各种预设的行为逻辑，比如博弈树和行为树，当然也会用到各种专家知识库。当这些预设的行为逻辑足够复杂的时候，往往会让游戏玩家觉得游戏里的人物很“智能”。从本质上来说，这些都还不算是真正的
> AI，但也能够给游戏的体验增加很多乐趣，这一课我们就来介绍三种常见的给角色预设行为逻辑的方法，分别是决策树、博弈树和行为树。

### 决策树（Decision Tree）

在介绍决策树之前，先说一下分类算法，它是机器学习领域里的基本算法之一，常见的分类算法有贝叶斯分类算法、KNN
算法、逻辑回归算法、神经网络算法等，当然，还有各种深度学习算法。决策树是一种简单但广泛使用的分类器，因此，决策树也是一种分类算法。

#### 决策树长什么样

决策树易于理解，通过解释后就能知道决策树所表达的意义了。决策树的每个内部节点表示在一个属性上的测试，每个分支代表该测试的一个输出，而每个树叶结点则代表一个分类标记，所谓的分类标记其实就是分类结果，比如“Yes”或“No”。

![](https://images.gitbook.cn/21c89660-01ab-11e9-b0f4-ff31f4e5691e)

图（1）相亲决策示意图

图（1）就是一个“疑似”决策树的示意图，这是一个姑娘的相亲决策。之所以用“疑似”来形容，是因为这个决策树上的判断条件都太主观、太抽象、没有量化。什么意思呢？比如说“年轻”这个条件，多少岁算年轻，多少岁算年老呢？再比如“长得帅”，什么样的才算帅啊？如果这个姑娘是个“颜控”，她就可以对每个属性给出量化打分，如果决策超过
20 分就见面，结果如图（2）所示：

![](https://images.gitbook.cn/351d2a50-01ab-11e9-b0f4-ff31f4e5691e)

图（2）带量化打分的决策树

#### 构造决策树

和学习其他算法原理一样，决策树也是先通过训练数据构建决策树，然后对未知的数据进行分类，前一个过程被称为学习或训练过程，后一个过程被称为测试过程。一般决策树的构建，都是需要将原始数据量化，然后按照特征（属性）分类，然后再用大量的数据“学习”，确定每个分支选择的概率（或者打分），最终再根据某个指定的阈值，对计算出来的概率（或分值）结果做“是”或“否”的裁决。

![](https://images.gitbook.cn/4577df30-01ab-11e9-8607-cb1ed2f849ca)

图（3）某杂志社做的读者问卷调查结果

图（3）是某杂志社做的一个读者问卷调查（我杜撰的，无任何实际意义），调查了各个年龄段和职业的人购买本社杂志的意愿，目的是今后的广告能够根据不同读者的购买愿望有的放矢。这里面，年龄、职业、收入和居住地就是我们要决策的特征，表格中的每一条记录就是一个样本，最后的结论买或不买就是分类结果。

构造决策树其实就是决策树学习的一个过程，学习算法的过程通常是：逐个选择最优特征（属性），并根据该特征对训练数据进行分割，使得各个数据子集有一个最好的分类。这一过程对应着特征空间的划分，也对应着决策树的构建，通常这个过程也是递归的。决策树的构造算法大致可分为四个步骤：

（1）开始构建根节点，将所有训练数据都放在根节点上，然后开始第（2）步；

（2）从节点中（数据子集和中）选择一个最优特征，按照这一特征将这个节点中的训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类；

（3）对第（2）步得到的子集，也就是子节点进行判断，如果这些子集已经能够被基本正确分类，那么停止分类过程，转到第（4）步，否则，转到第（2）步，继续分割子集；

（4）构建叶节点，并将这些子集分到所对应的叶节点去，其输出的结果为该节点数量占比最大的类别。

从上述四个步骤的描述中可以看出，要实现决策树算法需要解决三个问题，即如何选择最优特征（属性）、数据子集的分割方法、停止分类的判断方法。在解决这三个问题之前，我们先要了解一下如何对集合信息进行度量，度量的方法就是信息量或信息增益，信息增益是什么概念呢？

举个例子，某位小伙伴跑来跟你说：老板通知，下午两点在 906
会议室开会。这里面有两个信息，一个是时间、一个是地点，假如你之前完全不知道这个事情，那么这位小伙伴给你的信息增益就是
100%；假如你事先知道了开会时间，只是不知道地点，那么这个信息对你的信息增益就是
50%；假如你之前就已经知道了开会时间和地点，那么这个信息对你的信息增益就是 0。

对信息量的计算，常采用香农公式，或香农熵（简称熵），其中分类规则的数目，$p _{i}$ 是特征 $x_ {i}$ 发生的概率，H(X) 为集合 X
的熵。熵就是事物（样本数据）混乱的程度，或者是将无序数据变得更加有序所要付出的代价；熵值越大，说明决策的不确定性越大，我们分类的目的就是为了降低这种不确定性。

$$ H(X) = -\sum _{i=1}^{n}p_ {i}log(p_{i}) $$

以图（3）表中数据为例，在已知的 10 条样本数据组成的数据集合中，分类特征就是两个：买或不买，买杂志的概率是 6/10=0.6，不买的概率是
4/10=0.4。利用上述公式计算出集合 X 的熵值是（取 10 为底的对数）：

![avatar](https://images.gitbook.cn/FgwuKTX1WH94E154Sw819xDTnLw2)

当我们以某个特征为分割子集的依据时，就需要计算条件熵：

$$ H(Y|X)=\sum _{i=1}^{n}p_ {i}H(Y|X=x_{i}) $$

条件熵与条件概率公式类似，条件概率 P(Y|X) 的意义是在已知条件 $X=x _{i}$ 的情况下 Y 事件发生的概率。条件熵 H(Y|X)
表示在已知条件 $X=x_ {i}$ 的情况下，Y 的条件熵的数学期望，其中 $p _{i}=P(X=x_ {i})$。

假如要按照年龄进行分类，设年龄为条件 A，则条件熵为 H(X|A)，A 的信息增益就是 G(X, A) = H(X|A)−H(X)；同样，设职业为
B，可以计算出 B 的信息增益就是 G(X, B) =
H(X|B)−H(X)。以此类推，分别计算出四个特征的信息增益，选增益最大的那个作为分割子集和的条件。

由此可见，选择最优特征和分割子集是同时完成的，可用来计算上述增益的常用算法有 D3 算法、C4.5 算法、CART
算法等。那么剩下的问题就是停止分裂的判断方法，这个判断方法有很多，比如可以根据子集合样本数量判断，当分割的子集和样本数量少于某个值的时候就停止分裂；还可以根据特定结果决定何时停止分裂，比如检测到某个分类效果已经足够好了，就可以停止分裂。当然，出于系统资源的考虑，也不希望决策树无限制增长，因此当树的深度达到事先设定的最大深度时，也要停止分裂。总之，条件是自己定的，根据具体问题具体分析。

#### 特征和分类的量化

语言描述我们可以用 A 条件或 B
特征来解释算法，但是编写算法代码的时候需要对这些文字描述进行信息化，这也是用程序解决算法问题的基础。量化的方法很多，对于本问题，我们可以用 1 表示青年，2
表示中年，3 表示老年；用 1 表示工人，2 表示工程师，3表示干部；1 表示收入低，2 表示收入中等，3 表示收入高；1 表示北京，2 表示上海，3
表示深圳；最后，1 表示买，0 表示不买。这样量化后每条特征数据就可以表示为 5 元组：{3, 1, 2, 2, 0}，就可以用计算机表达和计算了。

### 博弈树

“博弈”可以理解为有限参与者进行有限策略选择的竞争性活动，如下棋、打牌、竞技、战争等。根据参与者种类和策略选择的方式可以将博弈分成很多种，为了方便对博弈树的理解，我们选择一种最简单的博弈来研究。选择的博弈具有二人零和、全信息和非偶然三个特点，这与棋类游戏的决策相似，也就是常说的“零和博弈（Zero-
sum Game）”。这样的博弈通常有三个特点：

  * 博弈的双方 MAX 和 MIN 轮流采取行动，博弈的结果要么 MAX 胜、要么 MIN 胜、要么平局、不存在共赢的情况；
  * 在博弈的过程中，任何一方都了解当前的格局，信息对双方都是公开的；
  * 参与博弈的双方在行动前都要根据当前的状态，进行得失分析，选取对自己最为有利而对对方最为不利的对策，参与博弈的双方的决策都是“理智”的行为，不存在失误和碰运气的情况。

在博弈过程中，任何一方都希望自己取得胜利，如果 MAX 当前有多个行动方案可供选择时，他总是挑选对自己最为有利同时对对方最为不利的那个行动方案；反之，对
MIN 来说也是同样的策略。假如我们站在 MAX 的立场上，会发现 MAX 所选择的行动方案之间是“或”关系，因为选择权在 MAX
手中，他可以选择这个行动方案，也可以选择另一个行动方案。但是，如果 MIN 也有若干个可供选择的行动方案，那么对 MAX 来说，MIN
的这些行动方案之间是“与”关系，MIN 可以选择这些方案中的任何一个，MAX 必须考虑对自己最不利的情况。

#### 博弈树的搜索

博弈树是一棵“与／或”树，它不同于普通的选择树，普通的选择树就是选择项谁都可以选。博弈树的选择，是站在不同的立场上的选择，MAX 选还是 MIN
选，结果是不一样的。对于大多数的博弈，我们都可以构成一个博弈树，以最常见的棋类游戏为例，博弈树的节点对应于某一个棋局，其分支表示走一步棋。博弈树的根节点对应于开始位置，其叶子节点表示博弈到此结束，结果已定（输、赢或平局）。参与博弈的双方在对抗或博弈的过程中会遇到各种状态和移动（也可能是棋子落子）的选择，博弈双方交替选择，每一次选择都会产生一个新的棋局状态。假设两个棋手（可能是两个人，也可能是两台计算机）MAX
和 MIN 正在一个棋盘上进行博弈，MAX 每选择一个决策方案就会触发产生一个新状态，MIN
也同样，最终这些状态就会形成一个状态树。由此可见，博弈树一般具有以下特点：

  * 博弈的初始棋局是博弈树的根节点；
  * 在博弈树中，“或”节点和“与”节点是逐层交替出现的，自己一方扩展的节点之前是“或”关系，对方扩展节点之前是“与”关系，双方轮流地扩展节点。

博弈树的根就是搜索开始时的棋盘状态，每一个子节点就是 MAX 的每一种决策方案可能产生的棋盘状态（局面），而这些子节点的子节点则是 MIN
的每一种决策方案可能产生的棋盘状态（各层相互间隔）。这棵树的叶子节点就是最终结局，结果无非三种：MAX 胜利、MIN 胜利或者平局。

##### **“极大极小值”搜索算法**

“极大极小值（Min-Max）”搜索算法是各种博弈树搜索算法中最基础的搜索算法。假如 MAX 和 MIN 两个人在下棋，MAX
会对所有自己可能的落子后产生的局面进行评估，选择评估值最大的局面作为自己落子的选择。这时候就该 MIN 落子，MIN
当然也会选择对自己最有利的局面，这就是双方的博弈，即总是选择最小化对手的最大利益（令对手的最大利益最小化）的落子方法。作为一种博弈搜索算法，“极大极小值”搜索算法的名字就由此而来。

从下棋的角度考虑是 MAX 和 MIN 双方轮流落子，但是从搜索算法的角度考虑，只能以其中一方为基准进行搜索。接下来就站在 MAX
的立场上分析一下“极大极小值”搜索算法的搜索过程。

首先，“极大极小值”搜索也将得到一棵博弈树，被称为“极大极小博弈树（Minimax Game Tree）”，这棵树的根（第 0 层）是搜索的开始状态。树的第
1 层节点是 MAX 的选择节点，这一层的节点 MAX 将选择对自己最有利的评估最大值，称为极大值节点；树的第 2 层节点是 MIN 选择节点，这一层的节点
MAX 将选择对自己最不利的评估最小值，因为这一层是对 MIN 落子后的局面进行评估，站在 MIN
的立场进行选择，所以这一层的节点又被称为极小值节点。极大值节点和极小值节点交错出现在每一层，直到最后一层的叶子节点对棋局进行评估。

![](https://images.gitbook.cn/59cf8b40-01ab-11e9-98b9-ed4f08a257ed)

图（4）井字棋游戏“极大极小博弈树”示意图

图（4）是简单的井字棋游戏的“极大极小博弈树”的一部分，第 1 层是极大值节点，三种落子位置得到的评估值分别是 −1、1 和 −2，MAX
会选择评估值最大的节点，也就是落子在中间位置的局面，这个局面的估值是 1。那么这一层的评估值是怎么得到的呢？那就是根据第 2 层的评估值进行选择，第 2
层是极小值节点，MAX 会选择对自己最不利的局面，也就是说，MAX 对每个分支都会选择评估最小的值作为第 1 层节点的估值。对于第一个分支，MAX 选择
−1作为评估值，对于第二个分支，MAX 选择 1 作为评估值，对于第三个分支，MAX 选择 −2 作为评估值，这就是第 1 层三个局面评估值的由来。

根据以上分析，我们可以给出“极大极小值”搜索算法的伪代码：

    
    
    int MiniMax(node, depth, isMaxPlayer)
    {
        if(depth == 0)
        {
            return Evaluate(node);
        }
    
        int score = isMaxPlayer ? -INFINITY : INFINITY;
        for_each(node的子节点child_node)
        {
            int value = MiniMax(child_node, depth - 1, !isMaxPlayer);
            if(isMaxPlayer)
                score = max(score, value);
            else
                score = min(score, value);
        }
    }
    

##### **“负极大极”搜索算法**

博弈树的搜索是一个递归的过程，“极大极小值”算法在递归搜索的过程中需要在每一步区分当前评估的是极大值节点还是极小值节点。1975 年 Knuth 和
Moore 提出了一种消除 MAX 结点和 MIN 结点区别的简化的“极大极小值”算法，称为“负极大值（Negamax）”算法，该算法的理论基础是：

max(a,b) = −min(−a, −b)

简单地将递归函数 MiniMax() 返回值取负再返回，就可以将所有的 MIN 结点都转化为 MAX
结点，对每个结点的搜索都尝试让结点值最大，这样就将每一步递归搜索过程都统一起来。

根据以上分析，我们可以给出“负极大值”算法的伪代码，其中 color 参数相当于传递了一个符号位：

    
    
    int NegaMax(node, depth, color)
    {
        if(depth == 0)
        {
            return color * Evaluate(node);
        }
    
        int score = -INFINITY;
        for_each(node的子节点child_node)
        {
            int value = -NegaMax(child_node, depth - 1, -color);
            score = max(score, value);
        }
    }
    

##### **“α-β”剪枝**

博弈树搜索算法很简单，但是需要搜索的状态是相当多的。以简单的井字棋（Tic—Tac—Toe）游戏为例，当设定搜索深度是 6
时，不带任何优化的“极大极小值”搜索算法确定第一个落子时需要搜索 56160
个状态；如果是五子棋或围棋这样的复杂棋类游戏，搜索的状态数会是天文数字，因此需要一些优化方法对简单搜索算法进行优化。“剪枝”是搜索算法中常见的优化方法，通过减除一些明显不可能得到正确解的状态，避免对这些状态的搜索，可以提高搜索算法的效率。本节我们将介绍一种可应用于“极大极小值”算法和“负极大值”算法的剪枝算法——“α-β”剪枝算法（Alpha-
beta Pruning）。

“α-β”剪枝算法维护了一个搜索的极大极小值窗口：[α,β]，其中，α 表示在搜索进行到当前状态时，博弈的 MAX 一方所追寻的最大值中最小的那个值（也就是
MAX 最坏的情况）。在每一步的搜索中，如果 MAX 所获得的极大值中最小的那个值比 α 大，则更新 α 值（用这个最小值代替 α），也就是提高 α
这个下限。而 β 表示在搜索进行到当前状态时，博弈的 MIN 一方的最小值中最大的那个值（也就是 MIN 的最坏的情况）。

在每一步的搜索中，如果 MIN 所获得的极小值中最大的那个值比 β 小，则更新 β 值（用这个最大值代替 β），也就是降低 β 这个上限。当某个节点的
α≥β 时，说明该节点的所有子节点的评估值既不会对 MAX 更有利，也不会对 MIN 更有利，也就是对 MAX 和 MIN
的选择不会产生任何影响，因此就没有必要再搜索这个节点以及其所有子节点了。

“α-β”剪枝算法实际上是两个过程，分别是极小值节点的“α 剪枝”和极大值节点的“β 剪枝”，接下来用两幅图分别说明一下这两个剪枝过程的原理。图（5）是“α
剪枝”过程示意图。极大值节点 A 搜索博弈树时会从两个极小值节点 B 和 C 中选择评估值最大的一个节点，而 B 和 C
节点则会从自己的子节点中（极大值节点）选择评估值最小的一个节点。假设已经对 B 节点完成了搜索，B 的四个子节点 D、E、F、G 中最小值是 2，则可知 B
节点的准确估值是 2，此时更新 α 的值为 2。

接下来开始搜索 C 节点的子节点 H、I 和 J，如果 H 节点的估值是 1，则说明 C 节点的评估值一定不会超过 1（因为 C 总是选择 H、I、J
节点中的最小值），也就是说，C 节点的评估值一定不会比 B 节点的评估值更大，此时就可以终止对 C 节点的搜索，此过程就称为“α 剪枝”。

![](https://images.gitbook.cn/6cbafd20-01ab-11e9-b8c2-a76e92740bf5)

图（5）“α 剪枝”过程示意图

图（6）是“β 剪枝”过程示意图。极小值节点 A 搜索博弈树时会从两个极大值节点 B 和 C 中选择评估值最小的一个节点，而 B 和 C
节点则会从自己的子节点（极小值节点）中选择评估值最大的一个节点。假设已经对 B 节点完成了搜索，B 的四个子节点 D、E、F、G 中最大值是 8，则可知 B
节点的准确估值是 8，此时更新 β 的值为 8。现在开始搜索 C 节点的子节点 H、I 和 J，如果 H 节点的估值是 10，则 C 节点的值一定不会小于
10（因为 C 总是选择 H、I、J 节点中的最大值），也就是说，C 节点的值一定不会比 B 节点更小，因此可以终止 C 节点的搜索，此过程就称为“β
剪枝”。

![](https://images.gitbook.cn/7c4ae7f0-01ab-11e9-98b9-ed4f08a257ed)

图（6）“β 剪枝”过程示意图

这就是“α-β”剪枝算法的原理，搜索开始时，可设定 α= − ∞，β = + ∞，在搜索过程中，这个范围会逐步收窄，直到出现 α≥β
的剪枝条件。下面就给出基于“极大极小值”算法的“α-β”剪枝算法的伪代码：

    
    
    int MiniMax_AlphaBeta(node, depth, α, β, isMaxPlayer)
    {
        if(depth == 0)
        {
            return Evaluate(node);
        }
    
        if(isMaxPlayer)
        {
            for_each(node的子节点child_node)
            {
                int value = MiniMax_AlphaBeta(child_node, depth - 1, α, β, FALSE);
                α= max(α, value);
                if(α >= β) /*β 剪枝*/
                    break;
            }
    
            return α;
        }
        else
        {
            for_each(node的子节点child_node)
            {
                int value = MiniMax_AlphaBeta(child_node, depth - 1, α, β, TRUE);
                β= min(β, value);
                if(α >= β) /*α 剪枝*/
                    break;
            }
    
            return β;
        }
    }
    

#### 估值算法

博弈树原理和实现都很简单，但是对于大多数棋类游戏来说，建立完整的博弈树，从根节点到叶子节点完整地搜索博弈树是不现实的。以中国象棋为例，建立一棵双方各走 50
步的博弈树需要生成大约 $10^{160}$ 个节点，即使处理一个节点只需要 $10^{-8}$ 秒，要处理这棵树也需要 $10^{140}$
年以上。因此，复杂棋类游戏的搜索算法通常都需要指定一个搜索深度，当达到搜索深度时就直接评估棋局，在时间和准确度之间做一个折中。

评估棋局的原理就是用某种算法将棋局量化为一个可比较的符号，通过比较确定最优的局面选择，这个量化算法就是估值函数。棋局的量化需要考虑很多因素，量化结果是这些因素按照各种权重组合的结果，这些因素通常包括棋子的战力（棋力）、双方棋子占领的空间、落子的机动性、威胁性（能吃掉对方的棋子）、形和势等。不同的棋类游戏会根据规则选择合适的参考因素与权重关系，组合出一个量化的评估结果。

对于博弈树搜索算法来说，其“智力”的高低基本上是由估值函数（评估函数）所决定。博弈树搜索算法基本上就是利用计算机强大的数据处理和计算能力进行蛮力计算，只有在进行棋局评估时才体现出一点点“智力”，这点“智力”就是估值函数的价值。

### 行为树（Behavior Tree）

如果没有引入自主学习，那么类似这样：“如果 XXX，就 XXX”的条目，对于游戏中的角色来说，这就是它的知识，角色的这种知识越多，它的行为就显得更智能。
一般来说，这种知识更类似状态机的组织方式，即触发某个状态，输出对应的结果。但是当知识的条目比较多的时候，对知识的分类处理就成了问题，这个时候人们引入了行为树来支撑角色的
AI。时至今日，在各种游戏引擎中，你都可以看到行为树的身影。

在开始介绍行为树之前，先来想象一个游戏场景：游戏的角色“阿紫”在一个房间里，这个房间有一扇门、一个桌子、墙角有一个箱子，桌子上有一盏油灯、一个木盒子和一张纸，墙角的箱子被一把锁锁着。接下来，将介绍行为树的组织方式，并将阿紫在这个房间里可能的行为组织成一棵行为树。

#### 行为树的节点

既然是树，那就有节点（Node），行为树的顶点有很多种类型，一般至少都要包括动作行为（Action）、条件行为（Condition）、装饰行为（Decorator）和复合行为（Composite）等四种类型。

##### **动作行为（Action）**

动作行为，顾名思义，要执行一个动作，通过这个动作可以改变游戏中的某些状态。就这个游戏场景来说，就有很多动作，比如与门相关的动作有开门、关门、锁门（如果门上有锁），甚至可以暴力一点，砸烂门（破门而出）。

##### **条件行为（Condition）**

条件行为主要是对一些状态进行判断，比如周围是否有敌人、角色手上是否有钥匙、门是否可以打碎等。条件判断节点的行为就是返回判断的结果，true
表示条件符合，false 表示条件不符合。

##### **装饰行为（Decorator）**

装饰行为通常不会是行为树的叶子节点，因为它的意义就是对子节点的返回结果做额外的处理，比如将某个条件行为的结果取反，或者是将某个动作重复 100
次等。注意装饰行为和复合行为的差异，当子节点有复杂的行为逻辑的时候，还是要用复合行为节点。

##### **复合行为（Composite）**

复合行为也叫组合行为，复合行为可以理解为是一个行为，在这个框架中有多个行为按照某种方式组合，最后得到一个结果。复合节点通常包含了对所有子行为节点处理的规则，因此复合节点也被称为是控制节点，复合节点一般可分为序列节点（Sequence）、选择节点（Selector
）和并行节点（Parallel）。

**序列节点（Sequence）**

序列节点的子节点是一系列行为节点，序列节点的动作就是将其所有子节点依次执行一遍。在执行过程中，只有前一个子节点返回成功，才继续执行后面的子节点，如果前一个子节点返回失败，则序列节点直接返回失败（不再执行后面的子节点）。序列节点可以理解为是对所有子节点的行为做“and”操作，以我们设想的游戏场景为例，阿紫走到箱子前、拿出钥匙、打开箱子、取出箱子内的物品。这四个动作就构成一个序列，如果阿紫没有钥匙，则拿出钥匙的动作就会返回失败，于是这个节点就直接返回失败，后面的两个动作也就不用做了。

**选择节点（Selector ）**

选择节点与序列节点的行为刚好相反，选择节点也是将其所有子节点依次执行一遍，但是在执行的过程中，只要有一个节点返回成功，就直接返回成功（不再执行后面的子节点）。选择节点可以理解为是对所有的子节点做“or”操作，同样以我们设想的游戏场景为例，阿紫对门的动作有三个，即打开门（如果门是关的）、开锁（如果门上有锁）和砸开门（如果门锁着），这三个动作就构成一个选择。

**并行节点（Parallel）**

简单的理解，就是把所有子节点都并发执行一遍，但是细节上有很多控制，主要是对并行节点的返回值的控制策略，根据控制策略的不同，行为上略有差异，差异主要体现在返回结果的处理上。根据返回值处理的方法，可以分为并行选择模式（Parallel
Selector ）、并行序列模式（Parallel Sequence）和并行超模式（Parallel Hybird）。

#### 行为树的样子

之前的游戏场景，用行为树描述出来应该是这个样子，看看你理解的对不对？

![enter image description
here](https://images.gitbook.cn/924a0270-01ab-11e9-b0f4-ff31f4e5691e)

图（7）例子中的行为树

### 总结

决策树算法是一种监管学习算法，所谓监管学习就是事先准备好一批样本数据，每个单独的样本都有一组属性和一个确定的类别，然后通过对样本的学习得到一个分类器，用这个分类器能够对新出现的样本数据给出正确的分类，这样的机器学习就被称为监督学习。决策树的优点是不需要任何领域知识或参数假设，适用于高维度数据（每个属性可以理解为一个决策维度），数据处理的效率比较高，可以在短时间内得到一个效果较好的分类器。

当然，决策树算法的缺点是对于各类别样本数量不一致的数据，信息增益偏向于那些具有更多数值的特征。换句话说，对于样本数量少的数据，获得的信息增益比较少，很容易在剪枝处理过程种被忽略掉，从而失去一些很重要的决策。话又说回来，貌似多数机器学习算法都是这样的。决策树算法的另一个缺点是它完全忽略属性之间的相关性，对于有关联的属性无法获得对应的信息增益。

行为树原理很简单，但是对于一个行为复杂的角色来说，背后支撑它的行为树可能是一个高度很高、枝繁叶茂的树，如何用高效的数据结构或数据库来组织这棵树才是游戏开发者需要考虑的核心任务。很多游戏开发库都有成熟且高效的行为树控制库，有兴趣的话大家可以参考相关的库代码，了解具体实现的细节。

[请单击这里下载源码](https://github.com/inte2000/play_with_algo)

### 答疑与交流

> **为了方便与作者交流与学习，GitChat 编辑团队组织了一个《算法应该怎么玩》读者交流群，添加小助手-
> 伽利略微信：「GitChatty6」，回复关键字「259」给小助手-伽利略获取入群资格。**

