>
> 分类算法有很多种理论，比如决策树理论、K-最近邻法（KNN）理论、朴素贝叶斯理论、神经网络理论等，每种理论都有对应的分类算法。贝叶斯分类算法是众多分类算法中的一种，确切地说是一类，因为这类算法都是以贝叶斯定理为理论基础，所以被统称为贝叶斯分类。这一课我们将介绍贝叶斯分类算法，并用贝叶斯分类算法做一个简单的文本分类器，演示区分垃圾邮件和正常邮件的过滤器原理。

### 贝叶斯定理

贝叶斯（Thomas
Bayes）是个英国牧师，为了证明上帝的存在，他发明了概率统计学原理。这可不是什么讽刺与幽默，历史上很多科学的发现，都是一些神职人员在研究神学过程中的“副产品”，比如被誉为现代遗传学之父的孟德尔（Gregor
Johann Mendel）就是一个修道院的神父，他的豌豆实验想必大家都知道。

玩贝叶斯分类算法之前，先要了解一下贝叶斯定理，该定理其实是一个与概率有关的推理，这里就简单介绍一下贝叶斯定理。
**高能预警：前方有公式，不过放心，贝叶斯定理真的很简单** 。

#### 概率和条件概率

概率论中常用 $P(A)$ 表示 $A$ 事件发生的概率，这个也被称为先验概率或边缘概率。$P(A|B)$ 表示已知条件 B 发生的情况下 $A$
事件发生的概率，这个被称为条件概率。在古典概率论中，条件概率 $P(A|B)$ 的计算公式是：

$P(A|B) = \frac{P(B|A)\cdot P(A)}{P(B)}$

  

这个公式做一个变换，可以计算 $P(B|A)$：

$P(B|A) = \frac{P(A|B)\cdot P(B))}{P(A)}$

  
这个可以理解为当 $A$ 事件发生的情况下，有多大概率是由于 $B$ 条件触发的。

#### 多个条件时的条件概率

当有 ${B _{1}, B_ {2},…, B _{n}}$ 个条件的时候，如果这些条件是两两互斥的，则当 $A$ 事件发生时，$B_ {i}$ 条件触发
$A$ 发生的概率的计算公式可由上一节的公式变形得到：

$P(B_{i}|A) = \frac{P(A|B_{i})\cdot P(B_{i})}{P(A)}$

  
假如 ${B _{1}, B_ {2},…, B_{n}}$ 是完整样本空间的话，则 $P(A)$ 可由全概率公式计算出来：

$$ P(A) = \sum _{i=1}^{n}P(A|B_ {i})\cdot P(B_{i}) $$

实际应用时，当 A 事件发生时，反求 $B_{i}$ 条件触发 $A$ 发生的概率的计算应用更为广泛，这也正是贝叶斯理论应用于分类算法的理论基础。

### 贝叶斯理论

#### 贝叶斯理论原理

如果某种实体有 n 个相互独立的特征（Feature），分别为 $F _{1}, F_ {2},…, F _{n}$，这种实体可被分为 m
个类别（Category），分别是 $C_ {1}, C _{2},…,C_ {m}$。贝叶斯分类理论的原理就是：

假设有一个待分类的实体 $A={ F _{1}, F_ {2},…, F _{n}}$，分别计算 $P(C_ {1}|A)$，$P(C
_{2}|A)$，…，$P(C_ {m}|A)$，如果其中最大的那个值是

$P(C_{k}|A) = max\\{P(C_{1}|A),P(C_{2}|A),...,P(C_{m}|A)\\}$

  

那么可以认为实体 $A$ 属于 $C_{k}$ 这个分类。

其中 $P(C_{i}|A)$ 的计算公式可由上一节的多条件贝叶斯公式计算：

$P(C_{i}|A) = \frac{P(A|C_{i})\cdot P(C_{i})}{P(A)}$

  
由此可见，贝叶斯理论的原理还是很简单的，使用贝叶斯理论的关键就是如何计算各个条件概率。$P(A)$ 比较难计算，但是好在对于 $P(C _{i}|A)$
的计算来说，$P(A)$ 可视为一个常量，因此在比较 $P(C_ {i}|A)$ 大小的时候，只要比较分子部分 $P(A|C _{i})\cdot P(C_
{i}))$ 的大小即可。

由于实体 $A$ 的所有特征 $F _{i}$ 之间是相互独立的，因而 $P(A|C_ {i})$ 可由以下公式计算：

$P(A|C_{i})=P(F_{1}|C_{i})\cdot P(F_{2}|C_{i})\cdot ...\cdot P(F_{n}|C_{i}) =
\prod_{j=1}^{n}P(F_{j}|C_{i})$

  
这样一来，计算 $P(C _{i}|A)$ 需要的条件就是对每个特征的 $P(F_ {j}|C _{i})$ 和 $P(C_
{i})$，这些概率的值都可以通过对已知分类的数据进行统计计算得到，得到这些概率的过程也被称为训练样本集合的过程。

#### 贝叶斯分类器原理

当我们验证实体 $A$ 的一个个体 $a$ 时，$a$ 通常并不具有实体 $A$ 定义的全部特征，即 $a$ 可能只有特征集合 ${ F _{1}, F_
{2},…, F _{n}}$ 的一个子集。假如 $a$ 只有 ${ F_ {1}, F _{3}, F_ {8}}$
三个特征，在这种情况下，分别计算每种分类的 $P(C _{i}|{ F_ {1}, F _{3}, F_ {8}})$，比较这些概率值，假如 $P(C
_{2}|{ F_ {1}, F _{3}, F_ {8}})$ 最大，就可推测 $a$ 属于 $C_{2}$
分类，这就是贝叶斯分类器的原理。当然，这种情况下也可以理解为 $a$ 还是拥有全部特征的，只是那些不具备的特征对应的特征值是 0。

#### 贝叶斯分类理论的应用示例

假设某学校有男生 320 人，女生 280 人，其中女生中留长头发的人有 200 人，留短头发的人有 80 人，男生中留长头发的人有20人，剩下的 300
个男生都是短头发。现在，某老师在操场上看到一个长头发的背影随手乱丢垃圾，请你分析一下这个人可能是男生还是女生？

好吧，很显然应该是个女生，不过我们还是来算一算吧。首先看分类的目的是要识别是男生还是女生，因此 C 就是两个分类，不妨令 $C _{0}$
为男生分类，$C_ {1}$ 为女生分类。接下来看看有哪些是特征属性，显然，这里关注的每个人的特征属性就是两个：长头发和短头发，不妨令 $F _{0}$
是长头发，$F_ {1}$ 是短头发。这样，学生 A 就有两个特征，即 $A = { F _{0}, F_ {1}}$，现在我们需要在只有样本的部分特征
$F_{0}$ 的前提下，推测一下这个人是男生还是女生。

根据贝叶斯理论，我们需要分别计算 $P(C _{0}|F_ {0})$ 和 $P(C _{1}|F_ {0})$ 的概率：

$P(C_{0}|F_{0})=\frac{P(F_{0}|C_{0}) \cdot P(C_{0})}{P(F_{0})}=\frac{(20/320)
\cdot (320/600)} {(200+20)/600} = \frac{1}{11}$

  

$P(C_{1}|F_{0})=\frac{P(F_{0}|C_{1}) \cdot P(C_{1})}{P(F_{0})}=\frac{(200/280)
\cdot (280/600)} {(200+20)/600} = \frac{10}{11}$

  
显然，根据两个概率值的比较，这个人属于 $C_{1}$ 分类，也就是说，是个女生。

#### 贝叶斯分类理论的使用方法

上一节是个简单的例子，概率的计算用经典概率理论就可以解决，但是实际应用贝叶斯分类理论的时候，往往没有这么简单。应用贝叶斯理论做分类算法，一般分为三个阶段：

  * 第一个阶段是准备阶段，主要工作就是识别出样本的所有分类 $C _{i}$，并确定样本的全部特征 $F_ {i}$，接下来有人工对一些已知的样本进行分类，形成训练样本集合。人工分类的方法就是对所有的待分类数据，由人工进行分类，并输出特征属性和训练样本。
  * 第二个阶段是分类器训练阶段，主要工作就是生成分类器，具体来说就是计算每个类别在训练样本中的出现频率及每个特征属性划分对每个类别的条件概率估计，并将结果记录。其输入是第一步中的人工分类输出的特征属性和训练样本，输出是分类器中的各种概率值。
  * 第三个阶段是应用阶段，这个阶段的任务是使用分类器对待分类项进行自动分类，其输入是第二个阶段形成的分类器和待分类的数据，输出是待分类数据与类别的映射关系。

第一个阶段的工作非常重要，人工分类的质量对分类器的形成影响很大，分类器的质量很大程度上由特征属性、特征属性划分及训练样本质量决定。另外，训练样本的数量对分类器的影响也很大，很多网站和应用客户端为了增加训练样本的数量，还会允许用户自己提交样本。比如某电子邮件客户端就提供了一键举报垃圾邮件的功能，其实就是让用户帮忙对样本进行了分类，垃圾邮件的内容提交到服务器后，后台应用程序会自动完成特征属性的区分，然后将其提交到训练样本集合库，管理员会周期性地用样本库训练分类器，使得分类器的准确度不断提高。

第二个阶段的工作主要是一些机械性的计算工作，这一阶段的训练通常也是由程序自动完成的。

### 垃圾邮件分类器原理

很多电子邮件客户端都有垃圾邮件过滤器的功能，能自动识别并过滤垃圾邮件。要识别垃圾邮件，就需要有一个分类器做分类，贝叶斯分类算法常常被用来做这种工作。这一节我们用一个例子来演示一下如何用贝叶斯分类算法对文本内容进行分类，并识别出垃圾邮件。

#### 第一步：准备工作

第一步是理论准备工作，其实就是针对问题的建模过程，贝叶斯分类理论可以用来解决很多分类问题，但是理论必须要与模型相结合才能解决实际的问题，按照第二节“贝叶斯理论”介绍的理论来“套”一下，看看需要做哪些准备工作。

我们的问题是对邮件进行分类，那么可认为邮件内容 A 就是问题的实体，问题的实体确定了，接下来是识别分类，也就是对 A
进行分类。这个比较简单，根据我们的目的，将 A 分为两个类别，分别是垃圾邮件和普通邮件，这样一来 C 就确定了。再接下来是确定邮件内容 A
有哪些相互独立的特征，特征怎么确定呢？并且还要相互独立？答案就是从邮件的内容着手开始分析，我们用分词算法将邮件内容分成一个一个独立的词，每个词可能出现的概率就是邮件内容的特征。因为词是独立的，每个词出现的概率也是独立的，所以用每个词可能出现的概率作为邮件
A 的特征是合适的。

“邮件 A 的特征是每个词出现的概率”，这句话还是有点抽象，我们进一步分析一下。我们在介绍 **余弦相似度与相似算法**
的时候，介绍过将文本转化为向量的方法，就是对文本进行分词，然后建立一个词的向量。这里我们用相同的思想来处理邮件，将邮件内容作为普通文本进行分词，得到一个词汇列表。所有样本邮件的词汇列表汇集、整理在一起，去掉重复的词，就能得到一个词汇表。这个词汇表中的每个词的位置是固定的，可以看作是向量的一个维度。词汇表中词的总数是多少，这个向量就有多少维。有了向量的定义，我们就可以进一步计算出每个词在分类中出现的概率，这个概率就是向量在这个词所在的维度上的数值。

##### **样本数据模型**

根据以上分析，我们先对样本数据进行建模。所谓样本数据，就是一组词和一个（这组词对应的）分类结果的映射。一组词就是一个词的列表，简单说就是字符串数组或字符串列表，Java
可以用 List\，C++可以用 std::list\ 或
std::vector\。分类结果可以用一个字符串表示分类名称，也可以用一个整数表示分类编号，如果分类确定只有两类，还可以用 bool
变量来标识分类。演示程序的样本数据的数据模型定义是：

    
    
    typedef std::pair<std::vector<std::string>, std::string>   EXAMPLE_T;
    

用 std::pair 表示这是一对数据，first 是字符串数据，second
是人工识别出的分类结果。为了分类器的准确性，往往需要很多这样的样本数据对分类器进行训练，我们的演示程序也准备了 14 组样本数据：

    
    
    //人工分类好的样本数据
    std::vector<EXAMPLE_T>  examples =
    {
        { { "周六", "公司", "庆祝", "聚餐", "时间", "订餐" },                  "普通邮件" },
        { { "喜欢", "概率论", "考试", "研究", "及格", "补考", "失败" },         "普通邮件" },
        { { "贝叶斯", "理论", "算法", "公式", "困难" },                        "普通邮件" },
        { { "上海", "晴朗", "郊游", "青草", "蓝天", "帐篷", "停车场", "拥堵" }, "普通邮件" },
        { { "代码", "走查", "错误", "反馈", "修改", "入库", "编译" },           "普通邮件" },
        { { "公司", "单元测试", "覆盖率", "时间", "用例", "失败", "成功" },     "普通邮件" },
        { { "优惠", "打折", "促销", "返利", "金融", "理财" },                  "垃圾邮件" },
        { { "公司", "发票", "税点", "优惠", "增值税", "返利" },                "垃圾邮件" },
        { { "抽奖", "中奖", "点击", "恭喜", "申请", "资格" },                  "垃圾邮件" },
        { { "爆款", "秒杀", "打折", "抵用券", "特惠" },                        "垃圾邮件" },
        { { "招聘", "兼职", "日薪", "信用", "合作" },                          "垃圾邮件" },
        { { "贷款", "资金", "担保", "抵押", "小额", "利息" },                  "垃圾邮件" },
        { { "正规", "发票", "税务局", "验证", "咨询", "打折" },                "垃圾邮件" },
        { { "诚意", "合作", "特价", "机票", "欢迎", "咨询" },                  "垃圾邮件" }
    };
    

##### **分类器模型**

分类器模型中的数据分两部分，一部分是训练过程中的中间数据，用于最后计算各个词的出现概率；另一部分是最终计算的结果，用于对新的数据进行分类测试。count
是这个分类的样本个数，用于计算最后的 $P(C _{i})$，而 $P(C_ {i})$ 就存在 pci 中。totalWords
记录所有样本中属于这个分类的词的总数，有一个词就算一个，重复出现的也算。wordsNum
是个整数数组，它记录属于这个分类的词出现的次数，每个词在数组中的对应位置由总的词汇表确定。最后 wordsNum 和 totalWords
计算出每个词的出现概率，并存储在 pfci 数组中。

    
    
    typedef struct
    {
        int count;
        int totalWords;
        std::vector<int> wordsNum;
        double pci;
        std::vector<double> pfci;
    }TRAINING_T;
    
    typedef std::map<std::string, TRAINING_T>  TRAINING_RESULT;
    

对于学习到的多个分类，我们用一个映射表来管理，Java 可用 TreeMap\，C++ 就用 std::map\。

##### **得到词汇表（向量）**

词汇表不区分样本类别，将所有样本中的词汇全都统计在内，但是要去掉重复。词汇表中词的先后顺序不影响训练和测试的结果，但是词的位置只要定了，后续的训练和测试过程中就不能变了。

因为词汇表不能重复，这里我们偷个懒，对于演示程序这种规模的数据，用了 C++ 的集合
std::set，集合会根据关键字保证集合元素的唯一性，所以只要遍历所有样本数据，将所有的词加入到集合中就行了。完成统计后，我们再把集合中的词 copy
到一个数组或向量中，从而保证词的位置固定不变。下面代码中，每个样本数据 e.first 就是词表，e.second
就是分类名称，但是这个函数不管分类，因此没有用 e.second。

    
    
    std::vector<std::string> MakeAllWordsList(const std::vector<EXAMPLE_T>& examples)
    {
        std::set<std::string> wordsSet;
        for (auto& e : examples) //遍历所有样本数据
        {
            wordsSet.insert(e.first.begin(), e.first.end()); //将样本数据中的词表加入到集合中
        }
    
        std::vector<std::string> wordsList;
        //将集合中的词复制到向量中
        std::copy(wordsSet.begin(), wordsSet.end(), std::back_inserter(wordsList));
        return std::move(wordsList); //C++ 11 的 move 语义，优化返回值的效率
    }
    

到这里，贝叶斯分类器的准备工作就完成了，接下来就是训练数据，完成分类器的学习。

#### 第二步：训练分类器

在开始分类器训练编码之前，我们先介绍一下分类器学习的原理，理解了原理，对后面理解算法代码很有帮助。接下来我们以具体数字为例，量化计算一下这个概率，训练我们的分类器。

假设定义 $C _{1}$ 分类是垃圾邮件，$C_ {2}$ 分类是普通邮件，并且我们对所有样本邮件的内容进行统计（不分类统计）后，得到一个有 1000
个词的词汇表，我们用特征 $F _{1},F_ {2},…,F_{1000}$ 来标识这 1000 个词。

接着我们对已经分类为 $C _{1}$ 的垃圾邮件样本进行统计，计算出词汇表中的词在这些邮件中出现的总次数（TRAINING_T 中的
totalWords），假设我们统计的结果是词汇表中的词总共出现了 4000 次，其中 $F_ {1}$ 代表的词出现了 32 次，$F _{2}$
代表的词出现了 68 次等。这样我们就可以计算出在 $C_ {1}$ 分类中特征 $F{1}$ 的概率 $P(F _{1}|C_
{1})=32/4000=0.008$，特征 $F{2}$ 的概率 $P(F _{2}|C_
{1})=68/4000=0.017$，以此类推，可以得到全部的 $P(F _{i}|C_ {1})$。

同理，我们可以对被标记为 $C _{2}$ 的普通邮件也进行同样的计算，得到每个特征的 $P(F_ {i}|C_{2})$。

$P(C _{1})$ 和 $P(C_ {2})$ 的计算就更简单了，样本中 $C _{1}$ 分类的邮件出现的次数（TRAINING_T 中的
count）除以样本总数得到的结果就是 $P(C_ {1})$，同样，样本中 $C _{2}$ 分类的邮件出现的次数除以样本总数得到的结果就是 $P(C_
{2})$。

以上就是分类器学习（训练）的原理，接下来我们就来看看如何实现训练样本的算法。首先，我们需要一个将样本中出现的词转化成词的向量，这个向量中的每个位置的数字代表了词汇表中这个词在当前样本中出现的次数。这个向量初始化都是
0，一般来说，测试样本中词的个数跟词汇表中词的个数不是一个数量级的，因此这个向量中绝大多数位置上的数字都是 0，只有少数位置上的数字大于 0。

我们的演示程序中词汇很少，就简单用了数组，实际应用中的词汇表是一个很大的表，如果用固定数组会很浪费空间。实际应用一般怎么组织这个向量呢？我们在介绍多项式建模时其实提到过这种方法，就作为课后问题吧，这里就不多说了。MakeWordsVec()
函数的 allWords 参数是准备阶段得到的词汇表，words 参数是当前样本中出现的词的列表，返回值就是每个词出现的统计向量。

    
    
    std::vector<int> MakeWordsVec(const std::vector<std::string>& allWords, const std::vector<std::string>& words)
    {
        std::vector<int> wordVec(allWords.size(), 0); //跟词汇表一样长
    
        for (auto& word : words) //对于样本中的每个词遍历
        {
            //是否在词汇表中？如果不在词汇表中就放弃这个词（一般词汇表准备充分的话，很少出现这种情况）
            auto it = std::find(allWords.begin(), allWords.end(), word);
            if (it != allWords.end())
            {
                // 如果在词汇表中，就计算在词汇表中的位置索引，然后这个词对应的统计向量 +1
                wordVec[it - allWords.begin()] += 1;
            }
        }
    
        return std::move(wordVec);
    }
    

有了将样本中出现的词转化成词的向量的 MakeWordsVec() 函数，接下来就可以开始训练分类器了。TrainingExample()
函数分两个阶段完成训练，第一个阶段（第 1 个 for 循环）是统计样本中的信息，第二个阶段（第 2 个 for 循环）是计算概率值。在第 1
个循环中遍历每个样本数据，GetTrainClassificationDate() 函数负责从 tr 中取出 e.second
表示的分类的分类训练数据，如果 tr 中没有这个分类，就创建一个分类。

注意，在创建新分类数据时，给 totalWords 初始化为一个非 0 的数字，是为了防止样本数据组织得不好导致出现除 0
错误。MakeWordsVec() 将当前样本中的词转化成词向量，然后由 std::transform()
算法将其按位置累加到分类数据的词向量（tt.wordsNum）统计中。

第 2 个 for
循环根据训练的中间数据计算出分类中每个词的出现概率，注意，这里对概率的值取了对数。取对数的原因是这里计算出来的概率值都是非常小的值，有时候甚至和 0
没有太大的区分度，因此要对其取对数，增大这些概率值之间的区分度。

根据高等数学的基础知识，对数函数是单调函数，并且不改变原函数的单调性。换句话说，如果两个数 a > b，那么 log(a) > log(b)
也是成立的。但是取对数会对前面理论部分介绍的计算方法产生一点影响，这个在第三步分类测试的时候再具体介绍。

    
    
    TRAINING_RESULT TrainingExample(const std::vector<std::string>& allWords, const std::vector<EXAMPLE_T>& examples)
    {
        TRAINING_RESULT tr;
    
        for (auto& e : examples)
        {
            TRAINING_T& tt = GetTrainClassificationDate(tr, e.second, allWords.size());
            tt.totalWords += e.first.size(); //累加词的数量
            std::vector<int> wordNum = MakeWordsVec(allWords, e.first);
            std::transform(wordNum.begin(), wordNum.end(), tt.wordsNum.begin(), 
                           tt.wordsNum.begin(), std::plus<int>());
        }
    
        for (auto& cr : tr)
        {
            cr.second.pci = double(cr.second.count) / examples.size();
            for (std::size_t i = 0; i < allWords.size(); i++)
            {
                cr.second.pfci[i] = std::log(double(cr.second.wordsNum[i]) / cr.second.totalWords);
            }
        }
    
        return std::move(tr);
    }
    
    TRAINING_T& GetTrainClassificationDate(TRAINING_RESULT& tr, const std::string& classification, std::size_t vecLen)
    {
        auto it = tr.find(classification);
        if (it == tr.end())
        {
            tr[classification].count = 1;
            tr[classification].totalWords = 2; //初始化为 2，避免除 0
            tr[classification].wordsNum = std::vector<int>(vecLen, 1); //初始化为 1
            tr[classification].pfci = std::vector<double>(vecLen);
        }
        else
        {
            tr[classification].count++;
        }
    
        return tr[classification];
    }
    

#### 第三步：应用分类器

分类器应用，就是用训练好的分类器参数，对未知的（或未分类）的数据进行自动分类。分类验证的原理就是用分词算法从待验证的邮件文本分离出一组词，然后应用
MakeWordsVec() 函数将其转化为这组词的统计向量。将这组词的统计向量与分类器学到的 $P(F _{j}|C_
{i})$（就是tr[classification].pfci 数组中的元素）相结合（结对相乘），得到这组词对应的特征向量（数组）。

统计向量中值为 0 的项与 tr[classification].pfci 数组中对应概率相乘的积是
0，说明这组词不具备这个词的特征（不存在这个词），后面的概率计算时，0 也不会对其他特征的结果产生影响。

因此，我们不需要像算法原理里描述的那样，将这组词对应的特征单独提取出来计算，因为数组中的 0 不影响结果。得到这组词的 $P(F _{j}|C_ {i})$
后，就可以用全概率公式计算 $P(A'|C_{i})$，注意这里的 $A'$ 是全部特征 $A$
的一个子集。本来全概率公式中各个分项之间是乘法运算，但是因为我们之前对概率取了对数，所以这里的乘法就变成了对数的加法，还记得对数运算规则吧：

$$ log(ab) = log(a) + log(b) $$

不止是计算全概率公式，后面计算 $P(A'|C _{i})\cdot P(C_ {i})$ 的时候，也要变成加法，因为我们对 $P(C_{i})$
也取了对数。

看看分类器的算法实现 ClassifyResult() 函数吧，是不是很简单？需要分类的词存在 wordsList 列表中，先用
MakeWordsVec() 函数得到这组词的统计向量 numVec，然后将 numVec
向量中的每个分量与训练得到的对应分量出现概率做乘法计算，统计向量中的 0 与概率分量的乘积也是 0。将得到的概率分量累加，然后再加上 $P(C _{i})$
的对数，得到的结果就是 $P(A'|C_ {i})\cdot P(C _{i})$，前面解释过了，因为对于这组词来说，$P(A')$ 是常量，直接比较
$P(A'|C_ {i})\cdot P(C_{i})$ 就可以了。

    
    
    std::string ClassifyResult(const TRAINING_RESULT& tr, const std::vector<std::string>& allWords, 
                               const std::vector<std::string>& wordsList)
    {
        double pm = -DBL_MAX;
        std::string classification;
    
        std::vector<int> numVec = MakeWordsVec(allWords, wordsList);
        for (auto& cr : tr)
        {
            double p = 0.0;
            for (std::size_t i = 0; i < allWords.size(); i++)
            {
                p += numVec[i] * cr.second.pfci[i];
            }
            p += std::log(cr.second.pci);
            if (p > pm)
            {
                pm = p;
                classification = cr.first;
            }
        }
    
        return classification;
    }
    

终于，到了激动人心的垃圾邮件识别环节了。做了那么多准备，也完成了训练，这个简单的分类器效果如何呢？我们假设从两封邮件中分别提取了两组词：

["公司","保险","讨论","喜欢","周六","郊游","蓝天"]

["公司","优惠","打折","秒杀","喜欢","合作"]

结果我们的分类器正确识别了它们：

![](https://images.gitbook.cn/d1b0c900-0fd7-11e9-b42e-390d5c3452ad)

图（1）分类结果示意图

### 总结

很多大名鼎鼎的算法，其原理剖开讲，其实都是很简单的。贝叶斯算法中的那些神秘词汇，什么样本啊、训练啊，其实就是这么简单。这一课我们用了 50 多行代码（5
个函数）实现了一个简单的垃圾邮件分类器，而且我们的演示算法支持多个分类，只要样本数据 examples
中出现的分类，演示算法都会进行统计计算，并按照概率计算的结果进行分类。

最后，朴素贝叶斯算法是基于条件概率的一种分类算法，通过概率大小来进行分类，和其他分类算法相比，贝叶斯分类算法有一定的局限性。比如贝叶斯理论要求特征之间是独立的，并且每个特征的权重是一样的，对于一些特征之间存在比较强的关联性，或者特征的重要性不一样的情况，贝叶斯分类算法的效果就不那么好了。

[请单击这里下载源码](https://github.com/inte2000/play_with_algo)

### 答疑与交流

> **为了方便与作者交流与学习，GitChat 编辑团队组织了一个《算法应该怎么玩》读者交流群，添加小助手-
> 伽利略微信：「GitChatty6」，回复关键字「259」给小助手-伽利略获取入群资格。**

