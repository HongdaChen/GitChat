本节课将系统地介绍
ELK（Elasticsearch、Logstash、Kibana）在项目中的应用，各自扮演了什么样的角色，以及如何配置索引、Logstash 等。

### Logstash

我们将用 Logstash 模拟整个天气指标数据的采集过程，首先我们将会对 Logstash
做一个简单的讲解，然后举几个例子，最后应用到天气数据采集过程中。

Logstash 是一款开源优秀的数据采集工具，数据源可以是控制台的标准输入、日志文件、Kafka
等，中间由过滤器对采集的数据进一步处理，然后由输出插件将结果输出到 ES，或者 Kafka。

Logstash 采集数据基本流程如下：

    
    
    #　输入
    input {
      ...
    }
    
    # 过滤器
    filter {
      ...
    }
    
    # 输出
    output {
      ...
    }
    

同样我们使用 Docker 作为平台来运行测试 Logstash。因为 Docker 跨平台，所以你不需要学习如何在 Linux、Windows、Mac
上配置，另外还不会污染了当前系统的环境。使用 Docker 随用随启动，不用就丢掉。也不用刻意去学习
Docker，当做一个方便的虚拟机使用就是了，言归正传。

进入到控制台，拉取 Logstash 镜像：

    
    
    docker pull docker.elastic.co/logstash/logstash:7.1.1
    

查看镜像 ID 启动容器：

![image-20200524170523288](https://images.gitbook.cn/62da7f70-ac71-11ea-
bbfe-31ec274b5742)

启动容器，并使用 bash 命令进入：

    
    
    docker run  --rm -it  --name logstash   b0cb1543380d  bash
    

进入到容器中，可以看到 Logstash 的目录结构：

    
    
    bash-4.2$ ls -l
    total 888
    drwxrwsr-x 2 logstash root   4096 May 23  2019 bin
    drwxrwsr-x 1 logstash root   4096 May 23  2019 config
    -rw-rw-r-- 1 logstash root   2276 May 23  2019 CONTRIBUTORS
    drwxrwsr-x 2 logstash root   4096 May 23  2019 data
    -rw-rw-r-- 1 logstash root   4119 May 23  2019 Gemfile
    -rw-rw-r-- 1 logstash root  22491 May 23  2019 Gemfile.lock
    drwxrwsr-x 6 logstash root   4096 May 23  2019 lib
    -rw-rw-r-- 1 logstash root  13675 May 23  2019 LICENSE.txt
    drwxrwsr-x 4 logstash root   4096 May 23  2019 logstash-core
    drwxrwsr-x 3 logstash root   4096 May 23  2019 logstash-core-plugin-api
    drwxrwsr-x 4 logstash root   4096 May 23  2019 modules
    -rw-rw-r-- 1 logstash root 808305 May 23  2019 NOTICE.TXT
    drwxr-sr-x 1 logstash root   4096 May 23  2019 pipeline
    drwxrwsr-x 3 logstash root   4096 May 23  2019 tools
    drwxrwsr-x 4 logstash root   4096 May 23  2019 vendor
    drwxrwsr-x 9 logstash root   4096 May 23  2019 x-pack
    

pipeline 目录是用来配置，输出 -> 过滤 -> 输出插件，这里我们将配置一个控制台输入输出作为一个简单使用的 Demo。

进入到 pipeline 目录，修改 logstash.conf 文件，配置输入是 stdin，输出是 stdout。

    
    
    /usr/share/logstash/pipeline
    
    
    
    input {
      stdin {
    
      }
    }
    
    output {
      stdout {
    
      }
    }
    

进入到 bin 目录执行 ./logstash 启动完成之后，就会提示输出，输出 `hello word` 后，黄色部分为输出。这是一个简单的
Logstash pipeline 案例。

![image-20200524175241833](https://images.gitbook.cn/794be3c0-ac71-11ea-8fbf-69de68d9fea0)

### Elasticsearch

weather_monitor 索引 Mapping 关系如下。我们要根据需求对 Mapping 进行修改，目的是优化 ES 内存，让 mapping
变的更简单。首先 city、weather 配置成 keyword，去掉 text 类型节约内存。另外最好把 dynamic 设置成 false。

    
    
    {
      "mapping": {
        "properties": {
          "city": {
            "type": "text",
            "fields": {
              "keyword": {
                "type": "keyword",
                "ignore_above": 256
              }
            }
          },
          "temperature": {
            "type": "long"
          },
          "timestamp": {
            "type": "date"
          },
          "weather": {
            "type": "text",
            "fields": {
              "keyword": {
                "type": "keyword",
                "ignore_above": 256
              }
            }
          }
        }
      }
    }
    

新建一个索引 weather_monitor_new，如果要重新建 mapping 必须要新建一个索引，然后通过 reindex
接口进行重新把数据索引起来，这里我们不使用 reindex，将会结合 Logstash 重新将数据导入。

这里把 dynamic 设置成 false，放置这个索引中由于人为的失误导致产生大量的字段。这个参数共有 3 个值，分别是
true、false、strict。

  * true：如果设置成 ture，那么当有新的字段往这个索引中索引的时候，就会更新 mapping，添加一个新的字段，并匹配上新的类型，这样是很方便，但是也很危险，如果某个字段名字写错了，并成功索引到文档中，那么这个条文档在后面可能因为字段名称问题而索引不到。
  * false：设置成 false，那么如果有陌生的字段，将会被忽略，但是在 _source 字段是能够被查询到的。
  * strict：设置成 strict 的话，_source 字段也将会查询不到。

真正用到生产环境中最好把默认的 true 改掉，否则很容易出 Bug。

    
    
    PUT weather_monitor_new
    {
      "mappings": {
        "dynamic": false, 
        "properties": {
          "city": {
            "type": "keyword"
          },
          "temperature": {
            "type": "long"
          },
          "timestamp": {
            "type": "date"
          },
          "weather": {
            "type": "keyword"
          }
        }
      }
    }
    

![image-20200524195603718](https://images.gitbook.cn/b4f2e2c0-ac71-11ea-a004-316ca14d0de8)

### slow log

ES 查询中最怕被一些不合理的查询语句造成了资源被耗尽，为了能够监控这些语句，可以通过配置慢日志设置阈值来监控这些语句。

我们针对 search 进行阈值监控，如果一条查询语句大于 10s，将会记录警告日志，info、debug 等同理配置阈值。

对于索引 index 操作，我都设置成 1ms，测试能否被记录下来：

    
    
    PUT weather_monitor_new/_settings
    {
        "index.search.slowlog.threshold.query.warn": "10s",
        "index.search.slowlog.threshold.query.info": "5s",
        "index.search.slowlog.threshold.query.debug": "2s",
        "index.search.slowlog.threshold.query.trace": "500ms",
        "index.indexing.slowlog.threshold.index.warn": "1ms",
        "index.indexing.slowlog.threshold.index.trace": "1ms",
        "index.search.slowlog.level": "info"
    }
    

打开我们使用的 ES 控制台面板查看，从日志中可以看到，索引记录已经被记录下来了。

![image-20200524203409135](https://images.gitbook.cn/c902d770-ac71-11ea-
acb2-8335c26cfba1)

### 数据采集

我们需要在 Docker 中启动一个 Logstash 容器，所以要保证 ES 集群要与 Logstash 在同一个网络中，所以查看 ES 的网络是
docker_es7net。将 Logstash 也加入这个网络中去。

    
    
    (base) ➜  ~ docker network ls
    NETWORK ID          NAME                DRIVER              SCOPE
    260fd98e25c2        bridge              bridge              local
    a21a0156c7e0        docker_es7net       bridge              local
    2a5698acf873        host                host                local
    e2ea1ddec447        none                null                local
    

启动容器，并映射一个 8888 端口号，这样外部就可以通过 8888 端口访问到 Logstash 容器内部。

    
    
    docker run  --rm -it  --name logstash   -p 8888:8888  --net=docker_es7net   b0cb1543380d  bash
    

修改 /usr/share/logstash/pipeline/logstash.conf 文件，配置 Logstash。

我们通过 HTTP 让 Logstash 去监控 8888 端口的数据，我们通过 HTTP 请求，发送 JSON 数据到 Logstash，然后
Logstash 收集到数据，然后过滤处理一下，发送到 ElasticSearch。

Logstash 的 input 插件提供了 HTTP 插件，配置一个端口号 8888 让它去监控，然后配置一个 filter
字段用来处理采集的数据，然后再通过 output 插件发送到 Logstash 与控制台面板，这里由于电脑性能的原因，我将 ES 集群设计成单机的。

这里我先配置一个 HTTP intput 插件，然后通过 HTTP 请求去看一下格式。

    
    
    input {
        http {
        port => 8888 # default: 8080
      }
    }
    output {
      stdout {
        codec => rubydebug
      }
    }
    

我使用 Python 写了一个 HTTP 请求（代码在文末），现在不管代码如何，我们先看一个采集的数据是什么样的。

下图可以看到，发送的数据都在 message 里面，然后好多 headers 数据。我们只想把 message 里面的数据发送到 ES，其他的不要。

![image-20200525200307068](https://images.gitbook.cn/04c65f20-ac72-11ea-
acb2-8335c26cfba1)

因为我们需要 filter 去处理过滤，首先我们需要一个 JSON 过滤器，用来处理 message；然后把 message 与 headers
字段都删除。time 我传输的是时间戳，时间戳方便不包涵时区信息，解析起来也方便。

要把 time 字段要处理成 date 格式，所以再配置一个 date 插件。解析成功的 time 字段默认会映射到 @timestamp
字段里面，time 字段就没用了，所以把 time 字段也配置删除 （remove_field 字段配置删除）。

    
    
    filter {
       json {
          source => "message"
          remove_field => [ "message","headers" ]
        }
       date {
          match => [ "time", "MM/dd/yy HH:mm:ss","ISO8601","UNIX","UNIX_MS","TAI64N" ]
           remove_field => ["time"]
       }
    }
    

我们把处理好的数据，发送到 ES 去。

配置 hosts 与 index：

    
    
    output {
      stdout {
        codec => rubydebug
      }
      elasticsearch {
        hosts => "es701"
        index =>"weather_monitor"
      }
    }
    

整个 Logstash 配置如下：

    
    
    input {
        http {
        port => 8888 # default: 8080
      }
    }
    
    filter {
       json {
          source => "message"
          remove_field => [ "message","headers" ]
        }
       date {
          match => [ "time", "MM/dd/yy HH:mm:ss","ISO8601","UNIX","UNIX_MS","TAI64N" ]
           remove_field => ["time"]
       }
    }
    
    output {
      stdout {
        codec => rubydebug
      }
      elasticsearch {
        hosts => "es701"
        index =>"weather_monitor"
      }
    }
    

再发送一下请求，可以看到如下，格式已经很整齐了。

![image-20200525153341206](https://images.gitbook.cn/367814f0-ac72-11ea-89ce-91247f7c923b)

到 Kibana 上配置好索引模式，就可以看到我们插入的数据了：

![image-20200525153840753](https://images.gitbook.cn/41a45780-ac72-11ea-b8fa-67c56133b3a3)

客户端请求代码如下：

    
    
    import urllib.request
    import json
    import time
    import random
    from datetime import datetime
    
    
    weather=['fine','cloudy','rainy']
    city=['shanghai','hefei','beijing','chengdu','chongqing','hangzhou','suzhou','ningbo','tianjin','nanjing','guangzhou']
    datetime_str = '01/01/20 13:55:26'
    time_falg = datetime.strptime(datetime_str, '%m/%d/%y %H:%M:%S')
    weather_monitor = {
        'weather': weather[random.randint(0,2)],
        'temperature':random.randint(-10,40),
        'city':city[random.randint(0,10)],
        'time': datetime.timestamp(datetime.fromtimestamp(time.time()-(time.time()-time_falg.timestamp())*random.random()))
    }
    # dict convert string
    
    
    json_info = json.dumps(weather_monitor)
    # string convert bytes
    data=bytes(json_info,"utf8")
    # build request
    req = urllib.request.Request("http://127.0.0.1:8888",data=data)
    # open request
    ret = urllib.request.urlopen(req)
    

### 小结

本课时是一个比较综合系统的课时，详细介绍了如何搭建 ELK 去采集日志，整体的流程是怎么样的，ES 提供了丰富的插件，基本上都能满足我们的需求。

本课时首先对 Logstash 基本介绍，然后优化 index mapping，然后使用 Logstash 监听 8888 端口，在外部使用 Python
制作请求，将数据发送给 Logstash 过滤处理后，发送给 ES，完成整个流程。

本课时的难点还是在于对 Logstash 配置的理解，要知道数据源可以是 HTTP、MySQL、file 等，所以 Logstash 是 ELK
环境搭建中非常重要的一环。

