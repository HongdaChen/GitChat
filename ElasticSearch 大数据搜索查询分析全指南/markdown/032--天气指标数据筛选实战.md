上节课中介绍了对天气指标数据的采集生成，这节课中将重点介绍如何使用 Python 对 ES 开发，以天气指标数据为例。Kibana
确实也能够提供丰富的功能对天气指标数据分析，但是我们的需求是要得到分析原始结果，Kibana 分析的结果需要登录在线看，不能够被缓存下来。

### 天气 App

假设一个项目场景，开发一款手机天气 App，这款 App 数据由 ES 进行管理。

假设公司在开发一个 15
天趋势预报功能，如下图，想要得到预测数据，那就得使用深度学习去训练不同地区的天气历史数据，大家都是知道深度学习非常的吃数据，因此数据当然是越多越丰富越好。而你在的部门专门有个程序负责采集每天的天气状态，然后存到
ES。你老大布置给你个任务，由你来负责把不同地区的天气历史数据，采集下来，然后用来训练模型。

![image-20200411143725125](https://images.gitbook.cn/52619150-99f3-11ea-83e2-c13c00016243)

### 数据采集

拿到这样的需求，先分析一波。

不同地区的话，那就是要根据地区进行筛选，配置个 filter 就行了，之前的课时也讲过，这种需求应该配置 filter 而不是用 must
来配置条件，这里回顾一下，如果使用 filter 来作为过滤条件，那么过滤出的数据将不会有 _score 相关性分数，这样的查询耗费更少的资源。

先确定 ES 中到底有哪些城市，登录 Kibana，新建一个统计表，字段选择 city，大小可以设置大一点，我这里设置为 100，假设最多只有 100
个城市。可以看到已经把所有城市都列出来了，并且还有不同地区的数据个数的计数。

![image-20200411145340789](https://images.gitbook.cn/6a764560-99f3-11ea-95eb-7d99e394b1c2)

假设先对北京的天气进行采集。这里最禁忌的就是一股脑把 ES 中所有的数据都下载下来，因为太耗费资源时间了。

优化查询的第一步，就是避免全索引搜索、全库搜索。为了避免单个索引过大，大多数公司都会以日期作为某个索引的后缀名，这也不是什么稀奇的做法，在传统数据库中我们也经常横向切分表，避免表过大，ES
也是如此。这里的全索引搜索就是如此。

如果在真实的情景中，公司应该会把天气数据按照不同的索引存下来，可以这样设置，wether-2019 表示 2019 年采集的数据，wether-2020
表示 2020 年采集的数据，如果只要查今年的，那就只使用 wether-2020，而不要直接使用 wehter-*
索引，对所有的索引都查一遍。全库查询指的是，能在 ES 使用的过滤条件，尽量在 ES
中过滤，不要全部都下载下来，在开发语言中再过滤，这里的原因就不用多说了。

### 优化查询效率——分区查询

采集数据，先把北京的采集下来，或者你也根据日期进行分割，这里最好按照地区进行分割，因为这样采集下来后，就不用写多余的代码再进行地区分类。

首先打开控制台，切换到 Python ES 开发环境，Python 开发我习惯使用 Anconda 自带的 Spyder 工具。

    
    
    (base) ➜  ~ conda env list
    # conda environments:
    #
    base                  *  /Users/penzhu/anaconda3
    es                       /Users/penzhu/anaconda3/envs/es
    pytorch                  /Users/penzhu/anaconda3/envs/pytorch
    
    (base) ➜  ~ source activate  es
    (es) ➜  ~ spyder
    

写好 DSL，然后在 Kibana 上测试，语句是否正确，结果是正确的：

    
    
    POST /weather_monitor/_search
    {
      "size": 10,
      "query": {
        "bool": {
          "filter": {"term": {
            "city.keyword": "beijing"
          }}
        }
      }
    }
    

![image-20200411151248703](https://images.gitbook.cn/bb9fdbe0-99f3-11ea-95eb-7d99e394b1c2)

### 优化查询效率——减少不必要的数据传输

我们知道，天气数据一共有四个字段，wether、temperature、city、timestamp。这四个字段不是所有的都需要，我们只需要“温度”和“时间戳”字段，所以过滤掉其他的无用字段，减少网络传输的开销，也减少本地内存的开销。_source
字段可选择性的，选择只要哪些字段。

    
    
    POST /weather_monitor/_search
    {
      "size": 10,
      "_source": ["temperature","timestamp"], 
      "query": {
        "bool": {
          "filter": {"term": {
            "city.keyword": "beijing"
          }}
        }
      }
    }
    

![image-20200411151805697](https://images.gitbook.cn/ded9cda0-99f3-11ea-
af90-cbb5738a0aa8)

### Python 开发

连接 ES：

    
    
    from elasticsearch import Elasticsearch
    def get_es_instance():
        '''
        es version:6.x
        '''
        ES_API="localhost:9200"
        es = Elasticsearch([ES_API],timeout=1000)
        return es
    
    es=get_es_instance()
    

配置 DSL：

    
    
    dsl={
      "size":10,
      "_source": ["temperature","timestamp"], 
      "query": {
        "bool": {
          "filter": {"term": {
            "city.keyword": "beijing"
          }}
        }
      }
    }
    

获取数据，7.x 版本以后，ES 不用配置 type 类型，统一是 _doc，所以这里只用传输两个参数就行。

    
    
    index="weather_monitor"
    data=es.search(dsl,index)
    

这里立马返回了数据，可以看到返回的结果：

![image-20200411153414114](https://images.gitbook.cn/fa6272c0-99f3-11ea-8c90-bb96514abba4)

同时可以看到只返回了 10 条结果，这是因为 DSL 语句中 size 被我们设置成了 10，即使把 size 删掉以后，也是只返回了 10
条，这是因为默认 size 是有值的并且是 10。

![image-20200411153558984](https://images.gitbook.cn/0d140ff0-99f4-11ea-8c90-bb96514abba4)

因此为了取回所有数据，必须把 size
设置得尽可能大一点，这样才能够把数据都去取回来，那到底要设置为多大呢？总不能设置成一个很大的值吧，如果太多，是不是也会造成占用 ES
时间过久，本地等待时间过长，内存爆炸等问题呢？

在 ES 中先统计好即可，统计结果可以看出有 9182 条，这里是个坑，但是必须要指出来，因为 Cardinality
统计方法是一个近似统计方法，得到是个近似值，所以不准。可以用 value_cout 来统计，value_count 是不去重复统计个数，使用不会重复的
_id 来统计，就能够得到正确的个数。

    
    
    POST /weather_monitor/_search
    {
      "size": 0,
      "_source": ["temperature","timestamp"], 
      "query": {
        "bool": {
          "filter": {"term": {
            "city.keyword": "beijing"
          }}
        }
      },
      "aggs": {
        "count": {
          "cardinality": {
            "field": "_id"
          }
        }
      }
    }
    

![image-20200411154218334](https://images.gitbook.cn/41d68830-99f4-11ea-9d43-4b614dacd168)

使用 value_count 统计后得到 9131，如果不放心，可以设置成 10000 来放置会漏掉一些数据。

![image-20200411155052022](https://images.gitbook.cn/4a1d0ff0-99f4-11ea-8c90-bb96514abba4)

这里我设置成 size 10000，仍然只返回了 9131 条数据，说明 value_cout 还是值得信任的。

![image-20200411155232564](https://images.gitbook.cn/53766740-99f4-11ea-a84f-f7d3d4dae1cc)

### 解析数据并导出 CSV

如果没有装 Pandas 的话，只需要使用 pip 安装一下就行了，值得注意的是一定要在 ES 环境的那个控制台安装：

    
    
    (base) ➜  ~ source activate  es
    (es) ➜  ~ pip install pandas
    Collecting pandas
      Downloading pandas-1.0.3-cp37-cp37m-macosx_10_9_x86_64.whl (10.0 MB)
         |████████████████████████████████| 10.0 MB 6.4 MB/s
    Requirement already satisfied: pytz>=2017.2 in ./anaconda3/envs/es/lib/python3.7/site-packages (from pandas) (2019.3)
    Collecting numpy>=1.13.3
      Downloading numpy-1.18.2-cp37-cp37m-macosx_10_9_x86_64.whl (15.1 MB)
         |████████████████████████████████| 15.1 MB 2.7 MB/s
    Requirement already satisfied: python-dateutil>=2.6.1 in ./anaconda3/envs/es/lib/python3.7/site-packages (from pandas) (2.8.1)
    Requirement already satisfied: six>=1.5 in ./anaconda3/envs/es/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas) (1.14.0)
    Installing collected packages: numpy, pandas
    Successfully installed numpy-1.18.2 pandas-1.0.3
    (es) ➜  ~
    

解析并导出 CSV 代码如下：

```python import pandas as pd wether _table=pd.DataFrame()
hits=data["hits"]["hits"] for hit in hits: source=hit["_ source"] wether
_table=wether_ table.append(source,ignore_index=True)

wether _table.to_ csv("wether.csv") ```

### 小结

本节以一个 App 应用需求为例，说明了如何进行对需求进行分析，去更高效地查询数据，利用 _scoure 字段减少不必要的网络传输，利用
value_count 得出需要设置 size 是多少，并把得到的数据整理成 CSV 文件导出。本课时基本上介绍了如何从 ES
取数据，以及需要注意点的全过程。ES 的开发查询优化远不止于此，下个课时将会介绍更多的优化技巧，在需要从 ES 获取大量数据或者是大量聚合数据时候非常实用。

