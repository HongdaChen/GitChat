首先，恭喜大家已经阅读完整个专栏。一般而言，不管是书籍也好，视频课程也罢，能够完整跟下来的就已经很不容易了。多半原因要归功于这些材料的内容上，多采用堆砌的创作手法，大多时候需要读者对材料进行再次整合、加工、理解。虽然我尽量以通俗简单的形式，将内容体现出来，但水平毕竟有限，望大家海涵。

其次，专栏内容定位为初级入门，尽量帮助大数据初学者减少初期的困难，环境搭建使用脚本完成，避免从入门到劝退的过程。内容上，尽量帮助大家理顺，很多知识看起来难，只是授课的人将它复杂化了，其实一旦掌握了本质之后，会发现它其实非常容易。但大道至简，知易行难，需要大家之后不断练习，在此基础上加强知识的认知深度。

当然很多学员也避免不了大失所望，认为专栏没有达到其理想的深度，这部分学员一般是有一定基础的，希望能窥探更深层次的技术。但一门课程的定位无法面向所有的群体，尤其是这门课程的初级定位，不会讲解很深层次的技术细节，对初学者来说却是较为友好的。对于大数据的其它技术，已经在写了，在不久的将来会与大家见面。

在学习完这门专栏之后，如何深入？首先一定是反复使用，看会了不等于真会了。其次，对于出现的一些问题，多借助搜索引擎，自行解决。对于技术上的细节，没有比文档更好的材料了，虽然文档大多为英文，但浏览器翻译的工具也不在少数。新学习的内容、解决的问题，一定要多记录，因为将来可能用得上；在代码上复用，在知识上也要复用，不然之后某个时间点，再需要用到这部分知识的时候，就只能再去重新整合材料，而这部分工作一般都是重复的，如果你留有之前的总结文档，就可以在此基础上继续去学习、增加新的内容，而不需要反复折磨自己。这好比玩单机游戏，玩了一半，只要你重装了电脑，没有留之前的存档，就需要重头来过。

事情最好只做一次，代码复用是这样，其它事情也都是这样。

附上 Hadoop 官网地址，想要深入，多逛逛。

> <http://hadoop.apache.org/>

很多讲师，学习一部分知识，只需要很短的时间，这虽然与他的工作性质有关，但更多的是学习思路的不同；他们更关注这个技术，它分了哪几部分，每部分有哪些内容，每部分内容它本质上是什么，之间的逻辑是怎样的，怎么用自己的话描述出来；实际上这就是深度学习的体现。虽然大家职业各异，但可以学习这种思路，把自己当成讲师，如果自己都觉得可以讲课了，那你对技术的理解就很深入了，也可以评判市面上的课程质量了。

好了，就此打住。

最后的最后，祝大家前程似锦，再会！

