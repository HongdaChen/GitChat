### 一个背景话题

本篇我们来介绍概率统计当中的极限思维，我们首先从一个大家都非常熟悉的场景切入来展开本篇内容的讨论。

比如说，我们想获得本省 $15$ 岁男生的平均身高，这时你会怎么做？显然你不会也不可能真的去统计全省所有 $15$
岁男生的身高，然后再求平均值，这样做不太现实。因此，你会去找一些样本，也就是找一部分本省 $15$
岁的男生，取他们身高的平均值，用这个样本的平均值去近似的估计所有 $15$ 岁男生的平均身高。

没错，一般就是这么干的。那接下来我再问你，找 $100$ 个样本取得的平均值和 $1000$
个样本所取得的平均值，哪一个你认为更有可能接近真实的全省男生的平均身高（也就是期望）呢？你会说应该是 $1000$
个吧，毕竟样本数量多，上下偏差相互抵消，应该会更接近一些。你的直觉没有错。

在数据分析的应用中，经常会有上述类似的应用场景，我们需要分析一类对象，常常得去获取它的关键参数，就比如上面所提到的全体男生身高的均值，但是现实中我们不可能去穷尽全部的研究对象，而是只能取得一部分的样本，通过计算这部分样本的参数值去近似的估计总体的目标参数，样本数量越大，近似效果越好。

这里的理论依据就是我们下面要详细讲解的大数定理，大数定理是一个非常底层的基础性原理，大量的机器学习理论和算法实际上都建立在这个基础之上。我们常常是理所当然地直觉上感受到它的存在，却很少仔细想过背后的原因。那么通过这篇内容，我们会深入透彻地理解大数定理、中心极限定理背后的极限思想，另一方面也为机器学习的后续内容——参数估计打下一个良好的基础。

这一篇的内容安排如下：

  * 首先，我们介绍大数定理的原理，并用 Python 语言对其进行模拟，给大家一个更直观的感受；
  * 接着，我们会介绍中心极限定理以及它在工程实践中的应用价值和意义，同时也会用 Python 语言来对其进行模拟；
  * 最后，我们会讲解大数定理在机器学习和数据分析中的一个重要应用——蒙特卡罗方法，并仔细剖析它的应用场景和内涵。

### 大数定理

#### 原理介绍

好了，下面我们开始正式进入到大数定理的内容中。

我们有如下的随机变量：$X_1,X_2,...,X_n$，它们彼此之间满足独立同分布，因此它们拥有相同的均值 $\mu$ 和方差 $\sigma^2$。

此时，我们重点来研究这一组随机变量的均值：$M_n=\frac{X_1+X_2+...+X_n}{n}$，显然 $M_n$
也是一个随机变量。那么，$M_n$ 的期望和方差就是我们此时重点关心的问题。

首先，我们从期望的定义入手，来观察一下随机变量 $M_n$ 的期望 $E[M_n]$：

$E[M_n]=E[ \frac{X_1+X_2+...+X_n}{n}]$
$=\frac{1}{n}(E[X_1]+E[X_2]+...+E[X_n])$ $=\frac{1}{n}\cdot n\cdot
\mu=\mu=E[X_i]$

不难发现，一组独立同分布随机变量均值的期望就等于随机变量的期望，这个结论很直观。

下面我们再来看看 $M_n$ 的方差 $var[M_n]$：

$var[M_n]=var[ \frac{X_1+X_2+...+X_n}{n}]$
$=\frac{1}{n^2}var[X_1+X_2+...+X_n]$
$=\frac{1}{n^2}(var[X_1]+var[X_2]+...+var[X_n])$ $=\frac{1}{n^2}\cdot n\cdot
\sigma^2=\frac{\sigma^2}{n}$

我们从推导中发现，$n$ 个独立同分布随机变量的均值的方差，是单一随机变量方差的 $\frac{1}{n}$。没错，均值的方差变小了，并且随机变量 $X$
的个数 $n$ 越多，方差越小，它们的分布更加紧密地围绕在了期望的周围。

特别地，当 $n \rightarrow \infty$ 时，随机变量均值的方差趋近于 $0$：$var[M_n]=\frac{\sigma^2}{n}
\rightarrow 0$。

结合前前后后的一大段推导和论述，我们可以得出这么一个结论：

独立同分布的随机变量 $X_1,X_2,...,X_n$，它们的均值 $M_n$ 的分布会更加接近于实际分布的均值 $\mu$，随着样本量 $n$
的增大，它逐渐收敛于 $\mu$，当 $n \rightarrow \infty$
时，也就是说当样本量非常大的时候，通过抽样样本计算所得到的平均值可以说就是 $E[X]$ 了。

独立同分布的随机变量序列的样本均值，在大样本的情况下，以很大的概率与随机变量的均值非常接近。这也就是为什么说，当独立同分布的样本数量 $n$
充分大时，样本均值（频率）是概率 $P$ 的一个非常好的估计。

这就回到本篇最初提到的那个小问题了，样本数量到底是选 $100$ 还是选 $1000$，相信大家都会有明确的理论支撑了。

这里我们还简单地提两个结论，其实说起来它们都比较直观。

#### 两个重要的不等关系

谈到这里，我们来看概率统计中的两个非常重要的不等关系，以及它们背后的直观逻辑。

第一个是 **马尔科夫不等式** 。

用简洁直白的语言描述它就是：对于一个非负的随机变量
$X$，如果它的均值很小的话，那么这个随机变量取到一个大值的概率是非常小的。想想确实很直观，因为随机变量的分布大部分都集中在均值附近，越远离均值，概率就越小。

描述这个现象，有一个专门的不等式，叫做马尔科夫不等式：

$$P(|M_n-\mu| \ge \epsilon) \le \frac{\sigma^2}{n \epsilon^2}$$

第二个是 **切比雪夫不等式** 。

我们同样简单的描述一下这个不等式的内涵：

如果一个随机变量的方差非常小的话，那么这个随机变量取到远离均值 $\mu$
的概率也是非常小的，这个说实话也非常直观，同样有一个专门的不等式来描述它，叫切比雪夫不等式：

$$P(|X-\mu|\ge c) \le \frac{\sigma^2}{c^2}$$

#### 大数定理的模拟

下面我们来实际模拟一下大数定理。

在第一个模拟的例子中，我们给大家一个感性的认识：我们生成 $3$ 组各 $15000$ 个服从参数为 $(10,0.4)$
的二项分布随机变量，随机变量的期望为 $n*p=4$，然后观察随着样本数目的增大，样本均值和实际分布期望之间的关系。

**代码片段：**

    
    
    import numpy as np
    from scipy.stats import binom
    import matplotlib.pyplot as plt
    import seaborn
    seaborn.set()
    
    n = 10
    p = 0.4
    sample_size = 15000
    expected_value = n*p
    N_samples = range(1, sample_size, 10)
    
    for k in range(3):
        binom_rv = binom(n=n, p=p)
        X = binom_rv.rvs(size=sample_size)
        sample_average = [X[:i].mean() for i in N_samples]
        plt.plot(N_samples, sample_average,
                 label='average of sample {}'.format(k))
    
    plt.plot(N_samples, expected_value * np.ones_like(sample_average),
             ls='--', label='true expected value:np={}'.format(n*p), c='k')
    
    plt.legend()
    plt.show()
    

**运行结果：**

![图1.大数定理模拟](https://images.gitbook.cn/3dbc3800-97b9-11e9-b832-bd43f398eec9)

我们设置了三个相同的试验组，从试验结果中我们可以发现，在每一组试验中，随着样本数量的逐渐增大，样本均值都会越来越收敛于随机变量的期望。

接下来我们再来看看第二个用于模拟大数定理的例子，我们从大数定理的定义出发，我们先生成 $1000000$ 个服从均值为 $0$，标准差为 $20$
正态分布的样本。依次进行三种不同的处理，并观察对应的三组分布图像：

  * 图像 1：原始正态分布的样本分布图像，颜色为蓝色。
  * 图像 2：从 $1000000$ 个原始正态分布样本中，每次随机选取 $5$ 个数，计算它们的均值，重复操作 $10000$ 次，观察这 $10000$ 个均值的分布，颜色为红色。
  * 图像 3：从 $1000000$ 个原始正态分布样本中，每次随机选取 $50$ 个数，计算它们的均值，重复操作 $10000$ 次，观察这 $10000$ 个均值的分布，颜色为绿色。

**代码片段：**

    
    
    import numpy as np
    import matplotlib.pyplot as plt
    from scipy.stats import norm
    import seaborn
    seaborn.set()
    
    norm_rvs = norm(loc=0, scale=20).rvs(size=1000000)
    plt.hist(norm_rvs, normed=True, alpha=0.3, color='b', bins=100, label='original')
    
    mean_array = []
    for i in range(10000):
        sample = np.random.choice(norm_rvs, size=5, replace=False)
        mean_array.append(np.mean(sample))
    plt.hist(mean_array, normed=True, alpha=0.3, color='r', bins=100, label='sample size=5')
    
    for i in range(10000):
        sample = np.random.choice(norm_rvs, size=50, replace=False)
        mean_array.append(np.mean(sample))
    plt.hist(mean_array, normed=True, alpha=0.3, color='g', bins=100, label='sample size=50')
    
    plt.gca().axes.set_xlim(-60, 60)
    plt.legend(loc='best')
    plt.show()
    

**运行结果：**

![图2.大数定理的模拟](https://images.gitbook.cn/fb2b5310-c191-11e9-9969-976e2ac29eb2)

这个程序采样的规模比较大，可能运行的时间会比较长。

从图中我们发现，随着每次选取的样本数量的增多，样本均值分布的图像越来越向期望集中，再一次佐证了大数定理。

### 中心极限定理

#### 原理介绍

下面接着来看另外一个现象，我们还是获取随机变量序列： $X_1,X_2,...,X_n$，这 $n$ 个随机变量满足独立同分布，均值为 $\mu$，方差为
$\sigma^2$。

我们在这组随机变量序列的基础之上得到一个新的随机变量：

$$Z_n=\frac{X_1+X_2+...+X_n-n\mu}{\sqrt{n}\sigma}$$

针对随机变量 $Z_n$，我们很容易计算出：

$$E[Z_n]=E[\frac{X_1+X_2+...+X_n-n\mu}{\sqrt{n}\sigma}]$$
$$=\frac{E[X_1+X_2+...+X_n]-n\mu}{\sqrt{n}\sigma}=0$$

$$var[Z_n]=var[\frac{X_1+X_2+...+X_n-n\mu}{\sqrt{n}\sigma}]$$
$$=\frac{var[X_1]+var[X_2]+...+var[X_n]}{n\sigma^2}=1$$

即，随机变量 $Z_n$ 的期望为 $0$，方差为 $1$。

**关键** 的一点是，随着样本个数 $n$ 的增大，随机变量 $Z_n$ 的分布逐渐趋向于一个标准正态分布，当 $n \rightarrow
\infty$ 时，随机变量的分布收敛于一个标准正态分布。

更为 **重要** 的一点是，这个定理对随机变量 $X$ 的原始分布没有任何要求，非常具有一般性。

#### 中心极限定理的工程意义

实际上，中心极限定理中的随机变量 $Z_n=\frac{X_1+X_2+...+X_n-n\mu}{\sqrt{n}\sigma}$
是经过标准化处理的，如果单单只考虑 $n$ 个随机变量的和，我们很容易得到：

$S_n=X_1+X_2+...+X_n$ 的分布趋近于一个均值为 $n\mu$，方差为 $n\sigma ^2$ 的正态分布。

中心极限定理的意义在于，大量样本的独立随机因素的叠加是趋近于一个正态分布的，这一点在很多工程领域很常见也很关键。

更重要的一点是，它不需要我们去搞明白随机变量 $X$
的分布列或者概率密度函数，这往往是非常复杂的，甚至根本就无从得知，我们只需要知道它的均值和方差就可以进行后续的处理和分析了。

#### 中心极限定理的模拟

下面，我们还是来举个例子，模拟验证一下中心极限定理。我们从一个服从参数 $p=0.3$ 的几何分布中进行采样，共分三组试验，分别每次采样 $2$ 个、$5$
个、$50$ 个样本，每组试验各重复 $100000$ 次，然后按照
$Z_n=\frac{X_1+X_2+...+X_n-n\mu}{\sqrt{n}\sigma}$
进行标准化，得到三组试验对应的结果，最后对试验结果进行可视化观察。

**代码片段：**

    
    
    import numpy as np
    from scipy.stats import geom
    import matplotlib.pyplot as plt
    import seaborn
    seaborn.set()
    
    fig, ax = plt.subplots(2, 2)
    
    geom_rv = geom(p=0.3)
    geom_rvs = geom_rv.rvs(size=1000000)
    mean, var, skew, kurt = geom_rv.stats(moments='mvsk')
    ax[0, 0].hist(geom_rvs, bins=100, normed=True)
    ax[0, 0].set_title('geom distribution:p=0.3')
    n_array = [0, 2, 5, 50]
    
    for i in range(1, 4):
        Z_array = []
        n = n_array[i]
        for j in range(100000):
            sample = np.random.choice(geom_rvs, n)
            Z_array.append((sum(sample) - n * mean) / np.sqrt(n * var))
        ax[i//2, i%2].hist(Z_array, bins=100, normed=True)
        ax[i//2, i%2].set_title('n={}'.format(n))
        ax[i//2, i%2].set_xlim(-3, 3)
    
    plt.show()
    

**运行结果：**

![图3.中心极限定理的模拟](https://images.gitbook.cn/52b76980-c15a-11e9-b0c7-8fa4261376e9)

左上第一幅图是几何分布的原始图像，我们发现，随着单次采样个数的逐渐增加，随机变量
$Z_n=\frac{X_1+X_2+...+X_n-n\mu}{\sqrt{n}\sigma}$ 的分布图像越来越趋近于一个标准正态分布。

### 大数定理的应用：蒙特卡罗方法

#### 机器学习中的应用背景

用大样本数据计算出来的频率去估计概率，这就是大数定理的本质，而大数定理思想的一个非常典型的应用就是蒙特卡罗方法。

蒙特卡罗方法，又叫统计模拟方法，名字很洋气，思想很粗暴，真的很管用。它使用随机数来进行场景的模拟或者过程的仿真，其思想核心就是通过模拟出来的大量样本集或者随机过程去近似我们想要研究的实际问题对象，这是一类非常重要的数值计算方法。

该方法的名字来源于世界著名的赌城蒙特卡罗。赌博和概率，二者相视一笑、不谋而合。这种方法最初应用于 20 世纪 40
年代美国的曼哈顿原子弹计划，如今在数据分析和机器学习领域中到处都有它的身影。

以下是蒙特卡罗方法的几类非常典型的应用：

  1. 近似计算不规则面积/体积/积分
  2. 模拟随机过程，预测随机过程可能性结果的区间范围
  3. 利用马尔科夫链—蒙特卡罗方法（MCMC）进行未知参数的统计推断

要知道，蒙特卡洛方法不仅是一种方法技巧，更是一种思考问题的方式。在本篇中，我们先介绍第一个应用点，让大家进一步理解大数定理的原理和蒙特卡罗方法的要点。第二个和第三个应用我们会结合后面相应的内容依次介绍。

#### 利用蒙特卡罗方法计算不规则面积

蒙特卡罗方法可以近似计算出不规则图形的面积，特别对于那些难以用解析方法计算的图像，非常有效。

这里，我们利用蒙特卡罗方法来近似计算一个圆的面积，然后估计出 $\pi$
的近似值，选择圆作为例子的原因不是因为圆无法通过解析法进行计算，而是因为大家对它的面积比较熟悉，方便我们进行结果的对比。

**代码片段：**

    
    
    import numpy as np
    import matplotlib.pyplot as plt
    from matplotlib.patches import Circle
    from scipy.stats import uniform
    import seaborn
    seaborn.set()
    
    n = 100000
    r = 1.0
    o_x, o_y = (0., 0.)
    
    uniform_x = uniform(o_x-r,2*r).rvs(n)
    uniform_y = uniform(o_y-r,2*r).rvs(n)
    
    d_array = np.sqrt((uniform_x - o_x) ** 2 + (uniform_y - o_y) ** 2)
    res = sum(np.where(d_array < r, 1, 0))
    pi = (res / n) /(r**2) * (2*r)**2
    
    fig, ax = plt.subplots(1, 1)
    ax.plot(uniform_x, uniform_y, 'ro', markersize=0.3)
    plt.axis('equal')
    circle = Circle(xy=(o_x, o_y), radius=r, alpha=0.5)
    ax.add_patch(circle)
    
    print('pi={}'.format(pi))
    plt.show()
    

**运行结果：**

    
    
    pi=3.14096
    

![图4.蒙特卡洛方法近似计算圆面积](https://images.gitbook.cn/0287b910-9257-11e9-8507-8f1e9df27c2c)

我们结合这张图，详细地分析一下这段程序。

我们近似计算的目标就是这个蓝色的半径为 $r=1$ 的单位圆面积，而这个单位圆的外接正方形的边长为 $l=2r=2$，因此外接正方形的面积为
$4$。我们生成 $100000$ 个在外接正方形内均匀分布的点，均匀的撒下去，这里面的核心原理就是：

$$\frac{圆形面积}{正方向面积}\approx \frac{圆内点的个数}{点的总个数}$$

这样就可以估算出单位圆的面积了。

而为了估算 $\pi$，我们回到这个例子中的参数，可以得到这么一个公式：

$$\frac{\pi r^2}{(2r)^2}\approx \frac{圆内点的个数}{点的总个数}\rightarrow \pi \approx 4
* \frac{圆内点的个数}{点的总个数}$$

而且大数定理告诉我们，随着样本数量的增大，我们用这种方式模拟出来的 $\pi$
值应该是越来越趋近于真实值，样本无穷大的时候收敛于真值。这就证明了应用大数定理的蒙特卡罗方法的合理性和有效性。

至于有同学会问了，是怎么判定一个点是否位于圆当中？这里用的是距离法，也就是计算每个点到原点的距离，如果小于等于半径，就说明这个点在圆内。

这里是不是有一个 Bug？就是说这个规则只对圆这种特殊图像有效，如果是一个完全不规则的图像，我们如何来计算这个图像的面积？

我们可以借助计算机图像中的像素来进行。比如把不规则图像内部都涂黑，图像外部都留白，我们还是均匀地撒下大样本量的点，如果某个点位于的坐标，它的像素是黑色的，则证明这个点在图像内部，反之就在图像外部，利用这个方法就能统计出位于图像内部的点的个数。

### 小结

在本篇中，我们从样本均值和总体期望的关系入手，详细地讨论了概率统计中的重要概念——大数定理与中心极限定理，目的是让大家能够建立好概率统计中极限思维的概念基础。在实践环节中，我们介绍了大数定理的重要应用——蒙特卡罗方法，列举了它在机器学习中的几类重要应用，希望能有助于大家强化大数定理的思想方法。

