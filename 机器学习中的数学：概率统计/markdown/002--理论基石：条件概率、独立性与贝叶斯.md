### 从概率到条件概率

#### 条件概率的发生背景

从这一篇开始，我们就正式进入到概率统计的内容板块中了。

对于概率，相信大家都不会陌生，在各阶段的数学课上，它都是高频出现的常客，最简单的概率场景比如掷骰子：第一次掷出的点数为 $5$
的概率为多大？你会毫不犹豫的说出答案：$\frac{1}{6}$。

这太简单了。接下来我增加一个限定条件：已知在抛出骰子是奇数的情况下，抛掷点数为 $5$ 的可能性有多大？

发现了没有，在第二个问题中我就没有直接的只问投掷出 $5$ 这个事件的概率，而是增加了一个前提条件：这次抛掷出的点数为奇数。

生活中这类场景更多，我们一般不会直接去推断一个事件发生的可能性，因为这样实际意义并不明显，而且也不容易推断出结果。比如我问你今天下雨的概率是多大？你可能是一头雾水，什么地点？什么月份？当日云层的厚度？这些条件都没有提供，这样是无法给出一个有意义、有价值的合理推断的。

而且在实际情况下，一个事件一般而言也不会是孤立的发生，它会伴随着其他事情一同出现，单独谈一个事件的概率，一般而言也是不存在的。

因此，在实际的应用中，我们更关心的是条件概率，也就是在给定部分信息的基础上对试验结果的推断。这些给定的信息就是我们附加的条件，是我们研究时关注的重点。

#### 条件概率的具体描述

这里，我们来具体描述一下条件概率：

假设我们知道给定事件 $B$ 已经发生，在此基础上希望知道另一个事件 $A$ 发生的可能性，此时我们就需要构造出条件概率，它需要先顾及事件 $B$
已经发生的信息，然后再求出事件 $A$ 发生的概率。

这个条件概率描述的就是在给定事件 $B$ 发生的情况下，事件 $A$ 发生的概率，我们专门把它记作：$P(A|B)$。

那我们回到投掷骰子的问题中来，在投出奇数点数骰子的前提下，投出 $5$ 的概率有多大？奇数点数一共有 $\\{1,3,5 \\}$ 三种，其中出现 $5$
的概率是 $\frac{1}{3}$。很明显，和单独问投出点数是 $5$ 的概率计算结果是不同的。

下面我们来抽象一下条件概率的场景。

我们再回到最简单、最容易理解的情景下来看，即在古典概率的模式下来分析：假定一个试验有 $N$ 个等可能的结果，事件 $A$ 和 $B$ 分别包含 $M_1$
个和 $M_2$ 个结果，这其中有 $M_{12}$ 个结果是公共的，这就是同时发生事件 $A$ 和事件 $B$，即 $A\cap B$
事件所包含的试验结果数。

形象地描述一下上述场景，如图所示：

![图1.事件发生的描述](https://images.gitbook.cn/3049c430-ae9d-11e9-a6ea-e739aa08289b)

那我问你，单纯的发生事件 $A$ 和事件 $B$ 的概率是多少？你肯定会脱口而出，分别是 $\frac{M_1}{N}$ 和
$\frac{M_2}{N}$，那进一步到条件概率中来，已知在事件 $B$ 发生的前提条件下，事件 $A$ 发生的概率是多少？

此时，我们的整体考虑范围由最开始的 $N$ 个全部的可能结果局限到现在的 $M_2$ 个结果，即 $B$ 事件发生的结果范围，而这其中只有 $M_{12}$
个结果对应事件 $A$ 的发生，那么我们不难计算出，条件概率 $P(A|B)=\frac{M_{12}}{M_2}$。

#### 条件概率的表达式分析

为了更加深入地挖掘这里面的内涵，我们进一步对条件概率的表达式 $P(A|B)=\frac{M_{12}}{M_2}$ 进行展开：

$$P(A|B)=\frac{M_{12}}{M_2}=\frac{(M_{12}/N)}{(M_2/N)}=\frac{P(AB)}{P(B)}$$

由此，我们得到了条件概率的一般定义：$P(A|B)=\frac{P(AB)}{P(B)}$。

### 两个事件的独立性

我们在上面的例子中，进一步进行分析，我们发现事件 $A$ 的无条件概率 $P(A)$ 与其在给定事件 $B$ 发生下的条件概率 $P(A|B)$
显然是不同的，即 $P(A|B)\neq P(A)$ ，而这也是非常普遍的一种情况，这两个概率值一般都存在着差异。

其实，这反映了两个事件之间存在着一些关联，假如满足 $P(A|B)>P(A)$，则可以说事件 $B$ 的发生使得事件 $A$ 的发生可能性增大了，即事件
$B$ 促进了事件 $A$ 的发生。

但是如果 $P(A)=P(A|B)$ 呢，这种情况也是存在的，而且这是一种非常重要的情况，它意味着事件 $B$ 的发生与否对事件 $A$
发生的可能性毫无影响。这时，我们就称 $A$ , $B$ 这两个事件独立，并由条件概率的定义式进行转换可以得到：

$$P(A|B)=\frac{P(AB)}{P(B) } \Rightarrow $$ $$P(AB)=P(A|B)P(B)=P(A)P(B)$$

实际上，我们拿这个式子来刻画独立性，比单纯使用表达式 $P(A)=P(A|B)$ 要更好一些，因为 $P(AB)=P(A)P(B)$ 这个表达式不受概率
$P(B)$ 是否为 $0$ 的因素制约。

由此我们说，如果 $A$ 和 $B$ 两个事件满足 $P(AB)=P(A)P(B)$，则称事件 $A$ 和事件 $B$ 独立。

### 从条件概率到全概率公式

首先我们假设 $B_1,B_2,B_3,...,B_n$ 为有限个或无限可数个事件，它们之间两两互斥且在每次试验中至少发生其中一个，我们用图直观的表示如下：

![图2.事件两两互斥且每次实验至少发生其中一个](https://images.gitbook.cn/5721eb50-ae9d-11e9-999a-5fbc96dcc0b6)

我们用表达式描述上面这幅图的含义就是：

$B_iB_j=\phi$

$B_1+B_2+B_3...+B_n=\Omega$

现在我们接着引入另一个事件 $A$，如下图所示：

![图3.引入新的事件A](https://images.gitbook.cn/6f60bca0-ae9d-11e9-8b99-7f1df0b7d2d2)

很明显，因为 $\Omega$ 是一个必然事件（换句话说就是事件全集），因此有 $P(A)=P(A \Omega )$，进一步进行推导有：

$P(A)=P(A\Omega)=P(AB_1+AB_2+AB_3+...+AB_n)$

因为事件 $B_i,B_j$ 两两互斥，显然 $AB_1,AB_2,AB_3,...,AB_n$ 也两两互斥，因此就有：

$$P(A)=P(AB_1)+P(AB_2)+P(AB_3)+...+P(AB_n)$$

再由条件概率公式 $P(AB_i)=P(B_i)P(A|B_i)$ 进行代入，将上式转换得到：

$$P(A)=P(B_1)P(A|B_1)+P(B_2)P(A|B_2)$$ $$+...+P(B_n)P(A|B_n)$$

这就是我们最终得到的 **全概率公式** ，“全”字的意义在于：全部的概率 $P(A)$ 被分解成了许多的部分概率之和。

我们再次回过头来看看全概率公式的表达式，我们从式子里可以非常直观的发现：事件 $A$ 的概率 $P(A)$ 应该处于最小的 $P(A|B_i)$ 和最大的
$P(A|B_j)$ 之间，它不是所有条件概率 $P(A|B_k)$ 的算术平均，因为它们各自被使用的机会（即 $P(B_i)$）各不相同。因此全概率
$P(A)$ 就是各 $P(A|B_k)$ 以 $P(B_k)$ 为权的加权平均值。

全概率公式的实际价值在于，很多时候，我们直接去计算事件 $A$ 的概率是比较困难的。但是如果条件概率 $P(A|B_k)$
是已知的，或很容易被我们推导计算时，全概率公式就成了计算概率 $P(A)$ 的很好的途径。

### 聚焦贝叶斯公式

#### 贝叶斯公式概述

了解了全概率公式之后，我们可以进一步的处理条件概率的表达式，得到下面这个式子：

$$P(B_i|A)=\frac{P(AB_i)}{P(A)}=\frac{P(B_i)P(A|B_i)}{P(A)}$$

$$=\frac{P(B_i)P(A|B_i)}{P(B_1)P(A|B_1)+P(B_2)P(A|B_2)+...+P(B_n)P(A|B_n)}$$

这就是大名鼎鼎的贝叶斯公式。

这个式子你千万不要觉得它平淡无奇，觉得仅仅只是数学式子的推导和罗列。这一个公式里包含了全概率公式、条件概率、贝叶斯准则，我们来挖掘一下里面所蕴藏的最重要的内涵：

贝叶斯公式将条件概率 $P(A|B)$ 和条件概率 $P(B|A)$ 紧密的联系了起来，其最根本的数学基础就是因为
$P(A|B)P(B)=P(B|A)P(A)$，它们都等于 $P(AB)$。

那这里面具体的深刻内涵是什么呢？我们接着往下看。

#### 本质内涵：由因到果，由果推因

现实中，我们可以把事件 $A$ 看成是结果，把事件 $B_1,B_2,...,B_n$ 看成是导致这个结果的各种可能的原因。

那么，我们所介绍的全概率公式

$P(A)=P(B_1)P(A|B_1)+$ $P(B_2)P(A|B_2)+...+P(B_n)P(A|B_n)$

就是由各种原因推理出结果事件发生的概率，是 **由因到果** 。

但是，更重要、更实际的应用场景是，我们在日常生活中常常是观察到某种现象，然后去反推造成这种现象的各种原因的概率。简单点说，就是 **由果推因** 。

贝叶斯公式
$P(B_i|A)=\frac{P(AB_i)}{P(A)}=\frac{P(B_i)P(A|B_i)}{\sum_{j}{P(B_j)P(A|B_j)}}$，最终求得的就是条件概率
$P(B_i|A)$，就是在观察到结果事件 $A$ 已经发生的情况下，我们推断结果事件 $A$ 是由原因 $B_i$
造成的概率的大小，以支撑我们后续的判断。

那么我们可以说，单纯的概率 $P(B_i)$ 我们叫做 **先验概率** ，指的是在没有别的前提信息情况下的概率值，这个值一般需要借助我们的经验估计得到。

而条件概率 $P(B_i|A)$，我们把它叫做是 **后验概率**，它代表了在获得了信息 $A$ 之后 $B_i$
出现的概率，可以说后验概率是先验概率在获取了新信息之后的一种修正。

#### 贝叶斯公式的应用举例

比如，贝叶斯公式应用的一个常见例子就是 $X$ 光片的病理推断案例，在某个病人的 $X$ 光片中，医生看到了一个阴影，这就是结果事件
$A$，我们希望对造成这个结果的三种可能原因（原因 1：恶性肿瘤；原因 2：良性肿瘤；原因 3：其他原因）进行分析判断，推断分属于各个原因的概率，如图所示：

![图4.X光片阴影原因举例](https://images.gitbook.cn/a30c94c0-ae9d-11e9-8b99-7f1df0b7d2d2)

例如，我们想求出原因是恶性肿瘤的概率，也就是求条件概率：$P(B_1|A)$ 的值。

我们只要知道在这三种原因下出现阴影的概率，也就是
$P(A|B_1)$，$P(A|B_2)$，$P(A|B_3)$，以及三种原因的先验概率：$P(B_1)$，$P(B_2)$，$P(B_3)$，就能通过贝叶斯公式

$P(B_1|A)=$
$\frac{P(B_1)P(A|B_1)}{P(B_1)P(A|B_1)+P(B_2)P(A|B_2)+P(B_3)P(A|B_3)}$

求得，而上述这些需要我们知道的值，基本上都可以通过历史统计数据得到。

### 全文思路梳理

这一小节里，我们从概率到条件概率，再到全概率公式，最终聚焦到贝叶斯公式，从概念的层面一路梳理过来，目的是帮助大家迅速形成一套以条件概率为基石的认识世界的视角。理解条件概率的重要性不言而喻，这个概念将贯穿我们整个概率统计专栏体系。

### 分享交流

我们为本专栏 **付费读者** 创建了微信交流群，以方便更有针对性地讨论专栏相关的问题（入群方式请到第 3 篇末尾查看）。

