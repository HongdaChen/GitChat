从这一篇开始，我们来介绍统计推断的具体方法。

### 统计推断的两大学派

在统计领域，有两种对立的思想学派：贝叶斯学派和经典学派（也称频率学派），它们之间最重要的区别就是如何看待被估计的未知参数。贝叶斯学派的观点是将其看成是已知分布的随机变量，而经典学派的观点是将其看成未知的待估计的常量。

#### 贝叶斯统计推断

具体来说，贝叶斯推断方法是将未知参数看做是一个随机变量，它具备某种先验分布。在已知观测数据 $x$ 的基础上，可以利用贝叶斯公式来推导后验概率分布
$p_{\Theta|X}(\theta|x)$，这样就同时包含人的先验知识以及观测值 $x$ 所能提供的关于 $\theta$
的新信息。贝叶斯统计推断的内容，我们这一篇里不展开，下一篇会详细介绍。

#### 经典统计推断

而经典统计方法是将未知参数 $\theta$ 看作是一个常数，但是它是未知的，那么，这就需要去估计它了。经典统计的目标就是提出参数 $\theta$
的估计方法，并且保证其具有一定的性质。

#### 一个例子

我们举个简单的例子，比如我们要通过一个物理试验来测量某个粒子的质量，从经典学派的观点来看，虽然粒子的质量未知，但它本质上是一个确定的常数，不能将其看成是一个随机变量。而贝叶斯学派则截然不同，会将待估计的粒子质量看做是一个随机变量，并利用人们对该粒子的已有的认知给它一个先验分布，按照分布的概率模型，使其集中在某个指定的范围中。

这一篇，我们重点介绍经典统计推断当中的极大似然估计法。为了给大家一个直观的感觉，这里我先来两个例子。

### 极大似然估计法的引例

第一个例子还是盒子摸球的例子。

有两个盒子，一号盒子里面有 $100$ 个球，其中 $99$ 个是白球，$1$ 个是黑球；二号盒子里面也有 $100$ 个球，其中 $99$
个是黑球，$1$ 个是白球。

现在我告诉你，我从其中某一个盒子中随机摸出来一个球，这个球是白球，那么你说，我更有可能是从哪个盒子里摸出的这个球？

显然，你会说是一号盒子。道理很简单，因为一号盒子当中，摸出白球的概率是 $0.99$，而二号盒子摸出白球的概率是 $0.01$。显然更有可能是一号盒子了。

第二个例子也是大家熟悉的丢硬币的例子。

我有三个不均匀的硬币，其中第一个硬币抛出正面的概率是 $\frac{2}{5}$，第二个硬币抛出正面的概率是
$\frac{1}{2}$，第三个硬币抛出正面的概率是 $\frac{3}{5}$，这时我取其中一个硬币，抛了 $20$ 次，其中正面向上的次数是 $13$
次，请问我最有可能是拿的哪一个硬币？

思考的过程也很简单。

三枚硬币，抛掷 $20$ 次，$13$ 次正面向上的概率分别是：

  * 第一枚：$C_{20}^{13}(\frac{2}{5})^{13}(1-\frac{2}{5})^{20-13}=0.014563052125736147$
  * 第二枚：$C_{20}^{13}(\frac{1}{2})^{13}(1-\frac{1}{2})^{20-13}=0.0739288330078125$
  * 第三枚：$C_{20}^{13}(\frac{3}{5})^{13}(1-\frac{3}{5})^{20-13}=0.1658822656197132$

**代码片段：**

    
    
    from scipy.special import comb
    import math
    
    def get_possibility(n, head, p_head):
        return comb(n,head)*math.pow(p_head,head)*math.pow((1-p_head),(n-head))
    
    print(get_possibility(20, 13, 2/5))
    print(get_possibility(20, 13, 1/2))
    print(get_possibility(20, 13, 3/5))
    

**运行结果：**

    
    
    0.014563052125736147
    0.0739288330078125
    0.1658822656197132
    

第三枚硬币抛掷出这种结果的概率最大，我更有可能拿的第三枚硬币？这种直观的认识是正确的，这种思维方式的背后正是我们要介绍的极大似然估计法，它就是这么的简单粗暴而有效。

### 似然函数的由来

有了这个例子，下面我们开始介绍极大似然估计方法。我们重点要理解的是 **似然** 这个词，这个词听起来比较陌生。

我们首先看离散型的情形，随机变量 $X$ 的概率分布已知，但是这个分布的参数是未知的，需要我们去估计，我们把它记作是
$\theta$，好比上面抛掷硬币的试验中，硬币正面朝上的概率是未知的，需要我们去估计，那么此时 $\theta$ 就代表了这个待估计的正面向上的概率值。

随机变量 $X$ 的取值 $x_i$ 表示抛掷 $k$ 次硬币，正面向上的次数，那么这个概率就表示为：

$$P(\\{X=x_i\\})=C_{k}^{x_i}\theta^{x_i}(1-\theta)^{k-x_i}$$

这里需要注意的是，$k$ 和 $x_i$ 都是指定的、已知的，而参数 $\theta$ 是一个未知的参数。因此在这个大的背景下，抛掷 $k$ 次，其中有
$x_i$ 次向上的概率是一个关于未知参数 $\theta$ 的函数，我们把它写作是 $P(\\{X=x_i\\})=p(x_i;\theta)$。

概括地说：概率质量函数 $PMF$ 是一个关于待估参数 $\theta$ 的函数。

那么此时，我们做 $n$ 次这种实验，每次实验中，都是连续抛掷 $k$
次硬币，统计正面出现的次数，这样就能取得一系列的样本：$x_1,x_2,x_3,...,x_n$，这些样本的取值之间满足相互独立，那么这一串样本取得上述取值
$\\{X_1=x_1,X_2=x_2,X_3=x_3,...,X_n=x_n\\}$ 的联合概率为：

$$p(x_1;\theta)\cdot p(x_2;\theta)\cdot p(x_3;\theta)\cdot ...\cdot
p(x_n;\theta)$$

用连乘符号写起来就是

$$\prod_{i=1}^{n}p(x_i;\theta)$$

这是一个通用的表达式，实际上，你别看它表达式是长长的一串，实际上它的未知数就是一个 $\theta$，而其他的 $x_i$ 都是已知的样本值，因此我们说
$\theta$ 的取值，完全决定了这一连串样本取值的联合概率。

由此，我们更换一个更加有针对性的写法：

$$L(\theta)=L(x_1,x_2,x_3,...,x_n;\theta)=\prod_{i=1}^{n}p(x_i;\theta)$$

那么，$L(\theta)=L(x_1,x_2,x_3,...,x_n;\theta)$ 就是这一串已知样本值 $x_1,x_2,x_3,...,x_n$
的似然函数，它描述了取得这一串指定样本值的概率值，而这个概率值完全由未知参数 $\theta$ 决定。这就是似然函数的由来。

当然如果 $X$ 是一个连续型的随机变量，我们只要相应地把离散型的概率质量函数替换成连续型的概率密度函数即可：

$$L(\theta)=L(x_1,x_2,x_3,...,x_n;\theta)=\prod_{i=1}^{n}f(x_i;\theta)$$

### 极大似然估计的思想

显然，似然函数 $L(x_1,x_2,x_3,...,x_n;\theta)$ 指的就是随机变量 $X$ 取到指定的这一组样本值
$x_1,x_2,x_3,...,x_n$ 时的概率的大小。当未知的待估计的参数 $\theta$ 取不同的值时，计算出来的概率的值会发生变化。

例如，当 $\theta=\theta_0$ 时，似然函数 $L(x_1,x_2,x_3,...,x_n;\theta_0)$ 的取值为 $0$ 或趋近于
$0$，那么意味着，当 $\theta=\theta_0$ 时，随机变量 $X$ 取得这一组样本 $x_1,x_2,x_3,...,x_n$ 的概率为
$0$，即压根儿不可能得到这一组样本值，或可能性非常非常小，那么你肯定不会觉得参数 $\theta$ 应该能够取 $\theta_0$ 这个数。

那么当 $\theta$ 取 $\theta_1$ 和 $\theta_2$ 两种不同的值时，似然函数的值
$L(x_1,x_2,x_3,...,x_n;\theta_1) \gt L(x_1,x_2,x_3,...,x_n;\theta_2)$，意味着，当
$\theta=\theta_1$ 时，随机变量 $X$ 取得这一组指定样本的概率要更大一些，换句话说，$\theta$ 取 $\theta_1$ 比取
$\theta_2$ 有更大的可能获得这一组样本值 $x_1,x_2,x_3,...,x_n$，那么当你面对这一组已经获得的采样值，在 $\theta_1$
和 $\theta_2$ 当中二选一作为估计值的时候，倾向于选择使似然函数取值更大的估计值，就是再自然不过的了。

这里就是盒子摸球试验中，我们选择一号盒子，丢硬币试验中，我们选择第三枚硬币的原因。

那么更进一步，跳出前面几个引导例子的限制，当我们的未知参数选择的余地更大时，比如我们的未知参数 $\theta$
是对一个概率值的估计，那么它的取值范围就是一个在 $[0,1]$
之间取值的连续变量，如果是估计总体的方差，那么它的范围就是非负数，如果估计的是总体的均值，那么它的范围就是全体实数了。

此时我们要做的就是在未知参数 $\theta$ 的取值范围 $\Theta$ 中选取使得似然函数
$L(x_1,x_2,x_3,...,x_n;\theta)$ 能够取得最大值的 $\hat{\theta}$，作为未知参数的估计值，由于
$\hat{\theta}$ 使得似然函数取值达到最大，因此 $\hat{\theta}$ 就是未知参数 $\theta$ 的极大似然估计。

换句话说，未知参数 $\theta$ 取估计值 $\hat{\theta}$ 时获取到这组已知样本 $x_1,x_2,x_3,...,x_n$
的可能性比取其他任何值时都要大，在这种思维框架下，我们有什么理由不用它呢？

### 极大似然估计值的计算

那么接下来，问题就到了如何求解这个极大似然估计值了。问题转换为一个求最值的问题：

即：在给定概率模型和一组相互独立的观测样本 $x_1,x_2,x_3,...,x_n$ 的基础上，求解使得似然函数
$L(\theta)=L(x_1,x_2,x_3,...,x_n;\theta)=\prod_{i=1}^{n}p(x_i;\theta)$
取得最大值的未知参数 $\theta$ 的取值。当然，如果是连续型随机变量，则把似然函数替换成
$L(\theta)=L(x_1,x_2,x_3,...,x_n;\theta)=\prod_{i=1}^{n}f(x_i;\theta)$ 即可。

那么下面问题就变得很直接了，对似然函数求导，使得导数为 $0$ 的 $\theta$ 的取值，就是我们要找的极大似然估计值 $\hat{\theta}$。

这个连乘的函数求导数比较复杂，由于函数 $g(x)$ 和 $ln(g(x))$ 的单调性是保持一致的。因此我们可以选择把似然函数 $L(x)$ 转化为
$ln(L(x))$，这样连乘就变成了连加：

$ln(L(\theta))=ln(\prod_{i=1}^{n}p(x_i;\theta))$ $=ln(p(x_1;\theta))\cdot
ln(p(x_2;\theta))\cdot ln(p(x_3;\theta))\cdot ... \cdot ln(p(x_n;\theta))$
$=\sum_{i=1}^{n}ln(p(x_i;\theta))$

此时再对它进行求导就变得容易了，如果方程有唯一解，且是极大值点，那么我们就求得了极大似然估计值。

如果有多个未知参数需要我们去估计呢？那也好办，用上偏导数就可以了：

$ln(L(\theta_1,\theta_2,...,\theta_k))$
$=ln(\prod_{i=1}^{n}p(x_i;\theta_1,\theta_2,...,\theta_k))$
$=\sum_{i=1}^{n}ln(p(x_i;\theta_1,\theta_2,...,\theta_k))$

为了使得 $lnL$ 达到最大，我们对每一个待估计的未知参数 $\theta_i$，都去求偏导数，并建立方程组：

$ \left\\{ \begin{aligned} \frac{\partial lnL}{\partial \theta_1}=0 \\\
\frac{\partial lnL}{\partial \theta_2}=0 \\\ ......\\\ \frac{\partial
lnL}{\partial \theta_k}=0 \\\ \end{aligned} \right. $

解得这个方程组就可以了。

### 极大似然估计的例子

说了这么多的理论方法，最后我们还是来看看实际例子中的估计方法。

#### 简单案例热身

第一个例子还是抛硬币的例子，我们的硬币正反面不规则，我们想要估计它正面向上的概率 $\theta$，我们连续抛掷 $10$
次，每一次抛掷的结果形成的样本序列如下：

> 正，正，正，反，反，正，反，正，正，反

很显然，每次抛掷的过程是都是彼此独立的，并且 $X$
是一个伯努利随机变量。我们知道：$p(\\{x_i=正\\})=\theta$，$p(\\{x_i=反\\})=1-\theta$，那么这组观测数据的似然函数为：

$$L(x_1,x_2,...,x_{10};\theta)=\theta^3(1-\theta)^2\theta(1-\theta)\theta^2(1-\theta)=\theta^6(1-\theta)^4$$

将其转换为对数似然函数：

$$ln(L(x_1,x_2,...,x_{10};\theta))=ln(\theta^6(1-\theta)^4)$$
$$=6ln\theta+4ln(1-\theta)$$

然后对对数似然函数求导：

$$ln'(L(x_1,x_2,...,x_{10};\theta))=(6ln\theta+4ln(1-\theta))' $$
$$=\frac{6}{\theta}+\frac{4}{\theta-1}$$
$$=\frac{10\theta-6}{\theta(\theta-1)}$$

让对数似然函数的导数为 $0$：

$$ln'(L(x_1,x_2,...,x_{10};\theta))=\frac{10\theta-6}{\theta(\theta-1)}=0$$

得到极大似然估计值 $\hat{\theta}=\frac{6}{10}$。

#### 单参数极大似然估计

再看一个指数分布的例子。

我们在前面学习过，在一个柜台前，相邻顾客的到达时间的时间间隔服从参数为 $\theta$ 的指数分布，我们获取了一组上述时间间隔的样本
$x_1,x_2,x_3,...,x_n$，下面来运用极大似然估计的方法来估计未知参数 $\theta$。

首先依据指数分布，得到：$f(x_i;\theta)=\theta e^{-\theta x_i}$。

紧接着，得到对数似然函数：

$$ln(L(x_1,x_2,...,x_n;\theta))=ln(\prod_{i-1}^{n}\theta e^{-\theta x_i})$$
$$=\sum_{i=1}^nln\theta e^{-\theta x_i}=\sum_{i=1}^nln\theta +\sum_{i=1}^nln
e^{-\theta x_i}$$ $$=\sum_{i=1}^nln\theta -\sum_{i=1}^{n}\theta
x_i=\sum_{i=1}^nln\theta -\theta\sum_{i=1}^{n}x_i$$ $$=nln\theta
-\theta\sum_{i=1}^{n}x_i$$

此时，我们对这个对数似然函数求导，来获取未知参数的极大似然估计值 $\hat{\theta}$，也很简单：

$$ln'(L(x_1,x_2,...,x_n;\theta))=(nln\theta -\theta\sum_{i=1}^{n}x_i)'$$
$$=\frac{n}{\theta}-\sum_{i=1}^{n}x_i=0$$

则有：

$$\frac{n}{\theta}=\sum_{i=1}^{n}x_i$$

那么未知参数 $\theta$ 的极大似然估计值 $\hat{\theta}=\frac{n}{\sum_{i=1}^{n}x_i}$。

#### 多参数极大似然估计

上面是单参数的极大似然估计的例子，那么下面我们再来看看多参数的例子，这里我们从一个服从参数为 $(\mu,\sigma^2)$
的正态分布当中获取一组采样值：$x_1,x_2,x_3,...,x_n$，通过这组采样值，我们来求得两个参数的极大似然估计值：

首先写出似然函数：

$L(x_1,x_2,x_3,...,x_n;\mu, \sigma^2)$
$=\prod_{i=1}^n\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(x_i-\mu)^2}{2\sigma^2}}$
$=(2\pi\sigma^2)^{-\frac{n}{2}}e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2}$

写成对数似然函数的形式：

$ln(L(x_1,x_2,x_3,...,x_n;\mu, \sigma^2))$
$=ln((2\pi\sigma^2)^{-\frac{n}{2}}e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2})$
$=ln(2\pi\sigma^2)^{-\frac{n}{2}}+ln(e^{-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2})$
$=-\frac{n}{2}ln2\pi-\frac{n}{2}ln(\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2$

由于这里有两个待估计的参数，我们分别对其进行求偏导，$\sigma^2$ 我们把它看作是一个整体即可：

$ \left\\{ \begin{aligned} \frac{\partial(
-\frac{n}{2}ln2\pi-\frac{n}{2}ln(\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2)}{\partial
\mu}=0 \\\ \frac{\partial(
-\frac{n}{2}ln2\pi-\frac{n}{2}ln(\sigma^2)-\frac{1}{2\sigma^2}\sum_{i=1}^n(x_i-\mu)^2)}{\partial
\sigma^2}=0 \\\ \end{aligned} \right. $

第一个求偏导数的式子中，我们可以得出：

$$2\sum_{i=1}^n(x_i-\mu)=0$$

则有：$\sum_{i=1}^n \mu = \sum_{i=1}^n x_i$，即

$$n \mu=\sum_{i=1}^{n} x_i \Rightarrow \hat{\mu}=\frac{1}{n}\sum_{i=1}^nx_i$$

即得到了均值的极大似然估计值。

第二个求偏导的式子中，注意我们是把 $\sigma^2$ 看作是一个整体，因此可以得出：

$$-\frac{n}{2\sigma^2}+\frac{1}{2\sigma^4}\sum_{i=1}^n(x_i-\mu)^2=0$$

得到：$\hat{\sigma^2}=\frac{1}{n}\sum_{i=1}^n(x_i-\mu)^2$，而这里我们要代入 $\mu$ 的极大似然估计值
$\hat{\mu}$，最终得到了方差的极大似然估计值：

$$\hat{\sigma^2}=\frac{1}{n}\sum_{i=1}^n(x_i-\hat{\mu})^2$$

细心的朋友们一定发现了，总体方差的极大似然估计值的分母是 $n$ 而不是 $n-1$，因此它不是一个无偏估计量。但是可以说它是渐近无偏的，因为随着 $n$
不断增大，它和无偏估计量逐渐趋向一致。

经典统计推断当中的极大似然估计方法，我们就介绍到这里，下一篇，我们将介绍贝叶斯推断方法。

