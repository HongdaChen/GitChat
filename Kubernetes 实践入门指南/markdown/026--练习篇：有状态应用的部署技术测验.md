有状态应用对象资源主要指 StatefulSet 对象，这个通过前面几篇的介绍，相信有了比较深刻的印象。StatefulSet
对象在应用中比较特别的地方是它的 Service 名字一定是 headless 的，解析出来的 DNS 别名就可以用来编排 Pod 实例，这是
Deployment 对象不具备的容器编排能力。虽然现在大家对 StatefulSet 已经开始应用起来，但是对它的 理解仍然还是有点模糊。我们用它能来代替
Deployment 会给我们带来什么好处，都需要在本篇梳理一下，以帮助读者可以掌握这方面的知识。

### 练习 1：StatefulSet 的编排能力

有状态应用要解决的问题就是复杂的应用状态和编排问题，最彻底的办法就是采用自己的调用针对 Pod 一个一个来维护。这是可行的，但是将失去 Kubernetes
全局管理的优势。从 Kubernetes 的资源管理范式，它推出了 StatefulSet 来应对这个问题。从服务发现的角度，它是要去掉 Service
层的 IP，这层反向代理对无状态应用是无效的。为了能细粒度的编排 Pod，它给每一个 Pod 实例提供一个唯一别名，这对编排提供了实例的唯一性。

对于细粒度的编排，StatefulSet 提供了滚动更新的能力，这是一种按照序号从高位到低位的顺序更新，当失败发生时，Pod
是会停止更新的。复杂的业务不仅仅需要按照顺序的更新编排，还有一些情况需求指定部分 Pod 实例更新。所以 StatefulSet
的滚动更新策略（RollingUpdate）提供了一个参数——partition，别小看这个属性，目前很多高级特性的编排都需要靠它来实现。

比如，Patch web StatefulSet 来对 updateStrategy 字段添加一个分区。

    
    
    kubectl patch statefulset web -p '{"spec":{"updateStrategy":{"type":"RollingUpdate","rollingUpdate":{"partition":3}}}}'
    statefulset.apps/web patched
    

在此 Patch StatefulSet 来改变容器镜像：

    
    
    kubectl patch statefulset web --type='json' -p='[{"op": "replace", "path": "/spec/template/spec/containers/0/image", "value":"k8s.gcr.io/nginx-slim:0.7"}]'
    statefulset.apps/web patched
    

删除 StatefulSet 中的 Pod：

    
    
    kubectl delete po web-2
    pod "web-2" deleted
    

等待 Pod 变成 Running 和 Ready：

    
    
    kubectl get po -lapp=nginx -w
    NAME      READY     STATUS              RESTARTS   AGE
    web-0     1/1       Running             0          4m
    web-1     1/1       Running             0          4m
    web-2     0/1       ContainerCreating   0          11s
    web-2     1/1       Running   0         18s
    

获取 Pod 的容器：

    
    
    kubectl get po web-2 --template '{{range $i, $c := .spec.containers}}{{$c.image}}{{end}}'
    k8s.gcr.io/nginx-slim:0.8
    

注意这里，为什么容器显示的结果还是老容器，就是因为分区后 Pod 的序号比 updateStrategy 指定的 partition 要小。

当你掌握了这个技巧，你就可以通过 partition 参数实现灰度发布的能力。比如你之前的副本数是 3，那你可以配置 partition 为
1，这样大于等于 1 序号的 Pod 就会被自动更新为最新的容器实例。等验证没有问题了，你可以直接更新 partition 为 0， 自动更新剩余的 Pod
实例。

### 练习 2：分布式系统的编排

StatefulSet 管理的是同质 Pod 的多个副本的编排。所以很多一致性系统都是可以用有状态编排来实现。比如 ZooKeeper 系统。

配置代码如下：

    
    
    apiVersion: v1
    kind: Service
    metadata:
      name: zk-hs
      labels:
        app: zk
    spec:
      ports:
      - port: 2888
        name: server
      - port: 3888
        name: leader-election
      clusterIP: None
      selector:
        app: zk
    ---
    apiVersion: policy/v1beta1
    kind: PodDisruptionBudget
    metadata:
      name: zk-pdb
    spec:
      selector:
        matchLabels:
          app: zk
      maxUnavailable: 1
    ---
    apiVersion: apps/v1
    kind: StatefulSet
    metadata:
      name: zk
    spec:
      selector:
        matchLabels:
          app: zk
      serviceName: zk-hs
      replicas: 3
      updateStrategy:
        type: RollingUpdate
      podManagementPolicy: OrderedReady
      template:
        metadata:
          labels:
            app: zk
        spec:
          affinity:
            podAntiAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                - labelSelector:
                    matchExpressions:
                      - key: "app"
                        operator: In
                        values:
                        - zk
                  topologyKey: "kubernetes.io/hostname"
          containers:
          - name: kubernetes-zookeeper
            imagePullPolicy: Always
            image: "mirrorgooglecontainers/kubernetes-zookeeper:1.0-3.4.10"
            resources:
              requests:
                memory: "1Gi"
                cpu: "0.5"
            ports:
            - containerPort: 2181
              name: client
            - containerPort: 2888
              name: server
            - containerPort: 3888
              name: leader-election
            command:
            - sh
            - -c
            - "start-zookeeper \
              --servers=3 \
              --data_dir=/var/lib/zookeeper/data \
              --data_log_dir=/var/lib/zookeeper/data/log \
              --conf_dir=/opt/zookeeper/conf \
              --client_port=2181 \
              --election_port=3888 \
              --server_port=2888 \
              --tick_time=2000 \
              --init_limit=10 \
              --sync_limit=5 \
              --heap=512M \
              --max_client_cnxns=60 \
              --snap_retain_count=3 \
              --purge_interval=12 \
              --max_session_timeout=40000 \
              --min_session_timeout=4000 \
              --log_level=INFO"
            readinessProbe:
              exec:
                command:
                - sh
                - -c
                - "zookeeper-ready 2181"
              initialDelaySeconds: 10
              timeoutSeconds: 5
            livenessProbe:
              exec:
                command:
                - sh
                - -c
                - "zookeeper-ready 2181"
              initialDelaySeconds: 10
              timeoutSeconds: 5
            volumeMounts:
            - name: datadir
              mountPath: /var/lib/zookeeper
          securityContext:
            runAsUser: 1000
            fsGroup: 1000
      volumeClaimTemplates:
      - metadata:
          name: datadir
        spec:
          accessModes: [ "ReadWriteOnce" ]
          resources:
            requests:
              storage: 10Gi
    

看上面的安装 Yaml 文件，要创建一个 Headless Service，配置 `clusterIP: None` 生效。配置
PodDisruptionBudget 的目的是防止人为删除 Pod 导致的 ZK 不工作的情况，通过声明来限制。最后是声明创建了 3 副本的 ZK
实例。这里面的规则有几处值得讲解，第一是同质的 Pod 实例组建的集群，为了高可用架构，通过 Pod 反亲和性的能力，在每个节点上只部署一个实例。

    
    
          affinity:
            podAntiAffinity:
              requiredDuringSchedulingIgnoredDuringExecution:
                - labelSelector:
                    matchExpressions:
                      - key: "app"
                        operator: In
                        values:
                        - zk
                  topologyKey: "kubernetes.io/hostname"
    

通过挂载资源是通过 volumeClaimTemplates 来声明挂载的 PVC。当 Pod 起来的时候，直接通过 volumeMounts
直接挂载。和无状态应用事先声明的 volumes 是不一样的。

最后，有状态应用的每一个实例都是独立的单元，一定要配置 readinessProbe 和 livenessProbe
保证实例的健康性。从实践中来看，当部署的是有状态应用的时候，故障一定会发生，你的系统本身要设计成冗余并多副本的架构。这样，一个副本的损失并不会影响整个系统的响应，这就是为什么高可用系统的好处。

如果你的系统本身就不支持高可用的架构，StatefulSet 是无法帮助到应用的，反而会给你的系统带来更多复杂的配置。这也是当前为什么很多应用想上
Kubernetes 集群部署的时候感觉迷茫的地方。

很多读者肯定听过 Kubernetes 只能很好的支持无状态应用，有状态应用支持的不好。通过上面的讲解，你肯定会了解到，这里支持的好不好并不是
StatefulSet 能解决的。当你的系统确实设计为读写分离的有状态应用，即使不用 Kubernetes 也可以实现高可用架构和系统部署。采用
Kubernetes 的好处是编排的声明式工作流，可以帮助你更快速地部署和运维复杂的架构系统。这样的系统改进也是云原生系统的目标。

