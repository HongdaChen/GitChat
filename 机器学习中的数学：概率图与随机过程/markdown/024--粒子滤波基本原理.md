### 粒子滤波：更一般的情况

上一讲中我们介绍过，卡尔曼滤波是可以得到解析解的，而原因是由于 $z_t$ 和 $z_{t-1}$
之间满足线性关系，且变量服从高斯分布。我们回想一下，正是因为高斯分布的完美特性，导致了我们可以拿出滤波结果的解析解。

但是这毕竟是一种非常特殊的情况，换句话说，如果在更一般的情况下 $z_t$ 和 $z_{t-1}$
之间不满足线性关系，而是可以满足任意关系，且变量不服从高斯分布，那么会是什么样的一种情形？这就是我们这一讲开始要介绍的粒子滤波。

### 粒子滤波的关注问题

粒子滤波中，我们最关心的同样也是滤波问题（filtering），即关注 $p(z_t|x_1,x_2,...,x_t)$
的概率分布。我们回忆一下那两个步骤：

**第一步：predict** 。从贝叶斯的角度来说，实际上就是利用上一步 $t-1$ 步的滤波值，先估计出一个 $z_t$ 的先验概率：

$$p(z_t|x_1,...,x_{t-1})=\int_{z_{t-1}}p(z_t|z_{t-1})p(z_{t-1}|x_1,...,x_{t-1})dz_{t-1}$$

**第二步：update** 。在拿到 $t$ 时刻的观测变量 $x_t$，我们称之为证据之后，对上一步得到的 $z_t$
先验概率分布进行修正，得到我们要的滤波，本质上就是去求得 $z_t$ 的后验分布：

$$p(z_t|x_1,...,x_t)\propto p(x_t|z_t)p(z_t|x_1,...,x_{t-1})$$

此时的我们，无法拥有高斯分布和线性关系这么完美的假设，因此不再能够像卡尔曼滤波那样得到滤波的解析解，怎么办？我们想到了之前学习过的蒙特卡洛方法，得不到解析解，我们去求数值解。

### 利用采样法求滤波值的数值解

具体怎么弄，我们看一下重要性采样方法。

这里我们要树立一个观点，我们在 predict 和 update 两步的不断迭代过程中，都是在想着不断地求得 $z$
的概率分布，实际上分布并不是我们最终需要的结果，有了分布后，我们最终还要做一步就是，用分布中 $z$ 的期望作为我们的估计。

数值解的方法，就是直接面向最终的估计值，如果说 $z_t$ 服从概率分布 $p(z_t|x_1,...,x_t)$，那么数值解方法的最终目标就是求得变量
$z_t$ 的期望，来作为 $t$ 时刻这一步滤波的估计值。那么每一步同样都能获得滤波结果。

具体怎么做，首先第一个要用到的思想就是重要性采样方法：假如变量 $z$ 服从分布 $p(z)$，如何求得 $z$ 的期望？

按照连续型随机变量期望的定义：

$$E[z]=\int zp(z)dz$$

进一步如果依据大数定理，依据采样的方法从分布 $p(z)$ 中采出 $N$ 个样本
$z_i$，求它们的算术平均，同样可以作为期望的近似，这个我们在概率统计基础中已经多次讲过：

$$E[z]=\int zp(z)dz \approx \frac{1}{N}\sum_{i=1}^Nz^{(i)}$$

其中，$z^{(i)}$ 就是从分布 $p(z)$ 中采样出来的 $N$ 个样本，但是问题来了，如何从 $p(z)$ 中生成样本
$z^{(i)}$，这个我们并不知道啊，换句话说 $p(z)$ 是一个复杂的分布，我们没办法从中采到服从分布的样本。

重要性采样就呼之欲出了，它在求期望的过程中引入了一个建议分布 $q(z)$，这个建议分布可以是一个任意的我们熟悉的分布，它可以使得我们可以轻松地从
$q(z)$ 中生成服从 $q(z)$ 分布的一系列样本。

那这个建议分布 $q(z)$ 有啥用呢？还是回到期望的定义式中：

$$E[z]=\int zp(z)dz=\int z\frac{p(z)}{q(z)}q(z)dz$$

经过这么一个漂亮的变形，我们发现，随机变量 $z$ 的期望就可以变成下面这种形式：

$$E[z]=\int z\frac{p(z)}{q(z)}q(z)dz \approx
\frac{1}{N}\sum_{i=1}^Nz^{(i)}\frac{p(z^{(i)})}{q(z^{(i)})}$$

这时，这一组 $N$ 个生成的样本 $z^{(i)}$ 就不再是从分布 $p(z)$ 中生成的了，而是从 $q(z)$ 中生成的，$q(z)$
是我们指定的建议分布，因此用它来生成一组样本，我们还是办得到的。

这个过程中 $\frac{p(z^{(i)})}{q(z^{(i)})}$ 就称为样本 $z^{(i)}$ 的权重，记作 $w^{(i)}$，即有：

$$E[z] \approx
\frac{1}{N}\sum_{i=1}^Nz^{(i)}\frac{p(z^{(i)})}{q(z^{(i)})}=\frac{1}{N}\sum_{i=1}^Nz^{(i)}
w^{(i)}$$

好，回到粒子滤波的问题中来，我们的问题也是类似的。

在 $t$ 时刻，我们用数值解求滤波的近似结果，实际上就是求服从分布 $p(z_t|x_{1:t})$ 的变量 $z$
的期望。那么套用重要性采样方法，我们引入建议分布 $q(z_t|x_{1:t})$，那么每一轮过程中，比如在 $t$ 时刻，我们用建议分布
$q(z_t|x_{1:t})$ 生成 $N$ 个样本：

$$z_t^{(1)},z_t^{(2)},z_t^{(3)},...,z_t^{(N)}$$

然后再用权重公式 $$w_t^{(i)}=\frac{p(z_t^{(i)}|x_{1:t})}{q(z_t^{(i)}|x_{1:t})}$$

来对应地生成样本 $z_t^{(1)},z_t^{(2)},z_t^{(3)},...,z_t^{(N)}$ 所对应权重：

$$w_t^{(1)},w_t^{(2)},w_t^{(3)},...,w_t^{(N)}$$

然后通过 $\frac{1}{N}\sum_{i=1}^Nz^{(i)} w^{(i)}$ 来求得这一轮滤波值的期望。

但是问题来了，不像单纯的重要性采样中的情形，在上面的重要性采样的例子当中，我们虽然不能从 $p(z)$ 中生成样本，但是毕竟我们知道 $p(z)$
的解析式，可以求得 $p(z)$ 的值。但是粒子滤波中的时间 $t$ 中，$p(z_t^{(i)}|x_{1:t})$ 我们也求不出来，只有当 $t=1$
时，$p(z_1^{(i)}|x_1)$ 可以通过 $p(z_1^{(i)}|x_1) \propto
p(z_1^{(i)})p(x_1|z_1^{(i)})$ 获得。

到了这儿，思路就明确了，怎么求？用递推，每一轮我们都得采样 $N$ 个样本 $z$。

### 用递推法解决问题

那么在 $t-1$ 时刻，$N$ 个样本
$z_{t-1}^{(1)},z_{t-1}^{(2)},z_{t-1}^{(3)},...,z_{t-1}^{(N)}$ 所对应的权重为：

$$w_{t-1}^{(1)},w_{t-1}^{(2)},w_{t-1}^{(3)},...,w_{t-1}^{(N)}$$

而在 $t$ 时刻，$N$ 个样本$z_{t}^{(1)},z_{t}^{(2)},z_{t}^{(3)},...,z_{t}^{(N)}$
所对应的权重为：

$$w_{t}^{(1)},w_{t}^{(2)},w_{t}^{(3)},...,w_{t}^{(N)}$$

那么我们只要试图去求得两个对应位置上的样本 $z_{t-1}^{(i)}$ 和 $z_{t}^{(i)}$ 它们二者权重之间的递推关系，也就是
$w_{t-1}^{(i)}$ 和 $w_{t}^{(i)}$，从$t=1$开始，一步一步地采样 $z_t^{(i)}$，一轮一轮地迭代出
$w_{t}^{(i)}$，就能成功地实现滤波的目标。

下面我们来看具体如何求解。

这里为了简化计算，用的是 $p(z_{1:t}|x_{1:t})$ 这个概率，这是一种简化运算的假设，大家注意这点就好，那么权重的表达式就表示为：

$$w_t=\frac{p(z_{1:t}|x_{1:t})}{q(z_{1:t}|x_{1:t})}$$

我们看看如何化简这个表达式，首先看分子：

$$p(z_{1:t}|x_{1:t})=\frac{p(z_{1:t},x_{1:t})}{p(x_{1:t})}=\frac{1}{C}p(z_{1:t},x_{1:t})$$

这里是因为 $p(x_{1:t})$ 是观测变量 $x_{1:t}$ 的概率，很显然可以记作常数 $C$，接着

$$p(z_{1:t}|x_{1:t})=\frac{1}{C}p(z_{1:t},x_{1:t})=\frac{1}{C}p(x_t|z_{1:t},x_{1:t-1})p(z_{1:t},x_{1:t-1})$$$$=\frac{1}{C}p(x_t|z_{1:t},x_{1:t-1})p(z_t|z_{1:t-1},x_{1:t-1})p(z_{1:t-1},x_{1:t-1})$$$$=\frac{1}{C}p(x_t|z_{1:t},x_{1:t-1})p(z_t|z_{1:t-1},x_{1:t-1})p(z_{1:t-1}|x_{1:t-1})p(x_{1:t-1})$$

这里由观测独立性假设有：

$$p(x_t|z_{1:t},x_{1:t-1})=p(x_t|z_t)$$

由齐次马尔科夫假设有：

$$p(z_t|z_{1:t-1},x_{1:t-1})=p(z_t|z_{t-1})$$

同时，$p(x_{1:t-1})$ 也是一组观测变量的概率，记作常数 $D$。

最终化简为：

$$p(z_{1:t}|x_{1:t})=\frac{D}{C}p(x_t|z_t)p(z_t|z_{t-1})p(z_{1:t-1}|x_{1:t-1})$$

再看看分母 $q$ 分布该如何化简，先按条件概率公式转化一下，

$$q(z_{1:t}|x_{1:t})=q(z_t|z_{1:t-1},x_{1:t})q(z_{1:t-1}|x_{1:t})$$

然后这里有一个小技巧，那就是时刻 $t$ 以前的隐含变量的取值显然不受$t$时刻观测变量 $x_t$ 取值的影响，因此就有：

$$q(z_{1:t-1}|x_{1:t})=q(z_{1:t-1}|x_{1:t-1})$$

那么：

$$q(z_{1:t}|x_{1:t})=q(z_t|z_{1:t-1},x_{1:t})q(z_{1:t-1}|x_{1:t-1})$$

那么此时我们权重 $w_t$ 的表达式就能够完整地写出来了：

$$w_t=\frac{p(z_{1:t}|x_{1:t})}{q(z_{1:t}|x_{1:t})}\propto
\frac{p(x_t|z_t)p(z_t|z_{t-1})p(z_{1:t-1}|x_{1:t-1})}{q(z_t|z_{1:t-1},x_{1:t})q(z_{1:t-1}|x_{1:t-1})}$$

我们欣喜地发现，等式的最后一部分就是 $w_{t-1}$ 的表达式：

$$\frac{p(z_{1:t-1}|x_{1:t-1})}{q(z_{1:t-1}|x_{1:t-1})}=w_{t-1}$$

$$w_t\propto
\frac{p(x_t|z_t)p(z_t|z_{t-1})}{q(z_t|z_{1:t-1},x_{1:t})}w_{t-1}$$

而此时我们还需要补充一点的就是，由于 $q$ 分布是我们的提议分布，意味着我们可以选择任意我们觉得方便的分布，那么我们让分布 $q$ 和分布 $p$
取得一样，那么就有：

$$q(z_t|z_{1:t-1},x_{1:t})=p(z_t|z_{1:t-1},x_{1:t})=p(z_t|z_{t-1})$$

最后一步转化是因为齐次马尔科夫性质决定的，那么 $w_t$ 的递推关系就瞬间简单了：

$$w_t\propto
\frac{p(x_t|z_t)p(z_t|z_{t-1})}{q(z_t|z_{1:t-1},x_{1:t})}w_{t-1}=\frac{p(x_t|z_t)p(z_t|z_{t-1})}{p(z_t|z_{t-1})}w_{t-1}=p(x_t|z_t)w_{t-1}$$

$$\Rightarrow w_t\propto p(x_t|z_t)w_{t-1}$$

而我们发现 $p(x_t|z_t)$ 反映的是 $t$ 时刻隐变量 $z_t$ 到观测变量 $x_t$ 之间的转移概率，这个是模型已知的，由此 $t$
时刻和 $t-1$ 时刻的权重递推关系就出来了。

有了这个递推关系，具体应该如何进行滤波的过程，我们在下一讲中详细介绍。

