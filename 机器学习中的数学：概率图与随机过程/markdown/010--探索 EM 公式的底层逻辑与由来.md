### EM 公式中的 E 步和 M 步

上一讲，我们是直接拿出了 EM 的迭代公式，通过证明了它的收敛性，验证了这种迭代法求解参数 $\theta$ 的合理性。那么这一讲，我们来彻底搞清楚为什么
EM 公式被设计成这个样子。

这里我们定义一些符号：

  * $x$：观测数据，observed data
  * $z$：非观测数据，也就是隐变量，latent variable
  * $(x,z)$：完整数据，complete data
  * $\theta$：待估计的参数

我们这里要说明的是参数 $\theta$ 的迭代公式是怎么来的：

$$\theta^{(t+1)}=argmax_{\theta}\int_{z}log~p(x,z|\theta)~p(z|x,\theta^{(t)})dz$$

EM 算法，包含了所谓的 E 步和 M 步。

所谓的 E 步，就是求积分，求的是 $log~p(x,z|\theta)$ 关于条件概率 $p(z|x,\theta^{(t)})$ 的期望：

$$E_{z|x,\theta^{(t)}}[log~p(x,z|\theta)]=\int_{z}log~p(x,z|\theta)~p(z|x,\theta^{(t)})dz$$

所谓的 M 步，则是获取令这个期望取得最大值的 $\theta$ 值，并作为下一轮的迭代值 $\theta^{t+1}$。

### 剖析 EM 算法的由来

下面我们来深入剖析一下这个迭代式是怎么来的。

我们最初的极大似然估计的目标就是：

$$\theta_{mle}=argmax~log(x|\theta)$$

我们就从 $log(x|\theta)$ 这个式子入手做一些变换。

根据贝叶斯定理：

$$p(x|\theta)p(z|x,\theta)=p(x,z|\theta)\Rightarrow
p(x|\theta)=\frac{p(x,z|\theta)}{p(z|x,\theta)}$$

那么有：

$$log~p(x|\theta)=log~p(x,z|\theta)-log~p(z|x,\theta)$$

我们引入一个关于 $z$ 的分布 $q(z)$，这个 $q(z)$ 具体是什么，现在先不管，放入式子中：

$$log~p(x|\theta)=log\frac
{p(x,z|\theta)}{q(z)}-log\frac{p(z|x,\theta)}{q(z)}$$

此时，我们对等式左右两边同时求关于分布 $q(z)$ 的期望，也就是求积分。

先看左边：

$$\int_{z}q(z)log~p(x|\theta)dz=log~p(x|\theta)\int_{z}q(z)dz$$

这里有两点很重要。

第一，$p(x|\theta)$ 和 $z$ 无关，因此可以拿到积分符号外面。

第二，$\int_{z}q(z)dz=1$，因此最终左边等于：

$$\int_{z}q(z)log~p(x|\theta)dz=log~p(x|\theta)\int_{z}q(z)dz=log~p(x|\theta)$$

因此，两边同时积分之后的等式就如下所示：

$$log~p(x|\theta)=\int_{z}q(z)log\frac{p(x,z|\theta)}{q(z)}dz-\int_{z}q(z)log\frac{p(z|x,\theta)}{q(z)}dz\\\=\int_{z}q(z)log~\frac{p(x,z|\theta)}{q(z)}dz+\int_{z}log~q(z)\frac{q(z)}{p(z|x,\theta)}dz$$

左边的式子，我们将其称之为——证据下界 Evidence lower bound （ELBO）：

$$ELBO:\int_{z}log~q(z)\frac{p(x,z|\theta)}{q(z)}dz$$

右边是一个 KL 散度的定义式：

$$\int_{z}log~q(z)\frac{q(z)}{p(z|x,\theta)}dz=KL(q(z)||p(z|x,\theta))$$

这里的 KL 散度描述的是 $q(z)$ 和 $p(z|x,\theta)$ 两个分布之间的差异性，它有一个性质：

$$KL(q(z)||p(z|x,\theta))\ge 0$$

当且仅当：

$$q(z)=p(z|x,\theta)时,KL(q(z)||p(z|x,\theta))=0$$

因此，整个式子的表达被转化为：

$$log~p(x|\theta)=ELBO+KL(q(z)||p(z|x,\theta))\ge ELBO$$

这也就是为什么称作是证据下界的原因，它成了 $log~p(x|\theta)$ 取值的下边界，那么就有了一个思路：我们想办法让 ELBO
达到最大，然后间接地让 $log~p(x|\theta)$ 达到最大，即我们通过用 ELBO 来等效的迭代控制
$log~p(x|\theta)$，但是问题来了，一般情况下 $log~p(x|\theta)$ 和 ELBO 并不相等，并不等效，因为还有 KL
散度该咋办？

这里就要充分利用 KL 散度性质，在每轮迭代的时候把它给弄没掉。

大家注意处理技巧，在第 $t$ 轮迭代的时候（已知 $\theta^{(t)}$，估计 $\theta^{(t+1)}$），我们把 KL 散度中概率
$p(z|x,\theta)$ 中的 $\theta$ 变量给固定住，让 $\theta=\theta^{(t)}$，同时让
$q(z)=p(z|x,\theta^{(t)})$，这样在每一轮迭代的时候 KL 散度就能做到恰好为 0，ELBO 和 $log~p(x|\theta)$
做到等效，我们求取 ELBO 的本轮的极大值，等效地就求取了 $log~p(x|\theta)$ 的极大值。

$$\theta^{(t+1)}_{mle}=argmax~log~p(x|\theta)\\\=argmax_{\theta}[\int_{z}q(z)log~\frac{p(x,z|\theta)}{q(z)}dz+\int_{z}log~q(z)\frac{q(z)}{p(z|x,\theta^{(t)})}dz]$$

我们让分布 $q(z)=p(z|x,\theta^{(t)})$，KL 散度为 0，则实现 ELBO 和似然函数的等效：

$$\theta^{(t+1)}_{mle}=argmax~log~p(x|\theta)\\\=argmax_{\theta}[\int_{z}p(z|x,\theta^{(t)})log~\frac{p(x,z|\theta)}{p(z|x,\theta^{(t)})}dz+0]\\\=argmax_{\theta}\int_{z}[p(z|x,\theta^{(t)})log~p(x,z|\theta)-p(z|x,\theta^{(t)})p(z|x,\theta^{(t)})]dz$$

而 $p(z|x,\theta^{(t)})p(z|x,\theta^{(t)})$ 中，只有常数 $\theta^{(t)}$，已经不包含变量
$\theta$，因此在求式子最大值的时候可以去掉，最终得到：

$$\theta^{(t+1)}_{mle}=argmax_{\theta}\int_{z}p(z|x,\theta^{(t)})log~p(x,z|\theta)dz$$

OK，我们最终得到了最开始的那个迭代式，这一讲我们花了点功夫，把 EM 公式是怎么来的，给大家讲明白了。

