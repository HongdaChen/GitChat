字典有个很严重的问题，当然不同语言实现可能不一样，但起码对 Go 来说字典有个很大的麻烦。把数据填入以后，你把它删除以后它不会收缩内存。

    
    
    const max = 10000000
    
    func one() map[int]int {
        // 创建字典
        m := make(map[int]int)
    
        //写入100w数据
        for i := 0; i < max; i++ {
            m[i] = i
        }
    
        //把所有数据全部删掉
        for i := 0; i < max; i++ {
            delete(m, i)
        }
        return m
    }
    
    func ten() []map[int]int {
        var ms []map[int]int
        cnt := 100
        for i := 0; i < max; i++ {
            ms[i%cnt][i] = i
        }
        for i := 0; i < max; i++ {
            delete(ms[i%cnt], i)
        }
        return ms
    }
    
    func main() {
        m := one()
        // m:= ten()
    
        for i := 0; i < 100; i++ {
            runtime.GC()
            time.Sleep(time.Second)
        }
        runtime.KeepAlive(m)
    }
    
    
    
    $ go build && GODEBUG=gctrace=1 ./test
    

字典只会扩容，不会收缩。创建一个字典，插入一批数据，接下来把数据全部删掉。我们可以看到内存一直没有回收。这样频繁地在字典里插入数据删除，会导致内存开销，会很麻烦。

很多人觉得，删除之后是不是内存就释放掉？容器类的数据结构有几种释放方式，第一种是完整释放，全部删掉之后就把那块内存完整释放掉；第二种是块释放，连续删掉 50
个就把 50 个的内存释放掉，每 50
个释放一次，因为每删一次释放一次性能会很差，就做批操作。很多语言里对这些管理策略有一个很严重的问题，可能不会告诉你删除内存根本没有释放掉。

### 使用多个 map 构成分片，减少单个 map 扩展问题，同时减少 lock

我们会注意到内存根本不会释放，它会一直持有。虽然没有数据了，但是它会一直占用。这给我们一个教训，当你使用字典频繁地进行删除和添加操作，或者说删除的量非常大的时候，一定需要搞清楚内存消耗是否可以接受。

有两种可能，第一种，比如灌 100w 个，接下来删除 50w，接下来又添加
50w，这时候内存占着接下来还能用，这个时候无所谓；还有另外一种情况是，很多时候灌入一批原始数据，接下来加工之后只留下 50w 行，也就意味这另外 50w
行内存不会被释放，白白浪费在那。

对于 Go
来说，字典有几个问题，第一对并发的写是有限制的，因为并发写会导致内存模型的崩溃，就是安全问题，有数据竞争效应，就是说读和写是互斥的，写操作必须要独占这个字典，否则可能会导致这个字典崩溃。因为我们往里面写可能会引发扩容操作，引发扩容操作时候可能需要重新哈希，那么两个写操作可能会引发数据竞争效应。这么一来对大数据字典来说，在大量读写操作情况下它的性能不是你想的那么好，也就是原来哈希表所带来的性能问题反而成为累赘。

我们创建一个字典，往里面加入 100w，接下来删除 50w，内存不会释放。这种时候合理的做法是把 100w
的字典不删除，遍历它的时候把我需要留下来的数据保存一个新字典里面去，新字典只存
50w，然后把原来整个字典释放掉，这样一来新的字典数据的内存就是紧凑的。这时候不适合在原来字典上操作。除非整个字典释放掉，这就是设计上一些问题。换句话说，这种
map 设计不适合处理海量数据。

因为很多人对这个 map 不做封装不做置换，很简单地用 map 做 cache
等很多东西，做很多缓存容器。我们知道缓存容器对数据删除会很多，有时候遇到数据清洗，有没有真实测试过内存真实使用率到底是多少，比如一共消耗了 1G
内存，实际上真正使用的有效内存只有 200M，其他 800M
内存一直被占用着。所以有些时候不管用什么语言，我们应该完整测试这种数据容器它存在着哪些官方文档根本没有说的行为。

那么不再开发新的容器的情况下，我们应该怎么做，大家都在用各种各样的 NoSQL 数据库，NoSQL
数据库有很典型的做法是把读写分离，把写分散到多台服务器上。这个时候有个简单的做法是，100w 数据分配到 10 个字典中去，每个字典存 10w
个，每个字典上有个单独的锁，这样一来就是在分布式上很常见的分片机制。好处是两个写操作可以并发，读操作可以分离掉。

还有另外一点，解决在一个字典删除 50w 和在每个字典删除 5w 个的扩容倍率是不一样的，一个是按照 50w 扩容，一个是按照 5w
扩容，所以在内存占用量上是有差异的。可能差异不是特别明显，重要的是选择分片结构是有几个好处。

  * 第一个，操作分离，分离到多个切片上去；
  * 第二个，锁分散掉，从一个锁变成 10 个锁，这里并发效应肯定会有很大差异的；
  * 第三个，我们可以把其中一个字典临时阻塞一下，比如大量删除，总量 10w，但是真实有效数据只有 5k，这种空间利用率就会很差，我们可能会把一个字典临时阻塞了，把它的数据倒换到新的字典里面去，然后替换掉原来字典，阻塞其中一个，其他 9 个还能继续工作呢，只是访问这个字典的操作临时阻塞了一下，而且数据量少，只有 5k，阻塞的时间很短。

但是上面想置换新的字典的时候，所有的操作全阻塞在这，而且遍历的范围非常大，就会造成数据密集型效应非常明显。这是很典型的数据结构设计方式。用更多的分片来实现读写分离，多写并发，减少锁的大小，然后对某个数据进行清理时候，可以保证每次清理的范围可以缩小。这是很常见的做法，把大的容器换成小的。

那么 ten() 函数优化创建 100 个字典，利用取模的方式把数据平均分配到 100 个字典中去，然后把 100 个字典数据全部删掉。

我们发现内存使用量是在下降的，虽然对象数量多了但是内存使用量下降了。归根结底 10w 字典扩容倍率和 100w 字典扩容倍率是不一样的。这种做法在很多
NoSQL 数据库都会有，比如 MongoDB
可以部署很多不同的节点，读写分离到不同节点里面去；另外可以做到两个节点数据进行相互备份，相互备份的好处是，删除数据阻塞时候把所有的读操作引到备份节点中去，读操作并不受影响。

这里除了取模以外，还可以用一致性哈希构成哈希环。用一致性哈希来确保每次的变化都存储到很小的片段上去。

比如程序启动请求比较多，把字典撑得非常大。接下来慢慢平稳以后，字典存储的数据比较少会有大量的资源浪费。第一种方式创建新的字典，把原来字典逐步迁移到新的字典中，然后把原来字典释放掉；第二种方式是使用分布式结构，建立多个多级字典，通过哈希算法把所有的请求分散到多级字典，每一个字典的扩容相对来说不会那么可怕。对单个字典的迁移的数据量会少很多。

假设对某一个字典有频繁的删除，用一个新字典替代，把新字典挂到前面，如果 Key 命中，新字典没有肯定在老字典里，把这个数据同步到新的字典里，类似 map
分批迁移算法，慢慢的把老数据全部迁移完把老字典释放掉。在迁移的过程中，用两种方式，随机迁移和顺序迁移结合，随机迁移就是访问哪个 Key 迁移哪个
Key，每次访问迁移一次，把数据慢慢转移到新字典里。

分布式设计有很多种做法可以把数据结构设计完善，同时节省内存占用。当数据量到达一定规模的时候，可能需要把字典重新封装成在一个进程内的分布式字典。map
内部就是基于桶的概念分布减少内存占用。

