Natural language processing (NLP) 即自然语言处理，它关注的是如何开发实际应用来促进计算机与人类之间的交互语言。
**典型应用包括语音识别，口语理解，对话系统，词汇分析，解析，机器翻译，知识图谱，信息检索，问答系统，情感分析，社交计算，自然语言生成和自然语言总结。**

在过去的五十年里，NLP 经历了三次浪潮，理性主义，经验主义，还有现在的深度学习。

在传统的机器学习中，需要我们做大量的特征工程，很多时候需要大量的专业知识，这就形成了一个瓶颈，
而且浅层模型也缺乏表示能力，缺乏抽象能力，在对观察到的数据进行建模时，无法识别复杂的因素。 而
**深度学习却可以通过使用深度多层的模型结构和端到端的学习算法来克服上述困难** 。

深度学习的革命有五个基本支柱：

  1. 通过嵌入实现语言实体的分布式表示；
  2. 得益于嵌入的语义泛化；
  3. 自然语言的长跨深度序列建模；
  4. 从低到高有效表示语言水平的分层网络；
  5. 可以共同解决许多NLP任务的端到端的深度学习方法。

本文将主要介绍一下深度学习在这几个典型场景的应用： **会话语言理解，对话系统，知识图谱，机器翻译，问答系统，情感分析还有视觉字幕**
。各领域会概括介绍一些先进的研究和模型，可以对 DL 在 NLP
的应用现状有个全局观，大家如果对某个领域特别感兴趣还可以着重去以此文为线索去读这个领域的相关论文。

* * *

### 1\. 会话语言理解

**会话语言理解 (Conversational Language Understanding)**
是要从口语或文本形式的自然对话中提取出含义，这样可以方便用户只需要使用自然的语言给机器下达指令就能执行某些任务。它是语音助理系统的一个重要组成部分，例如
Google Assistant, Amazon Alexa, Microsoft Cortana, Apple
Siri，这些助理系统可以帮助我们做很多事情，创建日历，安排日程，预订餐厅等等。

以目标为导向的会话语言理解 **主要包括 3 个任务** ；

  * 领域分类（domain classification）：要识别用户在谈什么方面的话题，例如旅行，
  * 意图分类（intent determination）：要判断用户想要干什么，例如想要订宾馆，
  * 语义槽填充（slot filling）：要知道这个目标的参数是什么，例如预订宾馆的日期，房型，地理位置等。

#### 1\. 领域分类和意图确定

Hinton 等在2006年提出的 deep belief networks 率先将深度学习用于话语分类，并在信息处理应用的各个领域中普及。
后来流行的技术是使用卷积神经网络 CNN 及其变体，例如 [(Kalchbrenner et al.
2014)](https://arxiv.org/pdf/1404.2188.pdf)。 随后 [(Lee 和 Dernoncourt
2016)](https://arxiv.org/pdf/1603.03827.pdf) 尝试使用 循环神经网络 RNN 来处理这类任务，并与 CNN
结合，同时利用 RNN 和 CNN 的优点。

#### 2\. 语义槽填充

对于这个任务，比较先进的算法是基于 RNN 及其变种的。

[(Dupont et al. 2017)](https://arxiv.org/pdf/1706.01740.pdf) 提出了一种新的 RNN
变体结构，其中输出标签也被连接到下一个输入中。

也有很多是基于双向 LSTM / GRU 模型的，[(Vukotic et al.
2016)](https://hal.inria.fr/hal-01351733/document)。

[(Liu and Lane 2016)](https://arxiv.org/pdf/1609.01454.pdf) 将 encoder–decoder
模型用于此任务，[(Chen et al.
2016)](https://www.csie.ntu.edu.tw/%7Eyvchen/doc/IS16_ContextualSLU.pdf)
使用了记忆网络 memory。[(Zhu and Yu 2016b)](https://arxiv.org/pdf/1608.02097.pdf)
应用了焦点注意力机制。

#### 3\. 理解上下文

自然语言理解还有一个重要的任务是理解上下文。 [(Hori et al.
2014)](https://pdfs.semanticscholar.org/54e2/c1748215a47681a9b127ef1e4a0d70bf493d.pdf)
提出了使用 role-based LSTM 层，可以有效地理解口语中上下文。 [(Chen et al.
2016)](https://www.csie.ntu.edu.tw/%7Eyvchen/doc/IS16_ContextualSLU.pdf)
提出了一个基于端到端神经网络的对话理解模型，用记忆网络来提取先验信息，作为编码器的上下文知识，进而理解对话的含义。

![](https://images.gitbook.cn/15764878512970)

* * *

### 2\. 口语对话系统

**口语对话系统（SDS：Spoken Dialog Systems）** 被认为是虚拟个人助理（VPA：virtual personal
assistants）的大脑，也就是我们熟知的聊天机器人，现在应用非常广泛，从客服到娱乐随处可见。 Microsoft 的 Cortana，Apple 的
Siri，Amazon 的 Alexa，Google 的 Home，和 Facebook 的 M 都集成了 SDS 模块，这样
**用户可以通过很自然的语言与虚拟助理交互，就能高效的完成任务** 。

**经典的口语对话系统包含多个模块** ，自动语音识别，语言理解，对话管理器，自然语言生成器。目前深度学习技术已被用于模拟其中几乎所有组件。

#### 1\. 语言理解

这一模块在上一章已经介绍了一些先进研究成果。

#### 2\. 对话状态跟踪器（Dialog State Tracker）

它是通过对话过程来跟踪系统对用户目标的信任状态。最先进的对话管理器就是通过状态跟踪模型来监控对话进度的。

[(Mrkšic ́ et al. 2016)](https://arxiv.org/pdf/1606.03777.pdf)
的神经对话管理器还提供了话语，时隙值对和知识图谱之间的联合表示，可以在较大的对话域中部署对话系统。

#### 3\. 深度对话管理器（Deep Dialog Manager）

可以让用户用很自然的方式进行交互。它负责对话的状态和流程，并且决定着应该使用什么策略。

对于复杂的对话系统，通常不可能先验地指定出来好的策略，并且环境还会随时变化， 所以 [(Singh et al.
2016)](https://papers.nips.cc/paper/1775-reinforcement-learning-for-spoken-
dialogue-systems.pdf); [(Fatemi et al.
2016b)](https://arxiv.org/pdf/1606.03152.pdf) 通过强化学习学出在线交互策略。

#### 4\. 基于模型的用户模拟器(Model-Based User Simulators)

用于生成人工交互对话。

[(Crook 和 Marin 2017)](https://www.microsoft.com/en-us/research/wp-
content/uploads/2017/09/seq2seq_userSim_2017June5.pdf) 研究了基于上下文的 sequence-to-
sequence 方法，可以产生像人类间说话一样水平的对话，超过了其他基线模型的表现。

#### 5\. 自然语言生成（NLG: Natural Language Generation）

是给一个含义来生成文本。

[(Vinyals 和 Le 2015)](https://arxiv.org/pdf/1506.05869.pdf)
关于神经对话模型的研究开辟了新篇章，使用基于 encoder–decoder 的模型进行文本生成，他们的模型有两个
LSTM，一个用于将输入句子编码为“思想向量”，另一个用于将该向量解码为答复语句，不过这个模型只能提供简短的问题答案。 后来 [(Williams 和
Zweig 2016b)](https://arxiv.org/pdf/1606.01269.pdf) 等学者开始重点研究如何使用强化学习来探索文本生成。

![](https://images.gitbook.cn/15764878512986)

* * *

### 3\. 知识图谱

**知识图谱**
也称为知识库，是一种重要的数据集，以结构化的形式组织了实体，实体属性和不同实体之间语义关系的丰富知识，是自然语言理解的基础资源，在人工智能的许多应用中发挥重要作用，如搜索，问答，语音识别等。

目前几个应用比较广泛的知识图谱是， **Freebase，DBpedia，Wikidata，YAGO，HowNet** 。

**基于深度学习的知识图谱技术有三大类：**

  1. 知识表示技术，用来将图谱中的实体和关系嵌入到密集低维的语义空间中。
  2. 关系提取技术，从文本中提取关系，用于构建图谱。
  3. 实体链接技术，将图谱与文本数据联系起来，可以用于许多任务中。

#### 1\. 知识表示技术

[(Bordes et al.
2013)](https://www.utc.fr/%7Ebordesan/dokuwiki/_media/en/transe_nips13.pdf) 的
TransE 是一种典型的基于翻译的知识表示学习方法，用它来学习实体和关系的低维向量简单有效。

但大多数现有知识表示学习方法仅关注知识图谱中的结构信息，而没有处理其他丰富的多源信息，如文本信息，类型信息和视觉信息。 [(Xie et al.
2016b)](https://dl.acm.org/citation.cfm?id=3016100.3016273) 提出根据 CBOW 或 CNN
编码器的描述学习实体表示。 [(Xie et al.
2016c)](https://www.ijcai.org/Proceedings/16/Papers/421.pdf)
通过构造投影矩阵，利用分层类型结构增强了 TransR。 [Xie et al.
2016a)](https://arxiv.org/pdf/1609.07028.pdf)
提出了体现图像的知识表示，通过用相应的图来学习实体表示，进而考虑视觉信息。

#### 2\. 关系提取技术

关系提取是要从文本中自动地提取出关系。神经网络在关系提取上主要有两个任务，句子级和文档级。

其中句子级关系提取是要预测一个句子中实体对之间的语义关系。 主要由三个部分组成：1. 输入编码器，用于表示输入的词。2.
句子编码器，将原始句子表示为单个向量或向量序列。3. 关系分类器，计算所有关系的条件概率分布。

[(Zeng et al. 2014)](http://www.aclweb.org/anthology/C14-1220) 用一个 CNN
对输入句子作嵌入，其中卷积层可以提取局部特征，再用 max-pooling 将所有局部特征组合起来，最后获得固定大小的输入句子的向量。

[(Zhang and Wang 2015)](https://arxiv.org/pdf/1508.01006.pdf) 用 RNN
嵌入句子，这样可以学习出时间特征。

#### 3\. 实体链接技术

实体链接任务有一个问题是名称歧义， 所以关键的挑战是如何有效使用上下文来计算名称与实体之间兼容度。
在实体链接中，神经网络主要用来表示异构的上下文证据，例如名称提及表示，上下文提及表示和实体描述。

[(Francis-Landau et al. 2016)](http://www.aclweb.org/anthology/N16-1150) 用 CNN
将名称提及表示，局部上下文，源文档，实体标题和实体描述投影到同一个连续特征空间，而且不同证据之间的语义相互作用被建模为它们之间的相似性，不仅可以考虑到单词的重要性和位置影响，还可用于实体链接中的文档表示

![](https://images.gitbook.cn/15764878513002)

