å‰ä¸¤ç¯‡æˆ‘ä»¬è®²äº† BERT çš„åŸç†å’Œåº”ç”¨ï¼Œè°·æ­Œåœ¨å‘å¸ƒ BERT æ—¶å°±å¼•èµ·äº†ä¸å°çš„è½°åŠ¨ï¼Œå› ä¸ºå½“æ—¶ BERT åœ¨ 11 é¡¹ NLP
ä»»åŠ¡æµ‹è¯•ä¸­åˆ·æ–°äº†å½“æ—¶çš„æœ€é«˜æˆç»©ï¼Œè¿™ä¸ªéœ‡æ’¼è¿˜æœªå¹³æ¯ï¼ŒCMU ä¸è°·æ­Œå¤§è„‘æå‡ºçš„ XLNet åˆæ€èµ·ä¸€é˜µé«˜æ½®ï¼Œå®ƒåœ¨ 20 ä¸ª NLP ä»»åŠ¡ä¸Šè¶…è¿‡äº† BERT
çš„è¡¨ç°ï¼Œå°¤å…¶æ˜¯åœ¨éš¾åº¦å¾ˆå¤§çš„å¤§å‹ QA ä»»åŠ¡ RACE ä¸Šä¹Ÿè¶³è¶³è¶…è¶Š BERT æ¨¡å‹ 6~9 ä¸ªç™¾åˆ†ç‚¹ï¼Œåœ¨å…¶ä¸­ 18
ä¸ªä»»åŠ¡ä¸Šéƒ½å–å¾—äº†å½“æ—¶æœ€ä½³æ•ˆæœã€‚ä»Šå¤©æˆ‘ä»¬å°±æ¥çœ‹çœ‹ XLNet çš„åŸç†å’Œåº”ç”¨ã€‚

* * *

å‰é¢çš„è¯¾ç¨‹ä¸­æˆ‘ä»¬æœ‰è®²è¿‡è¯­è¨€æ¨¡å‹ï¼Œå³æ ¹æ®ä¸Šæ–‡å†…å®¹é¢„æµ‹ä¸‹ä¸€ä¸ªå¯èƒ½çš„å•è¯ï¼Œè¿™ç§ç±»å‹çš„è¯­è¨€æ¨¡å‹ä¹Ÿè¢«ç§°ä¸º **è‡ªå›å½’è¯­è¨€æ¨¡å‹** ã€‚ä¾‹å¦‚ GPT
æ˜¯å…¸å‹çš„è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼Œæ­¤å¤– ELMo æœ¬è´¨ä¸Šä¹Ÿæ˜¯è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼Œè™½ç„¶å®ƒä½¿ç”¨äº†åŒå‘ LSTMï¼Œä½†å…¶å®åœ¨æ¯ä¸ªæ–¹å‘ä¸Šéƒ½æ˜¯ä¸€ä¸ªå•å‘çš„è‡ªå›å½’è¯­è¨€æ¨¡å‹ï¼Œä¸¤ä¸ªæ–¹å‘ä¸Šçš„
LSTM çš„è®­ç»ƒè¿‡ç¨‹å…¶å®æ˜¯ç‹¬ç«‹çš„ï¼Œåªæ˜¯æœ€åå°†ä¸¤ä¸ªæ–¹å‘çš„éšèŠ‚ç‚¹çŠ¶æ€æ‹¼æ¥åˆ°ä¸€èµ·ã€‚

![](https://images.gitbook.cn/3a0ef4b0-a0a8-11ea-bf38-950ba54cfedc)

å¦å¤–è¿˜æœ‰ä¸€ç§è¯­è¨€æ¨¡å‹å«åš **è‡ªç¼–ç è¯­è¨€æ¨¡å‹** ï¼ŒBERT å°±æ˜¯å…¸å‹çš„ä»£è¡¨ï¼Œå®ƒåœ¨é¢„è®­ç»ƒæ—¶éšæœºå°†æ‰€æœ‰å¥å­ä¸­ 15% çš„ token ç”¨ <Mask>
æ¥æ›¿ä»£ï¼Œç„¶åå†æ ¹æ®ä¸Šä¸‹æ–‡æ¥é¢„æµ‹è¿™äº›è¢«æ›¿ä»£æ‰çš„åŸå•è¯ï¼Œè¿™ç§æ–¹æ³•å¯ä»¥ä½¿æ¨¡å‹å……åˆ†ç”¨åˆ°ä¸Šä¸‹æ–‡çš„ä¿¡æ¯ã€‚

ä¸è¿‡è¿™ä¸¤ç±»è¯­è¨€æ¨¡å‹éƒ½æœ‰ä¸è¶³ä¹‹å¤„ï¼Œè‡ªå›å½’è¯­è¨€æ¨¡å‹åªæ˜¯å•å‘çš„ï¼Œä¸èƒ½è€ƒè™‘åˆ°åŒå‘çš„ä¿¡æ¯ï¼Œè‡ªç¼–ç è¯­è¨€æ¨¡å‹è™½ç„¶å…·æœ‰äº†åŒå‘çš„åŠŸèƒ½ï¼Œä½†æ˜¯åœ¨é¢„è®­ç»ƒæ—¶ä¼šå‡ºç°ç‰¹æ®Šçš„ <Mask>
tokenï¼Œå¯æ˜¯åˆ°äº†ä¸‹æ¸¸çš„ fine-tuning ä¸­åˆä¸ä¼šå‡ºç°è¿™äº› <Mask>ï¼Œè¿™å°±å‡ºç°äº†æ•°æ®ä¸åŒ¹é…ï¼Œä¼šå¸¦æ¥é¢„è®­ç»ƒçš„ç½‘ç»œå·®å¼‚ã€‚

XLNet å°±æ˜¯ä¸ºäº†å……åˆ†ç»“åˆäºŒè€…çš„ä¼˜ç‚¹ï¼Œå¼¥è¡¥å¯¹æ–¹çš„ä¸è¶³è€Œäº§ç”Ÿçš„ï¼Œå®ƒå¯ä»¥è®©è‡ªå›å½’è¯­è¨€æ¨¡å‹éƒ¨åˆ†å…·æœ‰åŒå‘çš„åŠŸèƒ½ï¼Œä¹Ÿèƒ½è®©è‡ªç¼–ç è¯­è¨€æ¨¡å‹éƒ¨åˆ†çš„é¢„è®­ç»ƒå’Œ Fine-
tuning ä¿æŒä¸€è‡´æ€§ã€‚ä¸‹é¢æ¥çœ‹çœ‹ XLNet æ˜¯å¦‚ä½•è¿›è¡Œæ”¹è¿›çš„ã€‚

### è‡ªå›å½’

è‡ªå›å½’ï¼ˆAutoRegressiveï¼ŒARï¼‰ï¼šç»™å®šä¸€æ®µåºåˆ— ${x_1,x_2,â€¦,x_t}$ï¼Œå…ˆä½¿ç”¨ ${x_1}$ é¢„æµ‹ $x_2$ï¼Œç„¶åä½¿ç”¨
${x_1,x_2}$ é¢„æµ‹ $x_3$ï¼Œæœ€åç”¨ ${x_1,x_2,â€¦,x_{t-1}}$ é¢„æµ‹
$x_t$ï¼Œè¿™æ ·å°±å¯ä»¥ç”Ÿæˆæ•´ä¸ªå¥å­ï¼Œè¿™ç§è¯­è¨€æ¨¡å‹çš„ç›®æ ‡æ˜¯æ‰¾å‡ºä¸€ä¸ªå‚æ•° Î¸ æœ€å¤§åŒ– ${x_1,x_2,â€¦,x_t}$
çš„å¯¹æ•°ä¼¼ç„¶å‡½æ•°ï¼Œå¯ä»¥çœ‹å‡ºè¿™ä¸ªè¿‡ç¨‹ä¸­ä¸‹ä¸€ä¸ªå•è¯åªä¾èµ–äº†ä¸Šæ–‡ï¼š

$$\underset{\theta}{max}\; log p_\theta(\mathbf{x})=\sum_{t=1}^T log
p_\theta(x_t \vert \mathbf{x}_{<t})=\sum_{t=1}^T log
\frac{exp(h_\theta(\mathbf{x}_{1:t-1})^T
e(x_t))}{\sum_{x'}exp(h_\theta(\mathbf{x}_{1:t-1})^T e(x'))} $$

å…¶ä¸­ ğ±<ğ‘¡ è¡¨ç¤º t æ—¶åˆ»ä¹‹å‰çš„æ‰€æœ‰ xï¼Œâ„ğœƒ(ğ±1:ğ‘¡âˆ’1) æ˜¯ RNN åœ¨ t æ—¶åˆ»ä¹‹å‰çš„éšçŠ¶æ€ï¼Œğ‘’(ğ‘¥) æ˜¯è¯ x çš„ embeddingã€‚

#### **XLNet å¦‚ä½•æ•æ‰ä¸Šä¸‹æ–‡ï¼Ÿ**

XLNet æƒ³è¦ä¿æŒè‡ªå›å½’çš„æ–¹æ³•æ¥é¢„æµ‹ä¸‹ä¸€ä¸ªå•è¯ï¼Œåˆæƒ³è¦åˆ©ç”¨ä¸Šä¸‹æ–‡çš„ä¿¡æ¯ï¼Œäºæ˜¯æå‡ºäº† **Permutation Language
Modelingï¼ˆPLMï¼‰** æ–¹æ³•ã€‚

![](https://images.gitbook.cn/a6b12ac0-a0a8-11ea-816e-999de338b69b)

PLM è¿™ä¸ªæ–¹æ³•çš„æ€æƒ³æ˜¯ï¼š

å‡è®¾æœ‰ä¸€ä¸ªåºåˆ— ${x_1,x_2,x_3,x_4}$ï¼Œå…ˆå¯¹è¿™ä¸ªåºåˆ—åš permutation æ’åˆ—ç»„åˆï¼Œæ¯”å¦‚ä¸€å¥è¯ x = [This, is, a,
sentence] æœ‰ 4 ä¸ª tokenï¼Œå°±ä¼šæœ‰ 4! ç§æ’åˆ—å¯èƒ½ï¼Œä¾‹å¦‚å¾—åˆ°å…¶ä¸­çš„ä¸€ç§æ’åˆ—æƒ…å†µ
${x_2,x_4,x_3,x_1}$ï¼Œç„¶åéšæœºé€‰æ‹©ä¸€ä¸ªå•è¯ä½œä¸ºé¢„æµ‹ç›®æ ‡ï¼Œä¾‹å¦‚ $x_3$ æ˜¯å½“å‰çš„é¢„æµ‹ç›®æ ‡ï¼Œåœ¨æ–°çš„æ’åˆ—åºåˆ—ä¸­å¯ä»¥çœ‹åˆ°ï¼Œ$x_3$
ä¸ä»…å¯ä»¥ç”¨åˆ°ä¸Šæ–‡çš„ $x_2$ï¼Œä¹Ÿèƒ½ç”¨åˆ°ä¸‹æ–‡çš„ $x_4$ äº†ï¼Œè¿™æ ·å°±å®ç°äº†åŒå‘ã€‚

![](https://images.gitbook.cn/ba01ee20-a0a8-11ea-a7e9-93a4ac8821bf)

ä¸Šå›¾ä¸­âœ…è¡¨ç¤º token ä¹‹é—´æœ‰ attetionï¼Œå¯ä»¥çœ‹åˆ° $x_3$ å¯ä»¥å…³è”åˆ° ${ x_3,x_2,x_4 }$ï¼Œ$x_4$ å¯ä»¥å…³è”åˆ°
${x_4,x_2}$ï¼Œ$x_2$ å¯ä»¥å…³è”åˆ°è‡ªå·±ï¼Œç©ºç™½æ˜¯è¢« mask äº†ã€‚

ä»ä»¥ä¸Šé¢çš„ `x = [This, is, a, sentence]` ä¸ºä¾‹ï¼Œè¦æ ¹æ®å‰ä¸¤ä¸ª token é¢„æµ‹ç¬¬ä¸‰ä¸ª token æ—¶ï¼Œæ’åˆ— [1, 2, 3,
4]ã€[1, 2, 4, 3]ã€[4, 3, 2, 1] å¯¹åº”çš„ç›®æ ‡å‡½æ•°å°±æ˜¯ $P (a, | This, is)$ã€$P(sentence | This,
is)$ å’Œ $P(is | sentence, a)$ï¼›è¦æ ¹æ®ç¬¬ä¸€ä¸ª token é¢„æµ‹ç¬¬äºŒä¸ª token æ—¶ï¼Œç›®æ ‡å‡½æ•°æ˜¯ $P (is |
This)$ã€$P(is | This)$ å’Œ $P(a | sentence)$ã€‚

å½“ç„¶ä¸ºäº†æ›´å…·æœ‰æ“ä½œæ€§ï¼Œä¸ä¼šçœŸçš„å°†åŸå§‹åºåˆ—è¿›è¡Œé‡æ’ï¼Œå› ä¸ºè¿™æ ·éœ€è¦è®°å½•å¾ˆå¤šæ•°æ®ï¼Œè€Œä¸”é¢„æµ‹åè¿˜éœ€è¦æŒ‰ç…§å­—å…¸å†æ’å›å»ï¼Œæ‰€ä»¥ä½œè€…åœ¨ä¸éœ€è¦å˜æ¢åŸå§‹åºåˆ—çš„åŸºç¡€ä¸Šï¼Œä½¿ç”¨
mask æ¥å®ç°æ’åˆ—ç»„åˆã€‚

![](https://images.gitbook.cn/ef1804a0-a0a8-11ea-bf38-950ba54cfedc)

ä¾‹å¦‚åŸå§‹åºåˆ—ä¸ºï¼š${x_1,x_2,x_3,x_4}$ï¼Œæ–°çš„åºåˆ—æ’åˆ—æƒ…å†µä¸º ${x_1,x_4,x_2,x3}$ï¼Œå¦‚æœ $x_3$
æ˜¯å½“å‰çš„é¢„æµ‹ç›®æ ‡ï¼Œå¦‚å·¦å›¾æ‰€ç¤ºï¼Œ$x_3$ å¯ä»¥å…³è”åˆ° ${ x_1,x_4,x_2,x_3 }$ï¼Œ$x_2$ å¯ä»¥å…³è”åˆ°
${x_1,x_4,x_2}$ï¼Œ$x_4$ å¯ä»¥å…³è”åˆ° ${x_1,x_4}$ï¼Œ$x_1$ å¯ä»¥å…³è”åˆ°è‡ªå·±ï¼Œé‚£ä¹ˆåªæ˜¯åœ¨æœ‰å…³è”çš„åœ°æ–¹åŠ ä¸Š
tokenï¼Œè€ŒçŸ©é˜µæœ¬èº«çš„è½´æ˜¯ä¸å˜çš„ï¼š

![](https://images.gitbook.cn/00000560-a0a9-11ea-97df-0d0e3bd6b465)

æˆ‘ä»¬ä»ä»¥ `x = [This, is, a, sentence]` æ¥å…·ä½“è¯´æ˜ï¼Œä¾‹å¦‚æ’åˆ—æƒ…å†µ [3, 2, 4, 1] ä¸‹ï¼š

  * æƒ³è¦é¢„æµ‹ç¬¬ä¸€ä¸ª token=3 çš„è¯ï¼Œå› ä¸ºå®ƒå‰é¢æ²¡æœ‰ä»»ä½•ä¿¡æ¯ï¼Œæ‰€ä»¥ä½ç½® mask ä¸º [0, 0, 0, 0]
  * æƒ³è¦é¢„æµ‹ç¬¬äºŒä¸ª token=2 æ—¶ï¼Œå®ƒå‰é¢è¦ç”¨åˆ°çš„ä¸ºâ€œ3â€ï¼Œæ‰€ä»¥ä½ç½® mask ä¸º [0, 0, 1, 0]
  * åŒç†ï¼Œç¬¬ä¸‰ä¸ª token=4 æ—¶çš„ä½ç½® mask ä¸º [0, 1, 1, 0]
  * ç¬¬å››ä¸ª token=1 æ—¶çš„ä½ç½® mask ä¸º [0, 1, 1, 1]ï¼Œè¿™æ ·æœ€åå¾—åˆ°çš„ mask çŸ©é˜µä¸ºï¼š

$$ \begin{bmatrix} 0 & 1 & 1 & 1 \\\ 0 & 0 & 1 & 0 \\\ 0 & 0 & 0 & 0 \\\ 0 & 1
& 1 & 0 \\\ \end{bmatrix} $$

è¿™æ ·è®­ç»ƒç›®æ ‡å°±å˜æˆäº†ï¼š

$$P(This | -, is+2, a+3, sentence+4)$$

$$P(is | -, -, a+3, -)$$

$$P(a | -, -, -, -)$$

$$P(sentence | -, is+2, a+3, -)$$

### è‡ªç¼–ç 

è‡ªç¼–ç ï¼ˆAutoEncodingï¼ŒAEï¼‰ï¼šå°†è¾“å…¥æ•°æ®æ„é€ æˆä¸€ä¸ªä½ç»´çš„ç‰¹å¾æ˜¯ç¼–ç éƒ¨åˆ†ï¼Œç„¶åå†ç”¨è§£ç å™¨å°†ç‰¹å¾æ¢å¤æˆåŸå§‹çš„æ•°æ®ã€‚

BERT å°±æ˜¯ä¸€ç§å»å™ªè‡ªç¼–ç æ–¹æ³•ï¼Œå®ƒåœ¨åŸå§‹æ•°æ®ä¸ŠåŠ äº† 15% çš„ Mask Token æ„é€ äº†å¸¦å™ªå£°çš„ç‰¹å¾ï¼Œç„¶åè¯•å›¾é€šè¿‡ä¸Šä¸‹æ–‡æ¥æ¢å¤è¿™äº›è¢«æ©ç›–çš„åŸå§‹æ•°æ®ã€‚

$$\underset{\theta}{max}\;log p_\theta(\bar{\mathbf{x}} | \hat{\mathbf{x}})
\approx \sum_{t=1}^Tm_t log p_\theta(x_t | \hat{\mathbf{x}})=\sum_{t=1}^T m_t
log \frac{exp(H_\theta(\mathbf{x})_{t}^T
e(x_t))}{\sum_{x'}exp(H_\theta(\mathbf{x})_{t}^T e(x'))} $$

å…¶ä¸­ $ğ‘š_ğ‘¡=1$ æ—¶åˆ™è¡¨ç¤º t æ—¶åˆ»æ˜¯ä¸€ä¸ª Maskï¼Œéœ€è¦æ¢å¤ï¼Œ$ğ»_ğœƒ$ æ˜¯ä¸€ä¸ª Transformerï¼Œç”¨æ¥å°†é•¿åº¦ä¸ºğ‘‡ çš„åºåˆ— ğ±
æ˜ å°„ä¸ºéšçŠ¶æ€çš„åºåˆ— $ğ»_ğœƒ(ğ±)=[ğ»_ğœƒ(ğ±)_1,ğ»_ğœƒ(ğ±)_2,...,ğ»_ğœƒ(ğ±)_ğ‘‡]$ã€‚

æ­¤å¤– BERT è¿˜æœ‰ä¸€ä¸ªé—®é¢˜æ˜¯å¦‚æœä¸€ä¸ªåºåˆ—ä¸­è¢« <Mask> çš„æœ‰ä¸¤ä¸ªä»¥ä¸Šæ—¶ï¼Œåœ¨é¢„æµ‹æ—¶ï¼Œè¿™æ ·è¿™äº›è¢« <Mask>
çš„ä½ç½®å°±å˜æˆäº†ç›¸äº’ç‹¬ç«‹çš„äº†ï¼Œè€Œäº‹å®ä¸Šå®ƒä»¬ä¹‹é—´æ˜¯å¯èƒ½å­˜åœ¨ä¾èµ–å…³ç³»çš„ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œ$x_3$ å’Œ $x_4$ ä¹‹é—´åº”è¯¥æ˜¯æœ‰ä¾èµ–å…³ç³»çš„ï¼Œå¯æ˜¯åœ¨æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­æ˜¯ç”¨
${x_1,x_2,x_5}$ ä¸€èµ·é¢„æµ‹ ${x_3,x_4}$ çš„ï¼Œä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œå°±éœ€è¦ç”¨åˆ°è‡ªå›å½’çš„æ–¹æ³•å…ˆé¢„æµ‹ $x_3$ å†é¢„æµ‹ $x_4$ï¼š

$$p(x_3|x_1, x_2, x_5) * p(x_4|x_1, x_2, x_5) $$

![](https://images.gitbook.cn/6c94cdf0-a0a9-11ea-9d24-cfb0df3065fc)

#### **XLNet å¦‚ä½•å–ä»£ <Mask>ï¼Ÿ**

BERT é€šè¿‡ <Mask> æ¥ä¼ é€’é¢„æµ‹ç›®æ ‡çš„ä½ç½®å’Œä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒXLNet é€šè¿‡ Conten stream å’Œ Query stream è¿™ç§
**Two-Stream Self-Attention** æ¥å®ç°è¿™ä¸¤ä¸ªåŠŸèƒ½ã€‚Content stream è´Ÿè´£å­¦ä¹ ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ŒQuery stream
è´Ÿè´£ä»£æ›¿ <Mask>ï¼ŒQuery stream åªå­˜åœ¨äºé¢„è®­ç»ƒæ—¶ï¼Œåœ¨ Finetune æ—¶å°±ä¸ç”¨äº†ã€‚

![](https://images.gitbook.cn/bf119860-a0a9-11ea-a321-115ce75343e0)

**1\. Content Stream**

Content Stream å°±æ˜¯ä¸€ä¸ªæ ‡å‡†çš„ self-attentionï¼Œå¦‚å›¾æ‰€ç¤ºï¼Œh1 åœ¨è¿›è¡Œ Self-Attention æ—¶ä¼šç”¨åˆ° QKVï¼Œå…¶ä¸­ Q
æ˜¯ h1ï¼ŒKV æ˜¯ h1~h4ï¼Œç„¶åä¼šå¾—åˆ° Attention weightï¼Œå†å’Œ V ç›¸ä¹˜å¾—åˆ° h1 åœ¨ä¸‹ä¸€å±‚çš„è¡¨ç¤ºï¼š

![](https://images.gitbook.cn/ec92eaf0-a0a9-11ea-853e-a34978cba4d6)

$$h^m_{z_t} = Attention(Q = h^{m-1}_{z_t}, KV = h^{m-1}_{z_{\leq t}};
\theta)$$

**2\. Query Stream**

Query Stream çš„ä½œç”¨æ˜¯é¢„æµ‹å•è¯ï¼Œåœ¨é¢„æµ‹æ—¶æ¨¡å‹ä¸èƒ½çŸ¥é“å®é™…çš„ token æ˜¯ä»€ä¹ˆï¼Œæ­¤æ—¶åˆä¸èƒ½ä½¿ç”¨ maskï¼Œäºæ˜¯ä½œè€…è®¾ç½®äº†ä¸€ä¸ª g
è¡¨ç¤ºï¼Œå¦‚å›¾æ‰€ç¤ºï¼Œç”¨ g1 å»ä½œç”¨ h2~h4ï¼Œæ ¹æ®å…¬å¼è®¡ç®— Attentionï¼Œä»å…¬å¼ä¸­å¯ä»¥çœ‹åˆ° Query stream å°†å½“å‰ t ä½ç½®çš„
attention weight æ©ç›–æ‰ï¼š

![](https://images.gitbook.cn/fee63b80-a0a9-11ea-8705-c338ee6eeef7)

$$g^m_{z_t} = Attention(Q = g^{m-1}_{z_t}, KV = h^{m-1}_{z_{<t}}; \theta)$$

ä¹Ÿå°±æ˜¯è¯´æ¯ä¸ª token çš„ä½ç½® i åœ¨æ¯ä¸ª self-attention å±‚ m ä¸Šæœ‰ä¸¤ä¸ªç›¸å…³çš„å‘é‡ï¼š$h^m_i$ã€$g^m_i$ï¼Œå…¶ä¸­ h å±äº
content streamï¼Œg å±äº query streamã€‚

  * h å‘é‡åˆå§‹åŒ–ä¸º token embeddings åŠ  positional embeddings
  * g å‘é‡åˆå§‹åŒ–ä¸º generic embedding åŠ  positional embeddings

å…¶ä¸­ generic embedding å‘é‡ w ä¸ token æ— å…³ï¼Œå³ä¸ç®¡ token æ˜¯ä»€ä¹ˆ w éƒ½æ˜¯ä¸€æ ·çš„ã€‚

ä¾‹å¦‚ä¸‹å›¾è¡¨ç¤ºäº†å¦‚ä½•è®¡ç®—ç¬¬ m å±‚çš„ç¬¬ 4 ä¸ª token çš„ g å‘é‡ï¼Œå¯ä»¥çœ‹åˆ° $g^m_4$ çš„è®¡ç®—ç”¨åˆ°äº† is+2ã€a+3 å’Œ w = 4
çš„ä¿¡æ¯ï¼Œä¹Ÿå°±æ˜¯åœ¨é¢„æµ‹ sentence è¿™ä¸ªå•è¯æ—¶è¦è€ƒè™‘ç¬¬äºŒä¸ªä½ç½®çš„ isï¼Œç¬¬ä¸‰ä¸ªä½ç½®çš„ aï¼Œä»¥åŠè‡ªèº«çš„ä½ç½® 4ï¼š

![](https://images.gitbook.cn/3351ba20-a0aa-11ea-a7e9-93a4ac8821bf)

è¿™æ ·æ¨¡å‹çš„è®­ç»ƒç›®æ ‡å˜æˆäº†ï¼š

$$P(This | *, is+2, a+3, sentence+4)$$

$$P(is | -, *, a+3, -)$$

$$P(a | -, -, *, -)$$

$$P(sentence | -, is+2, a+3, *)$$

å…¶ä¸­ `*` è¡¨ç¤ºå½“å‰æ­£åœ¨è®¡ç®—æ¦‚ç‡çš„ token ä½ç½®ã€‚

### XLNet å…·æœ‰å¤§å‹æ–‡æœ¬å­¦ä¹ èƒ½åŠ›

æ­¤å¤–ï¼Œä½œè€…è¿˜æ”¹è¿›äº†é¢„è®­ç»ƒçš„æ¶æ„è®¾è®¡ï¼Œå€Ÿé‰´äº† Transformer-XL çš„ Segment recurrence mechanism åˆ†æ®µé‡å¤æœºåˆ¶å’Œ
Relative positional encoding ç›¸å¯¹ä½ç½®ç¼–ç ä¸¤ç§æ–¹æ³•ï¼Œç®€å•è¯´å°±æ˜¯è®©ä¸åŒçš„ segment ä¹‹é—´å¯ä»¥äº’ç›¸åš Attentionï¼š

$$h^m_{z_t} = Attention(Q = h^{m-1}_{z_t}, KV = [\tilde h^{m-1},
h^{m-1}_{z_{\leq t}}]; \theta), [.,.]: Concatenate$$

è¿™éƒ¨åˆ†å¦‚æœå¤§å®¶å¯¹ç»†èŠ‚æ„Ÿå…´è¶£å¯ä»¥çœ‹çœ‹è¿™ç¯‡æ–‡ç« ï¼š[XLNet åŸç†](http://fancyerii.github.io/2019/06/30/xlnet-
theory/)ã€‚

è¿™æ · XLNet é€šè¿‡ PLM æ—¢ä¿æŒäº† ELMo, GPT ç­‰æ¨¡å‹çš„è‡ªå›å½’æ€§è´¨ï¼Œåˆå…·æœ‰äº† BERT ä¸€æ ·çš„æ•æ‰åŒå‘ä¿¡æ¯çš„åŠŸèƒ½ï¼Œè¿˜é€šè¿‡ Query
stream ä»£æ›¿äº† BERT çš„ <Mask> çš„ä½œç”¨ï¼Œæœ€åè¿˜åƒ Transformer-XL ä¸€æ ·èƒ½å¤„ç†å¤§å‹æ–‡æœ¬ï¼Œè¿™äº›æ”¹è¿›ä½¿ XLNet
åœ¨é•¿æ–‡æœ¬å¤„ç†ä»»åŠ¡ä¸­æœ‰çªå‡ºçš„è¡¨ç°ã€‚

### XLNet åº”ç”¨

ä¸‹é¢æˆ‘ä»¬æ¥é€šè¿‡ä¸€ä¸ªç®€å•çš„ä¾‹å­çœ‹å¦‚ä½•åº”ç”¨ XLNet æ¨¡å‹è¿›è¡Œæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚

æ‰€ç”¨çš„æ•°æ®ä¸º SST2 ç”µå½±è¯„è®ºæƒ…æ„Ÿåˆ†ææ•°æ®é›†ï¼Œç¬¬ä¸€åˆ—ä¸ºè¯„è®ºï¼Œç¬¬äºŒåˆ—ä¸ºæƒ…æ„Ÿæ ‡ç­¾ï¼Œä¸€å…±æœ‰ä¸¤ç±»ï¼Œ1 ä¸ºç§¯æè¯„è®ºï¼Œ0 ä¸ºæ¶ˆæè¯„è®ºã€‚

![](https://images.gitbook.cn/67e189a0-a0aa-11ea-816e-999de338b69b)

#### **1\. åŠ è½½åº“**

    
    
    !pip install transformers
    
    import numpy as np
    import pandas as pd
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.model_selection import GridSearchCV
    from sklearn.model_selection import cross_val_score
    import torch
    import transformers as ppb
    import warnings
    warnings.filterwarnings('ignore')
    

#### **2\. å¯¼å…¥æ•°æ®**

è¿™é‡Œæˆ‘ä»¬ä»¥å‰ 2000 ä¸ªæ ·æœ¬ä¸ºä¾‹ã€‚

    
    
    df = pd.read_csv('https://github.com/clairett/pytorch-sentiment-classification/raw/master/data/SST2/train.tsv', delimiter='\t', header=None)
    
    batch_1 = df[:2000]
    

#### **3\. åŠ è½½é¢„è®­ç»ƒ XLNet æ¨¡å‹**

    
    
    # xlnet
    model_class, tokenizer_class, pretrained_weights = (ppb.XLNetModel, ppb.XLNetTokenizer, 'xlnet-base-cased')
    
    # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œ tokenizer
    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)
    model = model_class.from_pretrained(pretrained_weights)
    

è¿™é‡Œè¿˜å¯ä»¥æ¢æˆå…¶ä»–æ¨¡å‹ï¼Œå¤§å®¶å¯ä»¥åœ¨ã€Š[transformers
å®˜æ–¹æ–‡æ¡£](https://huggingface.co/transformers/model_doc/xlnet.html#xlnettokenizer)ã€‹é‡Œæ‰¾åˆ°å…¶ä»–æ¨¡å‹çš„ä½¿ç”¨æ–¹æ³•ã€‚

ä¾‹å¦‚ï¼š

    
    
    # Albert
    model_class, tokenizer_class, pretrained_weights = (ppb.AlbertModel, ppb.AlbertTokenizer, 'albert-base-v2')
    
    # DistilBERT:
    model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')
    
    # BERT
    model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')
    

![](https://images.gitbook.cn/84a45c70-a0aa-11ea-9d24-cfb0df3065fc)

[å›¾ç‰‡æ¥æº](https://www.kdnuggets.com/2019/09/bert-roberta-distilbert-xlnet-one-
use.html)

#### **4\. å‡†å¤‡æ•°æ®**

Tokenizationï¼šå°†æ–‡æœ¬è½¬åŒ–ä¸ºæ•°å­—å‘é‡ã€‚

    
    
    tokenized = batch_1[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))
    

Paddingï¼šå°†æ•°æ®è¡¥é½æˆæŒ‡å®šé•¿åº¦ã€‚

    
    
    max_len = 0
    for i in tokenized.values:
        if len(i) > max_len:
            max_len = len(i)
    
    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])
    np.array(padded).shape
    

Maskingï¼šå‘Šè¯‰æ¨¡å‹å“ªäº›æ˜¯è¡¥é½çš„ä½ç½®ã€‚

    
    
    attention_mask = np.where(padded != 0, 1, 0)
    attention_mask.shape
    

#### **5\. åº”ç”¨æ¨¡å‹**

ä»å‰é¢å¾—åˆ°çš„ token çŸ©é˜µä¸­åˆ›å»ºäº†ä¸€ä¸ªè¾“å…¥å¼ é‡ï¼Œå°†å…¶ä¼ é€’ç»™æ¨¡å‹ï¼Œlast_hidden_states æ˜¯æ¨¡å‹çš„è¾“å‡ºï¼š

    
    
    input_ids = torch.tensor(padded)  
    attention_mask = torch.tensor(attention_mask)
    
    with torch.no_grad():
        last_hidden_states = model(input_ids, attention_mask=attention_mask)
    

å› ä¸ºæ¨¡å‹å¯¹æ¯ä¸ªå¥å­çš„ç¬¬ä¸€ä¸ª token çš„è¾“å‡ºæ„Ÿå…´è¶£ï¼Œæ‰€ä»¥ç”¨ features ä¿å­˜è¿™äº›æ•°æ®ï¼Œå¹¶ç”¨ labels ä¿å­˜å¥å­çš„æ ‡ç­¾ï¼š

![](https://images.gitbook.cn/8140a290-a0ab-11ea-97df-0d0e3bd6b465)

    
    
    features = last_hidden_states[0][:,0,:].numpy()
    
    labels = batch_1[1]
    

åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼š

    
    
    train_features, test_features, train_labels, test_labels = train_test_split(features, labels)
    

è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹ç”¨æ¥è¿›è¡Œåˆ†ç±»é¢„æµ‹ï¼š

    
    
    lr_clf = LogisticRegression()
    lr_clf.fit(train_features, train_labels)
    

åˆ†ç±»å‡†ç¡®ç‡ï¼š

    
    
    # xlnetï¼š
    lr_clf.score(test_features, test_labels)
    #0.732
    

å¤§å®¶æ„Ÿå…´è¶£å¯ä»¥æ¢æˆ BERT ç­‰å…¶ä»–æ¨¡å‹å¯¹æ¯”ä¸€ä¸‹æ•ˆæœã€‚

* * *

**é¢è¯•é¢˜ï¼š**

  * XLNet ä¸ºä»€ä¹ˆä¼šæ¯” BERT æ•ˆæœå¥½ï¼Ÿ

**å‚è€ƒæ–‡çŒ®ï¼š**

  * [_XLNet: Generalized Autoregressive Pretraining for Language Understanding_](https://arxiv.org/pdf/1906.08237.pdf)
  * Jay Alammar, [_A Visual Guide to Using BERT for the First Time_](http://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/)

