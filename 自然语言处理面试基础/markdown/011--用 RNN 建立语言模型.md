今天来看循环神经网络的一个重要的应用：语言模型。

本文结构：

  1. 什么是语言模型？
  2. 语言模型的应用？
  3. 为什么用基于 RNN 的语言模型？
  4. RNN 是如何实现语言模型的？
  5. RNN 是如何实现 character-level 语言模型的？
  6. character-level 语言模型的具体应用：生成莎士比亚风格的文本

* * *

### 什么是语言模型？

![](https://images.gitbook.cn/c9857f00-4726-11ea-869e-dd0ddfb7a363)

**语言模型，** 它的任务是要估计一个单词序列 $w_1, w_2, …, w_n$ 的概率 $P(w_1, w_2, …,
w_n)$。通过语言模型，可以在给出前面的单词的条件下，根据这些文本数据学习单词出现的概率，并 **预测序列中的下一个单词是什么。**

比较简单的模型可以学习短序列的单词，复杂一些的模型可以学习句子或者段落级别的任务，常见的是单词级别的模型。

### 语言模型的应用？

语言模型最直接的应用是 **用来生成文本** ，而这个应用是很多自然语言处理任务的基本组件，
**机器翻译、语音识别、手写识别、拼写纠正、图片字幕、文字摘要等等都会用到** 。

例如在对话系统中，输入一句话后，可以有很多种可能的句子作为回应，每个句子有自己的概率，这时就可以根据语言模型选择概率最大的句子作为回应。

### 为什么用基于 RNN 的语言模型？

在 RNN 出现以前，一般用 N-gram 模型来实现语言模型。

**N-gram** 是一个条件概率模型，如果计算概率时考虑前 1 个字就是 2-gram，考虑前两个字就是 3-gram，依此类推。

![图片来源：Depends On The
Definition](https://images.gitbook.cn/efcadc00-4726-11ea-936b-cfa88a589a44)

以 **2-gram** 为例，它的公式表示为：

$$P(w_1, w_2, w_3, …, w_n) = P(w_1|start)P(w_2|w_1)⋯P(w_n|w_{n−1})$$

要计算整个序列的概率，先通过计数来计算每一个因子

$$P(w_n|w_{n−1}) = { count(w_{n−1}w_n) \over count(w_{n−1}) }$$

一般而言，n 越大效果就越好，但同时也 **存在一些问题** ：

  * 随着 n 增加，N-gram 的参数变多，计算量也变大。
  * 受语料库影响，N-gram 中的 n 很大时，为了保证精度，需要语料库足够大，但是实际上可能很难获得足够的数据，就没有办法学到真正的语言空间中的概率。

而 **RNN 的好处就是在同样的句子条件下，参数比 N-gram 少，看得比 N-gram 远** 。例如有 t 个词汇，输入 $w_1$ 生成
$h_1$，输入 $w_2$ 生成 $h_2$，输入 $w_t$ 生成 $h_t$，最后这个 $h_t$ 是整个过去历史的表达，不管 history
有多长，RNN 的参数都不会变多。

### RNN 是如何实现语言模型的？

![](https://images.gitbook.cn/166b7860-4727-11ea-936b-cfa88a589a44)

输入初始向量一般为零向量后，模型的输出是一个概率 $p(w_1)$，对应着 $w_1$，下一时间步输入 $w_1$ 后，模型输出为条件概率
$P(w_2|w_1)$，因为是 RNN，所以 $h_t$ 具有前面所有输入的信息，以此类推，我们可以得到
$P(w_3|w_1，w_2)$，$P(w_4|w_1，w_2，w_3)$，再将所有时间步的输出连乘起来，就得到一个字符序列或者句子的概率：

$$P(w_1, w_2, w_3, …, w_n) = P(w_1) P(w_2|w_1) P(w_3|w_1,w_2) ...
P(w_n|w_1,w_2,... ,w_{n-1})$$

### RNN 是如何实现 character-level 语言模型的？

语言模型有 character-level、word-level、sentence-level、session-level 等级别的，我们这里讲第一种。

**character-level 语言模型** 的主要任务是根据一个序列数据中前面的所有字符来预测下一个字符，就是 **一个字符一个字符地生成文本**
。可以用来生成文风相似的文章、生成维基百科、生成含有 Latex 公式的文章、生成 Linux 代码，还可以给宝宝取名字等等。

#### **那么 RNN 是怎么实现字符级语言模型的？**

我们先以基本的 RNN 为例来看如何实现的，在后面的代码中可以用简单的 RNN，也可以用 LSTM，可以多层，也可以双向：

![](https://images.gitbook.cn/35fd22a0-4727-11ea-814b-0d3e32b9c16f)

首先 input 文本会被分解为字符级的序列，在投入到网络时，每个时间点输入一个字符，然后输出一个概率，再由这个概率与目标字符的 One-Hot
向量进行比较，训练时就让损失尽量最小，采样生成文本时就基于一种随机性规则选择出最可能的字符。

举个例子来看，已知 hell 想要生成 o，成为 hello：

**1\. 首先，需要建立一个词汇字典**

用语料库中所有文本的无重复字符作为 key，值是每个字符的 id，从 0 开始升序排列。例如：

    
    
    {"e": 0, "h": 1, "l": 2, "o": 3}
    

因此“hello”就对应着这样一个整数序列 [1, 0, 2, 2, 3]。

**2\. 根据词汇字典，将输入和输出字符序列转化为整数序列**

输入 x = "hell"，输出 y = "ello"，也就是

$$x = [1,0,2,2]，y = [0,2,2,3]$$

**3\. 对输入的每一个字符执行下面几步：**

  1. **将字符转化为 One-Hot 向量**
  2. 计算隐藏层
  3. 计算输出层， **将输出传递给 Softmax 得到概率**
  4. 将 t 时刻的目标字符，作为 t+1 时刻的输入字符
  5. 重复前面第一到四步，直到将输入序列的所有字符都输入到网络了

在训练模型时，输入的 x = "hell" = [1,0,2,2] 是已知的，预测目标 y = "ello" = [0,2,2,3]
也是已知的。由图所示，将输入投入到网络中后， **得到了一个概率序列，我们希望这个序列对应的就是预测目标** y =
"ello"，那么就希望网络输出的概率结果与实际 y 的 One-Hot 序列是越近越好。

也就是说，我们的目标是，在最后那层概率中， **希望绿色数字越大越好，红色数字越小越好** ，也就是实际的索引应该有最大的概率，即尽量接近于 1。

于是我们可以 **用 cross-entropy 作为损失**
的衡量方法，然后计算损失对参数的梯度，用梯度下降法更新梯度，在经过多次迭代更新参数后，模型通过训练数据集中的文本，就可以比较准确地根据前面所有的字符预测下一个字符是什么了。

### character-level 语言模型的具体应用：生成莎士比亚风格的文本

在这个例子中，我们选择莎士比亚的一部小说作为训练集，目的是建立一个基于 RNN 的 character-level
语言模型，来生成和输入文档风格相似的文本。这个模型是 many to many 的结构，并且由于输入和输出的时间步相等 $T_x =
T_y$，所以是同步的结构。

我们已经知道了 RNN 是如何建立字符级语言模型的了，下面这个具体应用的步骤和前面是一致的：

  1. 准备数据，预处理
    1. 创建两个字典
    2. 生成批次序列数据
    3. 将 x 和 y 划分为 mini-batches 数据
  2. 建立 LSTM 模型
    1. constructor：设定参数
    2. build：建立 LSTM 模型
    3. train：训练模型
    4. sample：生成文本
  3. 训练模型
  4. 采样生成文本

* * *

#### **1\. 准备数据**

我们可以在 [Project Gutenberg](https://www.gutenberg.org/) 找到很多免费电子书，这里以莎士比亚的 _The
Tragedie of Hamlet_ 为例，大家可以下载到本地：

> <http://www.gutenberg.org/cache/epub/2265/pg2265.txt>

**1\. 创建两个字典**

先读取文本，然后移除文本前面的描述部分，再创建两个字典。

  * char：是输入文本中无重复字符的集合。
  * char2int：用语料库中所有文本的无重复字符建立的字典，例如 {"e": 0, "h": 1, "l": 2, "o": 3}，用来将字符和整数对应起来，可以将一个文本转化为整数数组。
  * int2char：将整数对应到字符，这个字典在后面会被用来将模型的输出转化成字符，进而得到文本。
  * text_ints：根据词汇字典，将整个文本转化为整数序列。

    
    
    import numpy as np
    
    with open('pg2265.txt', 'r', encoding='utf-8') as f: 
        text=f.read()
    
    text = text[15858:]
    
    chars = set(text)
    
    char2int = { ch:i for i,ch in enumerate(chars) }
    int2char = dict( enumerate(chars) )
    
    text_ints = np.array( [char2int[ch] for ch in text], 
                         dtype = np.int32 )
    

**2\. 生成 x 和 y 的批次序列数据**

定义函数 reshape_data，用来生成具有下图结构的训练数据 x 和 y：

![](https://images.gitbook.cn/6694b5e0-4727-11ea-936b-cfa88a589a44)

因为我们的目标是根据目前为止观察到的字符序列来预测下一个字符， 因此，我们要将网络的输出和输入之间错开一个字符。

训练集的 x 和 y 有相同的维度：

  * 行数 = batch size
  * 列数 = number of batches × number of steps

sequence 是语料库中的字符所对应的整数数据：

    
    
    def reshape_data(sequence, batch_size, num_steps):
        mini_batch_length = batch_size * num_steps
        num_batches = int( len(sequence) / mini_batch_length )
    
        # 序列尾部不满 mini_batch_length 的部分就忽略了
        if num_batches * mini_batch_length + 1 > len(sequence):
            num_batches = num_batches - 1
    
        # y 和 x 之间错开一位
        x = sequence[0 : num_batches * mini_batch_length]
        y = sequence[1 : num_batches * mini_batch_length + 1]
    
        # 将 x 和 y 分成批次数据，分成 batch_size 批
        x_batch_splits = np.split(x, batch_size)
        y_batch_splits = np.split(y, batch_size)
    
        # 将批次数据堆叠起来，行数 = batch_size，列数 = num_steps * num_batches
        x = np.stack(x_batch_splits)
        y = np.stack(y_batch_splits)
    
        return x, y
    

**3\. 将 x 和 y 划分为 mini-batches**

由 reshape_data 得到的数据列数为 `num_steps * num_batches`，定义 create_batch_generator
将列数截成 num_batches 段，每段有 num_steps 列：

![](https://images.gitbook.cn/84aa4770-4727-11ea-8bef-d7e4fe1dc66a)

    
    
    np.random.seed(123)
    
    def create_batch_generator(data_x, data_y, num_steps):
        # data_x 的行数等于 batch_size，列数等于 num_steps * num_batches
        batch_size, tot_batch_length = data_x.shape    
        num_batches = int( tot_batch_length / num_steps )
    
        # 将列数截成 num_batches 段，每段有 num_steps 列
        for b in range(num_batches):
            yield (data_x[:, b * num_steps: (b + 1) * num_steps], 
                   data_y[:, b * num_steps: (b + 1) * num_steps])
    

前面是预处理部分，数据已经有了合适的格式，接下来建立 LSTM 字符级语言模型。

#### **2\. 建立 LSTM 模型**

我们通过建立一个 CharRNN 类来构建循环神经网络。

**CharRNN 类有四个方法：**

  1. constructor：设定参数，建立计算图，调用 build 方法，根据是采样模式还是训练模式建立图。
  2. build：定义 placeholders，建立 LSTM 模型，定义网络的输出，损失函数，优化算法。
  3. train：根据设定的 epoch 数，遍历 mini-batches，训练模型。
  4. sample：从一个给定的字符串开始，计算下一个字符的概率，根据这些概率随机选择一个字符，并重复这个过程，这些采样的字符会被连接在一起形成一个字符串，当长度达到了指定的大小，就返回这个字符串，即为生成的文本。

    
    
    import tensorflow as tf
    import os
    
    class CharRNN(object):
        def __init__()
    
        def build():
    
        def train():
    
        def sample():
    

**1\. constructor**

在 constructor 部分我们设定 batch _size、num_ steps、lstm_size
等参数，因为在训练和采样时会用两个不同的计算图，所以需要给 constructor 加一个 Boolean 型参数来决定我们是要训练模型，还是要采样。

其中参数 sampling 为 false 时是为了训练，为 true 时是采样。

参数 grad_clip 是用来做梯度剪切的，以应对梯度爆炸问题。

    
    
        def __init__(self, num_classes, batch_size = 64, 
                     num_steps = 100, lstm_size = 128, 
                     num_layers = 1, learning_rate = 0.001, 
                     keep_prob = 0.5, grad_clip = 5, 
                     sampling = False):
            self.num_classes = num_classes
            self.batch_size = batch_size
            self.num_steps = num_steps
            self.lstm_size = lstm_size
            self.num_layers = num_layers
            self.learning_rate = learning_rate
            self.keep_prob = keep_prob
            self.grad_clip = grad_clip
    
            self.g = tf.Graph()
            with self.g.as_default():
                tf.set_random_seed(123)
    
                self.build(sampling = sampling)
                self.saver = tf.train.Saver()
                self.init_op = tf.global_variables_initializer()   
    

**2\. build**

（可以将代码流程与前面第 5 小节的图进行对照）

首先定义两个局部变量 batch_size 和 num_steps：

  * 在 sampling 模式下，batch_size = 1，num_steps = 1，即一个字符一个字符地生成文本；
  * 在 training 模式下，batch_size = self.batch_size，num_steps = self.num_steps。

**One-Hot 编码**

用 One-Hot 方法为 x 和 y 编码，将字母转化为 One-Hot 向量 x_onehot、depth = num_classes，即 One-
Hot 向量的维度是 `num_classes x 1`，在每个字符所对应的索引位置为 1，其余位置为 0，其中 num_classes
是语料库中无重复字符的总数。

**定义多层 LSTM cell**

![](https://images.gitbook.cn/e430ee10-4727-11ea-814b-0d3e32b9c16f)

用 LSTM 比简单的 RNN 表现好，因为它能够捕捉到长期依赖的信息。

先用 BasicLSTMCell 建立 cell，然后用 DropoutWrapper 做 dropout，再用 MultiRNNCell 建立多层
LSTM。

  * tf.contrib.rnn.BasicLSTMCell：定义单个基本的 LSTM 单元
  * tf.contrib.rnn.DropoutWrapper：这里使用 dropout 方法来应对过拟合问题

**dropout** 是指在深度网络的训练过程中，按照一定的概率将一部分神经网络单元暂时从网络中丢弃，相当于从原始的网络中找到一个更瘦的网络。

在使用 DropoutWrapper 时有 input_keep_prob、output_keep_prob、state_keep_prob 三种形式：

  * input_keep_prob：对每一层 RNN 的输入进行 dropout
  * output_keep_prob：对每一层 RNN 的输出进行 dropout
  * state_keep_prob：对每一层 RNN 的中间传递的隐层状态进行 dropout

在 MultiRNNCell 中用 DropoutWrapper 时，一般不建议 input_keep_prob 和 output_keep_prob
同时使用，因为这样在层与层之间就会有 `keep_prob * keep_prob` 这个多 dropout。

  * tf.contrib.rnn.MultiRNNCell：用来对单层 Cell 进行堆叠，它将 x 输入到第一层 RNN 后，得到了隐层状态 h，这个隐层状态就相当于第二层 RNN 的输入，依次类推。

由 MultiRNNCell 得到的 cells 是 RNNCell 的子类，具有 call 方法、state_size 和 output_size
属性，可以通过 tf.nn.dynamic_rnn 来一次执行多步。

**定义 cell 的初始状态**

让我们回忆一下 LSTM 的结构，在每个时间步，需要用到之前一步的 cell 状态，所以当开始处理一个新的序列时，首先将 cell 的状态初始化为
0，之后在每个时间步都将更新后的状态存起来，作为下一个时间步的输入。

**建立 LSTM，具有前面定义的 cell 结构和 初始状态**

用输入序列数据、LSTM cell、初始状态，生成 LSTM 的展开后的结构，输出 lstm_outputs 的维度是 (batch_size,
num_steps, lstm_size)，self.final_state 会被存起来用作下一个批次数据的初始状态。

**lstm outputs 会被变型为一个 2D 的 tensor**

形状变为 `[batch_size * num_steps, lstm_size]`。

**然后传递给 tf.layers.dense 全连接层，得到 logits**

**最后得到下一批字符的概率**

将 output 传递给 Softmax 层，用来得到概率 proba，输出向量的每个值都在 0～1 之间，并且和为 1。

Softmax 层和输出层有相同的维度，$y^t[i]$ 表示在时刻 t 时，索引 i 对应的字符为预测的下一个字符的概率。

**定义损失函数**

损失函数用 cross-entropy 时刻 t 的损失为：

$$L^t(y, \hat{y}) = - \sum_{i} y_i log(\hat{y}_i) $$

总损失为：

$$L = \sum_{t=1}^{T_y} L^t(y, \hat{y}) $$

**用梯度剪切避免梯度爆炸问题**

在前面的文章中提到过 RNN 很容易出现梯度消失梯度爆炸问题，LSTM 虽然对基础的 RNN 的梯度消失问题有所改进，但还是存在梯度爆炸问题。

**LSTM 为什么会有梯度爆炸问题？**

在前面的文章中我们推导过 LSTM 的反向传播公式，为了说明它仍然存在梯度爆炸问题，我们可以看这篇论文 [_Supplementary Material
for LSTM: A Search Space Odyssey_](https://arxiv.org/pdf/1503.04069v1.pdf)
中更细致的表达式：

![](https://images.gitbook.cn/1a03c4e0-4728-11ea-815b-99042f14883a)

其中，

$$ \delta c^t = ... + \delta c^{t+1} \circ f^{t+1} $$

当 $f^{t+1}$ 接近于 1 时，$\delta c^{t+1}$ 相当于无损地累加到 $\delta c^t$。

但是在 LSTM 中，除了 $c^t$ 到 $c^{t+1}$，两个相邻时间步之间还存在其他路径，例如，$y^{t} → o^{t+1} →
y^{t+1}$。

![](https://images.gitbook.cn/375b0f80-4728-11ea-a3df-7bd6d9a26357)

经过两步反向传播，我们有：

$$\delta y^t ← R^T_o \delta o^{t+1} ← \delta y^{t+1} ← R^T_o \delta o^{t+2}$$

可以看到 $R^T_o$ 被累乘了两次，这和简单 RNN 一样，可能造成梯度爆炸。同理，由于 $R^T_i$、$R^T_f$、$R^T_z$
的累积，输入门和遗忘门的路径也可能造成梯度爆炸。

**所以这里我们用 gradient clipping 来应对此问题。**

tf.clip_by_global_norm：它的输入参数有 t_list 是要被修剪的张量, clip_norm 是修剪的阈值。返回的有
list_clipped 是修剪后的张量，global_norm 是一个中间计算量。

    
    
    def clip_by_global_norm(t_list, clip_norm, use_norm=None, name=None)
    

公式为：

$$ L^i_c = \begin{cases} L^i_t, & (N_g <= N_c) \\\ L^i_t * {N_c \over N_g} , &
(N_g > N_c) \end{cases} $$

$$ N_g = \sqrt \sum_{i} (L^i_t)^2 $$

其中，$L^i_t$ 和 $L^i_c$ 代表 t_list[i] 和 list_clipped[i]， $N_c$ 和 $N_g$ 代表
clip_norm 和 global_norm 的值。

即：当 t_list 的 L2 范数大于指定的范数阈值时，就会对 t_list 做等比例缩放。

下面是 build 的完整代码：

    
    
        def build(self, sampling):
    
            if sampling == True:
                batch_size, num_steps = 1, 1
            else:
                batch_size = self.batch_size
                num_steps = self.num_steps
    
            # 建立三个 placeholder： tf_x, tf_y, tf_keepprob，用来喂入输入数据
            tf_x = tf.placeholder(tf.int32, 
                                  shape=[batch_size, num_steps], 
                                  name='tf_x')
            tf_y = tf.placeholder(tf.int32, 
                                  shape=[batch_size, num_steps], 
                                  name='tf_y')
            tf_keepprob = tf.placeholder(tf.float32, 
                                  name='tf_keepprob')
    
            # 1. One-Hot 编码
            ## One-Hot 向量的维度是 num_classes x 1
            ## 在每个字符所对应的索引位置为 1，其余位置为 0
            x_onehot = tf.one_hot(tf_x, depth=self.num_classes)
            y_onehot = tf.one_hot(tf_y, depth=self.num_classes)
    
            # 2. 定义多层 LSTM cell
            ## 从内向外依次为，用 BasicLSTMCell 建立 cell，
            ## 用 DropoutWrapper 应用 dropout 
            ## 用 MultiRNNCell 建立多层 LSTM
            cells = tf.contrib.rnn.MultiRNNCell(
                [tf.contrib.rnn.DropoutWrapper(
                    tf.contrib.rnn.BasicLSTMCell(self.lstm_size), 
                    output_keep_prob = tf_keepprob) 
                for _ in range(self.num_layers)] )
    
            # 3. 定义 cell 的初始状态
            ## 回忆一下 LSTM 的结构，在每个时间步，需要用到之前一步的cell状态，
            ## 所以当开始处理一个新的序列时，首先将 cell 的状态初始化为 0，
            self.initial_state = cells.zero_state(
                        batch_size, tf.float32)
    
            # 4. 建立具有前两步定义的 cell 和 初始状态的 RNN，
            ## 用输入序列数据，LSTM cell，初始状态，生成 LSTM 的展开后的结构
            ## 输出 lstm_outputs 的维度是 (batch_size, num_steps, lstm_size)
            ## self.final_state 会被存起来用作下一个批次数据的初始状态
            lstm_outputs, self.final_state = tf.nn.dynamic_rnn(
                        cells, x_onehot, 
                        initial_state=self.initial_state)
    
            print('  << lstm_outputs  >>', lstm_outputs)
    
            # 5. 将形状变为 [batch_size * num_steps, lstm_size]
            seq_output_reshaped = tf.reshape(
                        lstm_outputs, 
                        shape=[-1, self.lstm_size],
                        name='seq_output_reshaped')
    
            # 6. 经过全连接层，得到 logits
            logits = tf.layers.dense(
                        inputs=seq_output_reshaped, 
                        units=self.num_classes,
                        activation=None,
                        name='logits')
    
            # 7. 得到下一批字符的概率
            ## 将 output 传递给 Softmax 层，用来得到概率，
            ## 这样输出向量的每个值都在 0～1 之间，并且和为 1
            ## Softmax 层和输出层有相同的维度，vocab_size x 1
            ## `$y^t[i]$` 表示在时刻 t 时，索引 i 对应的字符为预测得到的下一个字符的概率
            proba = tf.nn.softmax(
                        logits, 
                        name='probabilities')
    
            print(proba)
    
            y_reshaped = tf.reshape(
                        y_onehot, 
                        shape=[-1, self.num_classes],
                        name='y_reshaped')
    
            # 8. 定义损失函数        
            cost = tf.reduce_mean(
                        tf.nn.softmax_cross_entropy_with_logits(
                            logits=logits, 
                            labels=y_reshaped),
                        name='cost')
    
            # 9. 梯度剪切避免梯度爆炸问题
            ## LSTM 虽然对基础对 RNN 的梯度消失问题有所改进，但还是存在梯度爆炸问题
            ## 所以这里我们用 gradient clipping 技术来应对此问题，
            tvars = tf.trainable_variables()
            grads, _ = tf.clip_by_global_norm(
                        tf.gradients(cost, tvars), 
                        self.grad_clip)        
            optimizer = tf.train.AdamOptimizer(self.learning_rate)        
            train_op = optimizer.apply_gradients(
                        zip(grads, tvars),
                        name='train_op')
    

**3\. train**

在每个 epoch 开始时，RNN cell 以零初始状态开始，每次训练小批数据 batch_x、batch_y。

当执行完每一小批数据时，将状态更新为 dynamic_rnn 返回的 final_state，
这个更新后的状态会被用来执行下一小批数据，当前状态会随着迭代的次数增加不断更新。

最后将训练好的模型保存起来，在采样时调用。

    
    
        def train(self, train_x, train_y, 
                  num_epochs, ckpt_dir='./model/'):
    
            if not os.path.exists(ckpt_dir):
                os.mkdir(ckpt_dir)
    
            with tf.Session(graph=self.g) as sess:
                sess.run(self.init_op)
    
                n_batches = int( train_x.shape[1] / self.num_steps )
                iterations = n_batches * num_epochs
    
                for epoch in range(num_epochs):
    
                    # 在每个 epoch 开始时，RNN cell 以零初始状态开始
                    new_state = sess.run(self.initial_state)
                    loss = 0
    
                    ## 生成 Mini batch 数据
                    bgen = create_batch_generator(
                            train_x, train_y, self.num_steps)
    
                    for b, (batch_x, batch_y) in enumerate(bgen, 1):
                        iteration = epoch * n_batches + b
    
                        # 喂入数据 batch_x, batch_y，训练每一小批数据
                        feed = {'tf_x:0': batch_x,
                                'tf_y:0': batch_y,
                                'tf_keepprob:0': self.keep_prob,
                                self.initial_state : new_state}
    
                        # 当执行完每一小批数据时，将状态更新为 dynamic_rnn 返回的 final_state    
                        # 这个更新后的状态会被用来执行下一小批数据
                        # 当前状态会随着迭代的次数不断更新
                        batch_cost, _, new_state = sess.run(
                                ['cost:0', 'train_op', 
                                self.final_state],
                                feed_dict=feed)
    
                        if iteration % 10 == 0:
                            print('Epoch %d/%d Iteration %d'
                                  '| Training loss: %.4f' % (
                                  epoch + 1, num_epochs, 
                                  iteration, batch_cost))
    
                    ## 保存训练好的模型    
                    self.saver.save(
                            sess, os.path.join(
                                ckpt_dir, 'language_modeling.ckpt'))
    

**4\. sample**

**什么是 sampling 采样？**

sampling 在一个字符一个字符地生成文本时使用， **用来控制随机性** ，是一种将 RNN 在每个时间步生成的字符变成一个有趣的文本的方法。

在每个时间步 t，RNN 输出了在已知前面所有字符情况时下一个字符的条件概率，即 $P(c_n|c_1,c_2,... ,c_{n-1})$。例如我们在时刻
t = 3，得到了条件概率 $P(c_3/c_1,c_2)=(0.2,0.3,0.4,0.1)$，这时，我们可以用两种方法来选择下一个字符。

  * 最大熵：使用均匀概率分布随机地选择字符。这时词汇字典中的所有字符都有相同的可能性被选上，这也意味着在选择下一个字符时的随机性是最大的，这样生成的文本一般没有实际意义。
  * 最小熵：在每个时间步上会选择具有最大条件概率的字符。 这时下一个字符是训练好的模型预测到的，因此，这样生成的文本会比较有意义。 不过，单纯用这个方法会得到很多重复的枯燥的文本。

可见，当我们增加了随机性，文本就会失去局部结构，当我们降低随机性，文本就会更加有意义并且保持局部结构，不过重复性高。

在这个例子中，我们会取介于最大熵和最小熵之间的一个方法， **选择概率最大的几个作为候选集，再从中随机选择** 。即 get_top_char 函数：

除了最大的 n 个值以外，其他值变为 0，将 top_n 传递给 numpy.random.choice，从最大的 n 个概率中随机选择一个，返回它对应的
index。

    
    
    def get_top_char(probas, char_size, top_n = 5):
        p = np.squeeze(probas)
        p[np.argsort(p)[:-top_n]] = 0.0
        p = p / np.sum(p)
        ch_id = np.random.choice(char_size, 1, p = p)[0]
        return ch_id
    

下图为采样的过程：

![](https://images.gitbook.cn/68db2e00-4728-11ea-869e-dd0ddfb7a363)

给 sample 传递一个初始序列 starter_seq，output_length
为要生成的文本所包含的字符数，将初始序列投入到训练好的网络中，得到概率向量 proba，用 get_top_char 从中随机采样了一个 index，将这个
index 转化为相应的字符，附加到要生成的序列 observed_seq 中，然后将每次采样新得到的 index 向量输入到 LSTM
模型中，得到下一个概率，选择下一个 index，将对应的字符附加到序列上，一直到达到了要求的长度。

最后返回的是文本序列 observed_seq。

    
    
        def sample(self, output_length, 
                   ckpt_dir, starter_seq="The "):
    
            # 给一个初始序列作为生成文本的开头，生成的序列从参数 starter_seq 开始     
            observed_seq = [ch for ch in starter_seq]  
    
            # 调用训练好的模型     
            with tf.Session(graph=self.g) as sess:
                self.saver.restore(
                    sess, 
                    tf.train.latest_checkpoint(ckpt_dir))
    
                new_state = sess.run(self.initial_state)
    
                # 将 starter sequence 输入到 LSTM 模型中
                for ch in starter_seq:
                    x = np.zeros((1, 1))
                    x[0,0] = char2int[ch]
    
                    feed = {'tf_x:0': x,
                            'tf_keepprob:0': 1.0,
                            self.initial_state: new_state}
    
                    # self.final_state 是 tf.nn.dynamic_rnn 的输出
                    proba, new_state = sess.run(
                            ['probabilities:0', self.final_state], 
                            feed_dict=feed)
    
                # 随机采样了一个 index
                ch_id = get_top_char(proba, len(chars))
    
                # 将这个 index 转化为字符，加入到要生成的序列中
                observed_seq.append(int2char[ch_id])
    
                # 将每次采样新得到的 index 向量输入到 LSTM 模型中，
                ## 得到下一个概率，选择下一个 index，将对应的字符附加到序列上，一直到达到了要求的长度
                for i in range(output_length):
                    x[0,0] = ch_id
    
                    feed = {'tf_x:0': x,
                            'tf_keepprob:0': 1.0,
                            self.initial_state: new_state}
    
                    proba, new_state = sess.run(
                            ['probabilities:0', self.final_state], 
                            feed_dict=feed)
    
                    ch_id = get_top_char(proba, len(chars))
    
                    observed_seq.append(int2char[ch_id])
    
            return ''.join(observed_seq)
    

#### **3\. 训练模型**

接下来训练模型。

我们迭代 100 epoch，num _steps 为 100 个时间步， 将预处理好的 train_x、train_ y 投入到循环神经网络中进行训练，
训练好的模型被存在 ./model-100/ 中，预测时或者继续训练时可以调用。

    
    
    batch_size = 64
    num_steps = 100 
    
    train_x, train_y = reshape_data(text_ints, 
                                    batch_size, 
                                    num_steps)
    
    rnn = CharRNN(num_classes = len(chars), batch_size = batch_size)
    
    rnn.train(train_x, train_y, 
              num_epochs = 100,
              ckpt_dir='./model-100/')
    

#### **4\. 采样生成文本**

最后进行采样，调用 sample 方法，加载存储好的训练过的模型，生成长度为 500 的字符序列：

    
    
    np.random.seed(123)
    
    rnn = CharRNN(len(chars), sampling = True)
    
    print(rnn.sample(ckpt_dir = './model-100/', 
                     output_length = 500))
    

输出结果为：

    
    
      << lstm_outputs  >> Tensor("rnn/transpose_1:0", shape=(1, 1, 128), dtype=float32)
    Tensor("probabilities:0", shape=(1, 65), dtype=float32)
    INFO:tensorflow:Restoring parameters from ./model-100/language_modeling.ckpt
    
    The soull bestit,
    I haue sime, then mad to that in it to my Laer
    
       Ham. To shim'd to to thit in that may,
    But some, the till seese to meet then as ald buch,
    The preanisse sine in, ward as his tould of hin, and and well
    
       Opre. I doone hass ofe his allisine in the prount
    In the mosh andelles tine in heruse atrestine, and but tane,
    With my selfe, to me thinke ther as mistare, ard it my sind
    Oh thin the Soristoof that it a teere that here therre,
    In's mestre my Fathing, wo ding he mourses is tee to
    

从结果可以看到，网络生成了一些具有实际意义的单词，为了让结果更有趣可以迭代更多代，还可以选择更多的数据、更深的网络，训练时间更长一些。

### 用 TensorFlow 2.0 实现语言模型

下面我们再用 TensorFlow 2.0 将上面的语言模型实现一下，在数据处理和模型建立上的思路都是一样的，大家可以比较一下二者的区别。

#### **1\. 导入 TensorFlow 和其他库**

    
    
    from __future__ import absolute_import, division, print_function, unicode_literals
    
    try:
      # %tensorflow_version 仅存在于 Colab
      %tensorflow_version 2.x
    except Exception:
      pass
    import tensorflow as tf
    
    import numpy as np
    import os
    import time
    

#### **2\. 导入数据**

数据也是同样的文本，导入后取 16246 之后的文本，因为前面是版权声明等文字。

    
    
    with open('pg2265.txt', 'r', encoding='utf-8') as f: 
        text=f.read()
    
    text = text[16246:]
    

#### **3\. 建立字典**

这一步要做的也和 TF1 中的一样，先创建两个索引字典，然后将文本转化为数字。

    
    
    # 取文本中的非重复字符
    vocab = sorted(set(text))
    print ('{} unique characters'.format(len(vocab)))    # 65 unique characters, eg: ['\n', ' ', '!', '&', "'",...]
    
    # 创建从非重复字符到索引的映射
    char2idx = {u:i for i, u in enumerate(vocab)}        # length is 65, eg: {'\n': 0, ' ': 1, '!': 2, '&': 3, "'": 4}
    # 直接用array的序号与char对应
    idx2char = np.array(vocab)                            # eg: array(['\n', ' ', '!', '&', "'"], dtype='<U1')
    
    # 将文本 text 转化为数字序列
    text_as_int = np.array([char2idx[c] for c in text])    # length is  162849, eg: array([32, 46, 43,  1, 32, 56, 39, 45, 43, 42])
    
    # 可以看一下文本前 10 个字符被映射成了什么样的整数序列：
    print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:10]), text_as_int[:10]))
    
    # 'The Traged' ---- characters mapped to int ---- > [32 46 43  1 32 56 39 45 43 42]
    

#### **4\. 创建训练数据**

    
    
    # 设定每个输入句子长度的最大值
    seq_length = 100   
    examples_per_epoch = len(text)//seq_length  
    

这里我们用 tf.data.Dataset 对数据进行各种处理。

    
    
    # 创建训练样本 / 目标
    char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)
    

可以打印出 dataset 的前几个数据看一下：

    
    
    for i in char_dataset.take(5):
      print(i.numpy())
    
    
    
    32
    46
    43
    1
    32
    

然后用 batch 方法将单个字符转换为所需长度的序列：

    
    
    sequences = char_dataset.batch(seq_length+1, drop_remainder=True)    # 最后剩余的忽略
    

就是原本在 char_dataset 中是单个数字的形式，用 batch 将 char_dataset 数据分成了每句话长度为
`seq_length+1=101` 的 sequences：

    
    
    for item in sequences.take(2):
      print(item.numpy())
    
    
    
    [32 46 43  1 32 56 39 45 43 42 47 43  1 53 44  1 21 39 51 50 43 58  0  0
     14 41 58 59 57  1 28 56 47 51 59 57  9  1 31 41 53 43 52 39  1 28 56 47
     51 39  9  0  0 18 52 58 43 56  1 15 39 56 52 39 56 42 53  1 39 52 42  1
     19 56 39 52 41 47 57 41 53  1 58 61 53  1 16 43 52 58 47 52 43 50 57  9
      0  0  1  1 15]
    [39 56 52 39 56 42 53  9  1 34 46 53  4 57  1 58 46 43 56 43 13  0  1  1
     19 56 39 52  9  1 26 39 63  1 39 52 57 61 43 56  1 51 43 11  1 31 58 39
     52 42  1  3  1 60 52 44 53 50 42  0 63 53 59 56  1 57 43 50 44 43  0  0
      1  1  1 15 39 56  9  1 24 53 52 45  1 50 47 59 43  1 58 46 43  1 23 47
     52 45  0  0  1]
    

接下来构造 x 和 y，同样的，y 和 x 之间需要错位一个字符，例如输入序列为 "Hell"，则目标序列为 "ello"。

这里可以直接用 map 对 sequences 的每个序列执行 split_input_target 函数，split_input_target
就是生成错位的 x 和 y：

    
    
    def split_input_target(chunk):
        input_text = chunk[:-1]
        target_text = chunk[1:]
        return input_text, target_text
    
    dataset = sequences.map(split_input_target)
    

现在 dataset 的每条数据包括一个输入序列和输出序列，二者之间错位一个数字：

    
    
    for input_example, target_example in  dataset.take(1):
      print ('Input data: ', input_example.numpy())
      print ('Target data:', target_example.numpy())
    
    
    
    Input data:  [32 46 43  1 32 56 39 45 43 42 47 43  1 53 44  1 21 39 51 50 43 58  0  0
     14 41 58 59 57  1 28 56 47 51 59 57  9  1 31 41 53 43 52 39  1 28 56 47
     51 39  9  0  0 18 52 58 43 56  1 15 39 56 52 39 56 42 53  1 39 52 42  1
     19 56 39 52 41 47 57 41 53  1 58 61 53  1 16 43 52 58 47 52 43 50 57  9
      0  0  1  1]
    Target data: [46 43  1 32 56 39 45 43 42 47 43  1 53 44  1 21 39 51 50 43 58  0  0 14
     41 58 59 57  1 28 56 47 51 59 57  9  1 31 41 53 43 52 39  1 28 56 47 51
     39  9  0  0 18 52 58 43 56  1 15 39 56 52 39 56 42 53  1 39 52 42  1 19
     56 39 52 41 47 57 41 53  1 58 61 53  1 16 43 52 58 47 52 43 50 57  9  0
      0  1  1 15]
    

#### **5\. 构造批次数据**

先用 shuffle 将数据打乱，然后再用 batch 构造批次数据。

    
    
    # 批大小
    BATCH_SIZE = 64
    
    # 设定缓冲区大小，以重新排列数据集 
    BUFFER_SIZE = 10000
    
    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)
    
    dataset        # <BatchDataset shapes: ((64, 100), (64, 100)), types: (tf.int64, tf.int64)>
    

我们可以看看此时 dataset 的第一条数据，它的维度是 (64, 100), (64, 100)，即 Input 和 Output 的每一条维度都是
(64, 100)，可见再用一次 batch 后，就将 64 个 sentense 组成了一个 batch，非常的方便。

    
    
    for input_example, target_example in  dataset.take(1):
      print ('Input data: ', input_example.numpy())
      print ('Target data:', target_example.numpy())
    
    
    
    Input data:  [[56 41 46 ... 47 52 45]
     [ 1 29 59 ... 16 59 41]
     [41 39 58 ... 57 53 54]
     ...
     [39 52 42 ...  7  1 39]
     [ 0 58 47 ...  1 21 39]
     [39  1 60 ... 46 39 50]]
    Target data: [[41 46 39 ... 52 45  1]
     [29 59  9 ... 59 41 49]
     [39 58 43 ... 53 54 46]
     ...
     [52 42  1 ...  1 39 52]
     [58 47 50 ... 21 39 51]
     [ 1 60 47 ... 39 50 50]]
    

#### **6\. 建立模型**

在 TF1 的代码中我们的模型超参数是这样的：

    
    
    def __init__(self, num_classes, batch_size = 64, 
                 num_steps = 100, lstm_size = 128, 
                 num_layers = 1, learning_rate = 0.001, 
                 keep_prob = 0.5, grad_clip = 5, 
                 sampling = False):
    

下面在建立模型时也采用同样的配置。

    
    
    # 词集的长度
    vocab_size = len(vocab)
    
    # 嵌入的维度
    embedding_dim = 256
    
    # RNN 的单元数量
    rnn_units = 128
    

使用 tf.keras.Sequential 定义模型：

  * tf.keras.layers.Embedding：输入层，将每个字符的数字映射到一个 embedding_dim 维度的向量。
  * tf.keras.layers.LSTM：采用 LSTM 层，rnn_units = 128。
  * tf.keras.layers.Dropout：使用 Dropout，这里的 rate = 0.5。
  * tf.keras.layers.Dense：输出层，有 vocab_size 个输出。

    
    
    def build_model(vocab_size, embedding_dim, rnn_units, batch_size):
      model = tf.keras.Sequential([
        tf.keras.layers.Embedding(vocab_size, embedding_dim,
                                  batch_input_shape=[batch_size, None]),
        tf.keras.layers.LSTM(rnn_units,
                            return_sequences=True,
                            stateful=True,
                            recurrent_initializer='glorot_uniform'),
        tf.keras.layers.Dropout(0.5),
        tf.keras.layers.Dense(vocab_size)
      ])
      return model
    
    
    model = build_model(
      vocab_size = len(vocab),
      embedding_dim=embedding_dim,
      rnn_units=rnn_units,
      batch_size=BATCH_SIZE)
    
    model.summary()
    

![](https://images.gitbook.cn/8eff6e70-4728-11ea-b5fc-c30e6f2b5954)

#### **7\. 定义损失函数，优化器**

    
    
    def loss(labels, logits):
      return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)
    
    model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.001, clipvalue=5),
                  loss=loss)
    

#### **8\. 配置检查点**

用 tf.keras.callbacks.ModelCheckpoint 来确保训练过程中保存检查点。

    
    
    # 检查点保存至的目录
    checkpoint_dir = './training_checkpoints'
    
    # 检查点的文件名
    checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt_{epoch}")
    
    checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_prefix,
        save_weights_only=True)
    

#### **9\. 训练模型**

作为示例，这里先设置 epoch=10：

    
    
    EPOCHS=10
    history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])
    

#### **10\. 生成文本**

    
    
    # 恢复最新的检查点
    tf.train.latest_checkpoint(checkpoint_dir)
    
    # 为了让预测步骤简单，将批次大小设定为 1。
    model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)
    
    model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))
    
    model.build(tf.TensorShape([1, None]))
    
    model.summary()
    

![](https://images.gitbook.cn/aa74d9b0-4728-11ea-815b-99042f14883a)

文本生成时，首先设置起始字符串，初始化 RNN 状态并设置要生成的字符个数，根据起始字符串和 RNN
的初始状态，模型可以得到下一个字符的预测分布，然后用这个类别分布计算预测出来的字符的索引，并将预测字符和前面的隐藏状态一起传递给模型作为下一个输入，就这样不断从前面预测的字符获得更多上下文，进行学习。

    
    
    def generate_text(model, start_string):
      # 评估步骤（用学习过的模型生成文本）
    
      # 要生成的字符个数
      num_generate = 1000
    
      # 将起始字符串转换为数字（向量化）
      input_eval = [char2idx[s] for s in start_string]
      input_eval = tf.expand_dims(input_eval, 0)
    
      # 空字符串用于存储结果
      text_generated = []
    
      # 低温度会生成更可预测的文本
      # 较高温度会生成更令人惊讶的文本
      # 可以通过试验以找到最好的设定
      temperature = 1.0
    
      # 这里批大小为 1
      model.reset_states()
      for i in range(num_generate):
          predictions = model(input_eval)
          # 删除批次的维度
          predictions = tf.squeeze(predictions, 0)
    
          # 用分类分布预测模型返回的字符
          predictions = predictions / temperature
          predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()
    
          # 把预测字符和前面的隐藏状态一起传递给模型作为下一个输入
          input_eval = tf.expand_dims([predicted_id], 0)
    
          text_generated.append(idx2char[predicted_id])
    
      return (start_string + ''.join(text_generated))
    

下面就以“Ham：”为开头预测 1000 个字符：

    
    
    print(generate_text(model, start_string=u"Ham: "))
    

![](https://images.gitbook.cn/c52f3250-4728-11ea-8138-55f994072888)

可以看到预测的文本结果，这个模型能够知道什么时候使用大写字母，什么时候分段，而且模仿出了莎士比亚式的词汇，不过由于训练的周期小，模型尚未学会生成连贯的句子，大家之后可以对模型进行改进，让生成的文本更加像是莎士比亚写出来的。

* * *

在前面几篇文章中我们主要是看单层 RNN 的应用，今天学习了多层 RNN，以及 RNN 是如何实现字符级语言模型的，在建立完基于 RNN
的语言模型后，用采样法生成了莎士比亚风的文章，并用 gradient clipping 来处理 LSTM 的梯度爆炸问题。讲完了单向 RNN
及其应用，下一篇将学习双向 RNN。

经过今天的学习，相信大家对语言模型有了一定的了解，下面几个面试题大家可以思考一下如何解答：

**面试题：**

  1. 什么是梯度爆炸？
  2. LSTM 为什么会有梯度爆炸的问题？
  3. 如何建立一个语言模型？
  4. 什么是 N-gram？如何用 N-gram 语言模型来随机生成句子？
  5. 传统的统计语言模型与神经网络语言模型有什么不同？

* * *

参考资料：

  * [循环神经网络（RNN）文本生成](https://www.tensorflow.org/tutorials/text/text_generation)
  * Sebastian Raschka，Vahid Mirjalili， _Python Machine Learning_

