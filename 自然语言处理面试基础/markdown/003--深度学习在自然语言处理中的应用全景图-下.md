### 机器翻译

**机器翻译** 研究的是如何使用计算机自动翻译人类语言。

**深度学习用于机器翻译主要有两类方法：**

  1. 将深度学习用于改进统计机器翻译的各个组件，如词对齐，翻译规则概率估计，短语重新排序模型，语言模型和模型特征组合。
  2. 基于编码器-解码器框架的端到端翻译系统，可以直接用神经网络将源语言映射到目标语言。

端到端神经机器翻译与传统统计机器翻译的主要区别是它可以直接从数据中学习，无需手动设计特征来捕获翻译规则。

[Sutskever et al. 2014)](https://papers.nips.cc/paper/5346-sequence-to-
sequence-learning-with-neural-networks.pdf) 提出用一个 RNN 作为编码器，将源上下文编码为向量表示，用另一个
RNN 作为解码器，逐字生成翻译。

![Jn7Nf6](https://images.gitbook.cn/Jn7Nf6)

在 Sutskever 的编码-解码框架中，不管句子的长度是多少，编码器需要将整个源句子表示为一个固定长度的向量。 [Bahdanau et al.
2015)](https://arxiv.org/pdf/1409.0473.pdf)
表明这样会使神经网络难以处理长期依赖，并引入了注意机制，动态地选择相关源上下文来生成目标词。

![TbVA39](https://images.gitbook.cn/TbVA39)

神经机器翻译有一个很大的挑战是如何解决目标语言词汇引起的效率问题，因此 Sutskever 和 Bahdanau
使用的都是完整词汇表的子集，但这样会显著影响子集或词典外的词的翻译质量。 [Luong et al.
2015)](http://www.aclweb.org/anthology/P15-1002)
提出的模型可以识别源语句和目标句子中的词典外单词之间的对应关系，并在后期处理步骤中翻译词典外单词。

神经机器翻译还有一个重要课题是如何将先验知识整合到神经网络中。 [Zhang et al.
2017b)](http://nlp.csai.tsinghua.edu.cn/~ly/papers/acl2017_zjc.pdf) 在
[(Ganchev et al.
2010)](http://www.jmlr.org/papers/volume11/ganchev10a/ganchev10a.pdf)
提出的后验正规化的基础上，提出了一个框架可以整合任意知识来源。

* * *

### 问答系统

**问答系统 (QA: Question answering QA)** 是自然语言处理中一个非常有挑战性的任务，

**深度学习在 QA 中两个比较典型任务上有了很不错的应用：**

  1. 深层学习问答知识库，即用深度学习来理解问题的含义，并将它们翻译成结构化查询。2. 深度学习机器理解，用来直接计算问题和答案之间的深层语义匹配。

很多神经网络或它们的变种都已经被用于这个任务，例如
CNN，RNN（LSTM，BLSTM），注意机制和记忆网络。这些研究主要分为两类：信息提取方式（information
extraction）或语义解析方式（semantic parsing）。信息提取即使用一些关系提取技术从知识库中获得一组候选答案，然后将其与问题进行比较。
语义解析是设法借助新颖的网络结构从句子中提取出符号表示或结构化查询。

**信息提取方式** 的工作通常是在一个神经网络结构中对答案进行 retrieval–embedding–comparing。

[Bordes et al. 2014a)](http://www.aclweb.org/anthology/D14-1067)
最早提出了一个联合嵌入框架，可以学习出一个结构化知识库中对单词，实体，关系等语义项的向量表示，并设法将一个自然语言问题映射到知识库的某个子图。

[Dong et al. 2015)](http://www.aclweb.org/anthology/P15-1026) 用 CNN
来编码问题和候选答案之间的不同类型的特征。他们提出了一种多列卷积神经网络（MCCNN）来捕捉问题的不同方面，并通过三个渠道，答案路径，答案语境，答案类型进一步对一组问答进行评分。

[Hao et al.
2017)](http://www.nlpr.ia.ac.cn/cip/~liukang/liukangPageFile/ACL2017-Hao.pdf)
提出的一种基于交叉注意力机制的神经网络比基于知识库的问答系统要好。

记忆网络是一种新颖的学习框架，根据一个记忆机制设计，可以在特定任务期间被读取，修改和添加。 [Miller et al.
2016)](https://aclweb.org/anthology/D16-1147) 研究了记忆知识的各种 Key- Value
形式，他们的模型还可以从存储器中进行多次寻址和读取，可以收集上下文，动态地更新问题并获得最终答案。

基于 KBQA
的另一种主流是语义分析的模型，这种模型尝试正规地表示问题的含义，然后使用知识库进行实例化，并在知识库上面建立结构化查询，进而可以显式地捕获复杂查询。 [Xu
et al. 2016)](http://www.aclweb.org/anthology/P16-1220) 提出了一个多通道卷积神经网络（MCC-
NNs），可以从词汇和句法角度学习紧凑稳健的关系表示。这个方法很适合开放域知识库问答系统。因为在开放域知识库中通常存在数千个关系，传统的基于特征的模型会遇到数据稀疏问题，而且在看不见的单词上的泛化能力也差。

在语义理解领域，[Seo et al. 2016)](https://arxiv.org/pdf/1611.01603.pdf)
提出了双向注意流网络（BiDAF），采用多阶段分层过程，可以不需要提前总结就能在不同粒度下表示上下文。

![YOTDpe](https://images.gitbook.cn/YOTDpe)

* * *

### 情感分析

**情感分析**
要做的是从社交网络，博客或产品的评论中识别和提取用户的情绪，在数据挖掘，网络挖掘和社交媒体分析方面有广泛应用。主要任务有情绪分类，意见提取，细粒度情绪分析。
接下来我们主要看深度学习在 句子级，文档级的应用。

**句子级别** 的情感分析就是对句子的情感极性进行分类。
很多神经网络结构都可以用来处理这个问题，卷积神经网络，循环神经网络，递归神经网络和辅助增强句子表示。

CNN 可以更好地捕获基于窗口的局部组合, 基本的 CNN 有一个卷积层和池化层，[Kalchbrenner et al.
2014)](http://www.aclweb.org/anthology/P14-1062) 将其扩展为多层结构，并用动态k-max
池化来更好地表示句子。 [Yin and Schütze 2015)](http://www.aclweb.org/anthology/K15-1021)
构建了多通道的多层 CNN，可以使用若干个不同的词嵌入。

RNN 可以有效学习隐式长期依赖性， [Wang et al.
2015)](http://www.aclweb.org/anthology/P15-1130) 提出了用长期短期记忆（LSTM）神经网络进行推文情绪分析。

[Zhang et al. 2016c)](http://www.aclweb.org/anthology/N16-1177) 结合了 LSTM 和
CNN，提出了一个依赖敏感的 CNN 模型，使 CNN 网络结构也能够捕获长距离依赖性，很好地利用了二者的优点。

**文档级** 的情感分类是要识别一个文档的情感类型。

[Tang et al. 2015a)](http://aclweb.org/anthology/D15-1167) 用 CNN 来计算句子向量，然后用双向
GRU 来计算整个文档的嵌入表示。

![eOuQ1a](https://images.gitbook.cn/eOuQ1a)

* * *

### 视觉字幕

由图像生成自然语言，或者称为 **视觉字幕**
，是一种新兴的深度学习应用，属于自然语言生成（NLG），是计算机视觉和自然语言处理的交叉。是很多重要应用的基础技术，如语义视觉搜索，聊天机器人的视觉智能，帮助视障人士感知视觉内容等等。

过去2年深度学习技术发展迅速才使这一领域得到突破性进展，在这之前这个任务几乎是不可能完成的。

[Sutskever et al. 2014)](https://papers.nips.cc/paper/5346-sequence-to-
sequence-learning-with-neural-networks.pdf); [Bahdanau et al.
2015)](https://arxiv.org/pdf/1409.0473.pdf) 将 sequence-to-sequence
用于机器翻译取得了比较成功的效果后，[(Vinyals et al. 2015)](https://www.cv-
foundation.org/openaccess/content_cvpr_2015/papers/Vinyals_Show_and_Tell_2015_CVPR_paper.pdf);
[(Karpathy and Fei-Fei
2015)](https://cs.stanford.edu/people/karpathy/cvpr2015.pdf); [(Fang et al.
2015)](https://www.cv-
foundation.org/openaccess/content_cvpr_2015/papers/Fang_From_Captions_to_2015_CVPR_paper.pdf);
[(Devlin et al.
2015)](https://people.eecs.berkeley.edu/~sgupta/pdfs/devlin2015language.pdf);
[(Chen and Zitnick 2015)](http://xinleic.xyz/papers/cvpr15_rnn.pdf)
也研究了用于图像字幕的端到端的 end-to-end encoder–decoder 框架。

在这个框架中，原始图像通过深度 CNN 被编码为一个全局视觉特征向量，承载着图像的整体语义信息，

提取出全局视觉特征向量后，被投入到基于 RNN 的解码器中来生成字幕，

上述 encoder–decoder 结构不仅可以用于图像字幕，还被 [(Ballas et al.
2016)](https://arxiv.org/pdf/1511.06432.pdf) 用于视频字幕，主要区别是用了不同的 CNN 的结构和基于 RNN
的语言模型。

[(Xu et al. 2015)](https://arxiv.org/pdf/1502.03044.pdf)
也将注意力机制用于这个方向，来学习字幕生成时应该聚焦在图像的什么位置。

[(Anderson et al. 2017)](https://arxiv.org/pdf/1707.07998.pdf) 提出的 bottom-up
注意力模型，可以将整个模型的所有部分，包括 CNN，RNN 和 attention，从头到尾共同训练，实现端对端并且达到了非常好的效果。

此外，[(Rennie et al.
2017)](http://openaccess.thecvf.com/content_cvpr_2017/papers/Rennie_Self-
Critical_Sequence_Training_CVPR_2017_paper.pdf)
提出了一种自我批判序列训练算法，是强化学习在视觉字幕领域的应用。 强化学习在视觉字幕领域也越来越流行，

生成对抗网络（GAN）也被用于文本生成，SeqGAN[(Yu et
al.2017)](https://arxiv.org/pdf/1609.05473.pdf)将生成器建模为一种强化学习的随机策略，用于输出文本，RankGAN
[(Lin et
al.2017)](https://arxiv.org/pdf/1705.11001.pdf)提出了一种基于排序的鉴别器的损失，可以更好地评估生成文本的质量。

![8DTtCB](https://images.gitbook.cn/8DTtCB)

![kfyptY](https://images.gitbook.cn/kfyptY)

* * *

以上就是简要介绍了深度学习在 NLP
的其中几个领域的重要研究，会话语言理解，对话系统，知识图谱，机器翻译，问答系统，情感分析还有视觉字幕，由此我们也可以看出
RNN，LSTM，GRU，双向RNN，Seq2seq，Attention机制在深度学习自然语言处理领域的重要作用，本门课程也会对其中几个应用进行详细讲述和给出代码实战。
在提到的文献中都给出了论文链接，如果大家有兴趣可以点击学习，也可以进一步找相关论文学习，了解更新的研究进展，另外推荐大家看一下这本书《Deep
Learning in Natural Language Processing》。

