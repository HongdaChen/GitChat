前一篇文章中我们学习了 CNN 的基础结构，并且知道了它是计算机视觉领域的基础模型，其实 CNN
不仅仅可以用于计算机视觉，还可以用于处理自然语言处理问题，今天就来看看如何用 CNN 处理文本分类任务。

**文本分类**
就是根据文本的内容给句子或者文档打标签分类别，是自然语言处理领域中很基础的应用，前面学过的情感分析和垃圾邮件识别都属于文本分类任务。再比如主题标签、语言检测、意图识别，这些也都在社交媒体分析、品牌监测、客户服务上有广泛的应用。我们平常遇到的数据有
80% 是非结构化数据，而文本数据是最常见的类型之一，如果能够将社交媒体上的、网页上的、聊天对话中的有用信息提取出来，那么会带来很高的应用价值。

既然是分类问题，那么在情感分析那一章中提到的很多算法都是可以尝试的，Naive Bayes、Support Vector
Machines、RNN，还有今天要讲的 CNN。

### CNN 在处理文本任务上有什么优势

前面一篇文章中我们介绍了 CNN 的基础，在 NLP 领域中 CNN 比较适合分类任务，因为将卷积用于文本任务可以保持数据的空间特征，相比而言，对于 POS
这种需要局部信息的任务简单的 CNN 就不会有很大优势。

对于分类任务，例如在一个情感分析任务中，有这样两句话：

  * I like the movie, not too bad
  * I did not like the movie, bad

假设卷积窗口长度为 5，则第一句话的卷积窗口经过的区域是：

  * [I, like, the, movie, ',']，这句话大概率会被识别成情感标签为正
  * [like, the, movie, ',', not]
  * [the, movie, ',', not, too]
  * [movie, ',', not, too, bad]，这句话也会被识别成情感标签为正

第二句话的卷积窗口经过的区域是：

  * [I, did, not, like, the]
  * [did, not ,like, the, movie]，这句话会被大概率识别成情感标签为负
  * [not, like, the, movie, ',']
  * [like, the, movie, ',', bad]

从这个例子可以看出卷积的窗口可以保持空间的特征，如果没有窗口的话，假如用 bag-of-words 等模型来做就会损失空间信息。

CNN 还有一个很大的优势就高效快速，而且可以自动地学习出很好的特征表示，第一个卷积层所学习出来的特征理论上很像 n-grams，但是会比 n-grams
更加高效，普通的 3-grams 算法就已经需要很昂贵的计算了，一般都不会超过 5-grams，而 CNN 可以轻松地将窗口设置为 3、4、5。

### CNN 模型如何用于文本分类任务

关于将 CNN 用于文本分类，有一篇经典的论文是 Yoon Kim 的 [Convolutional Neural Networks for
Sentence Classification](https://arxiv.org/abs/1408.5882)，作者用一个很简单的 CNN
模型，就能够在当时 7 个基准测试中的 4 个任务上的表现优于其他模型，可见 CNN 不仅在图像领域表现很好，在 NLP
领域同样可以得到不错的效果，今天这篇文章主要就是解读和重现 Kim 的论文。

**首先看论文中的模型结构：**

![](https://images.gitbook.cn/1f1214c0-7f3e-11ea-a918-478f642cdd64)

由上图可以看出模型的整体结构由 4 层组成。

#### **输入层**

这一层主要是需要将输入的文本数据转化成二维的形式，这样就让 CNN 像处理图像数据一样处理文本。

**首先，要将文本数据转化为词嵌入向量的形式** ，这一步需要找到最长的句子长度 n，将长度不足的句子用 0
在前部补齐；并设置词向量的大小，每个句子中的每个单词都被表示为长度为 k 的向量；这样一个句子就有一个矩阵表示，如果再考虑 batch 的大小
b，那么输入的维度就是 b×n×k，如下图所示：

![](https://images.gitbook.cn/58446a40-7f3e-11ea-bee5-61432feb39ea)

为了简单起见，我们只看一句话的情况，即输入维度为 n×k。

**另外，在词向量这个角度上，作者做出了多种尝试，一共拓展出了四种模型：**

  * CNN-rand：每个词的词向量都是随机初始化的，词嵌入矩阵会随着模型的训练过程不断地优化。
  * CNN-static：输入单词的词向量会使用 Google 的 Word2Vector 预训练结果，并且在 CNN 模型训练过程中是固定不变的。
  * CNN-non-static：和 static 一样是使用 Google 的 Word2Vector 预训练结果，但是会在训练过程中被微调。
  * CNN-multichannel：这个模型同时使用了 CNN-static 和 CNN-non-static 两种词向量，这样输入层会有两个矩阵，作为两个 channel，然后再进行卷积等后续操作，在模型训练过程中一个会被微调，一个会保持不变，用来起到正则化的作用。

#### **卷积层**

这一层包括三个不同窗口大小的卷积层，分别为：3、4、5，输入数据经过三个不同长度的卷积窗口，会得到 3 个结果向量，这样的效果会比只做一层卷积的效果好。

  * 首先，每个卷积窗口的大小可以表示为 m×k，注意它是作用在整个词向量上面的；
  * 当 n×k 的输入和一个 m×k 的卷积窗口作用后，会得到一个长度为 n-(m-1) 的向量；
  * 3 种窗口分别代表着 3 个 channel，即 q=3；
  * 在每个 channel 中还会设置 100 个 filters，所以最后每个 channel 会得到 100 个向量。

![](https://images.gitbook.cn/b541a6e0-7f3e-11ea-9456-634f43a6106e)

其中 filter
数量的意思是卷积神经网络的神经元个数，每个神经元的权重不同，会对输入数据采用不同的卷积操作，和上一篇文章中提到的一样，不同的神经元可以提取出不同的特征。

#### **池化层**

论文中使用的是最大池化，即从卷积层的每个 feature map 中选择一个最大值。

  * 对于卷积层得到的 q*filters 个大小为 n-(m-1) 的向量，每一个向量采用最大池化提取出一个值来；
  * 再将这 q*filters 个值拼接成一个向量。

#### **全连接层**

接着进入带有 dropout=0.5 的全连接层，最后用 Softmax 映射到输出层，得到每一类别的概率。

**CNN 用于 NLP 和 CV 领域的对比：**

下图是 CS231n 中 CNN 用于视觉领域的原理示意图，对比上面这个模型结构来看，最大的区别就是卷积窗口是作用在整个词向量长度上的，即窗口维度的 k 和
输入维度的 k 是一样的，而其他操作如卷积计算、池化、全连接都是一样的。

![](https://images.gitbook.cn/f9d7c9b0-7f3e-11ea-8ec2-752fc54f41de)

[图片来源](http://cs231n.github.io/convolutional-networks/)

### 实战：如何用 CNN 分类句子

接下来我们尝试将论文中的模型复现一下，论文中作者将模型用于 MR、SST-1、SST-2、Subj、TREC、CR
六个数据集上，有电影评论数据、情感分析数据、产品评价数据等等，这里我们使用 [kaggle
上的情感分析数据集](https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-
only/data)。

![](https://images.gitbook.cn/271339a0-7f3f-11ea-bee5-61432feb39ea)

这个数据来自 Rotten Tomatoes 电影评论数据，常被用来做情感分析。其中每个句子还经过 Stanford parser
进行了解析，就是会分解成树状的结构，所以一个句子会分解出好多个短语，在这个数据集中，每个句子和短语都有情感标签，我们要做的是用 CNN
模型来识别每个短语句子的情感类别，一共有以下五种情感：

  * 0：negative
  * 1：somewhat negative
  * 2：neutral
  * 3：somewhat positive
  * 4：positive

首先[下载数据](https://www.kaggle.com/c/movie-review-sentiment-analysis-kernels-
only/data)，其中 train.tsv 包括每个短语及其情感标签，以及该短语所属的句子ID，test.tsv
中只有短语，没有情感标签，我们需要用模型预测出来。

![](https://images.gitbook.cn/4257d0e0-7f3f-11ea-b497-6b28b57af19c)

#### **1\. 加载需要的库**

    
    
    import numpy as np 
    import pandas as pd 
    import os
    
    from sklearn.feature_extraction.text import CountVectorizer
    from sklearn.metrics import confusion_matrix, accuracy_score
    from sklearn.model_selection import train_test_split
    
    from keras.preprocessing.text import Tokenizer
    from keras.preprocessing.sequence import pad_sequences
    from keras.initializers import Constant
    from keras.models import Model
    from keras.layers import *
    from keras.utils.np_utils import to_categorical
    from keras import regularizers
    import re
    
    import matplotlib.pyplot as plt
    %matplotlib inline
    

#### **2\. 读取数据**

将 train.tsv 和 test.tsv 下载到本地后进行读取，每个 sentenceid
的第一行是完整的一句话，之后是根据这句话拆分出来的短语或者短句，每个语句都有相应的情感标签。我们选取其中两个最关键的列 Phrase 和
Sentiment。

    
    
    df = pd.read_csv('train.tsv', delimiter='\t')
    df = df[['Phrase', 'Sentiment']]
    df.head()
    

![](https://images.gitbook.cn/5f20be30-7f3f-11ea-a918-478f642cdd64)

#### **3\. 数据预处理**

这里的预处理比较简单，就是将 urls 替换成 url，删除非字母、数字、空格的字符，并转化成小写字母。

    
    
    def clean_str(in_str):
        in_str = str(in_str)    
        in_str = re.sub(r"(https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9][a-zA-Z0-9-]+[a-zA-Z0-9]\.[^\s]{2,}|https?:\/\/(?:www\.|(?!www))[a-zA-Z0-9]\.[^\s]{2,}|www\.[a-zA-Z0-9]\.[^\s]{2,})", "url", in_str)
        in_str = re.sub(r'([^\s\w]|_)+', '', in_str)
        return in_str.strip().lower()
    
    df['text'] = df['Phrase'].apply(clean_str)
    

![](https://images.gitbook.cn/7f639000-7f3f-11ea-a205-47a8f48db192)

然后看一下各个种类的数据数目，发现数据并不均衡，最少的具有 7072 条，于是我们在每个类别中随机选择 7072 条数据组成最后的 data，即 35360
行。

    
    
    df.Sentiment.value_counts()
    
    
    
    2    79582
    3    32927
    1    27273
    4    9206 
    0    7072 
    Name: Sentiment, dtype: int64
    
    
    
    df_0 = df[df['Sentiment'] == 0].sample(frac=1)
    df_1 = df[df['Sentiment'] == 1].sample(frac=1)
    df_2 = df[df['Sentiment'] == 2].sample(frac=1)
    df_3 = df[df['Sentiment'] == 3].sample(frac=1)
    df_4 = df[df['Sentiment'] == 4].sample(frac=1)
    
    sample_size = 7072
    
    data = pd.concat([df_0.head(sample_size), df_1.head(sample_size), df_2.head(sample_size), df_3.head(sample_size), df_4.head(sample_size)]).sample(frac=1)
    

看看 data 中每个句子的最大长度，进而决定输入序列的长度为多少。

    
    
    data['l'] = data['Phrase'].apply(lambda x: len(str(x).split(' ')))
    print("max length of sentence: " + str(data.l.max()))
    # max length of sentence: 52
    
    sequence_length = 52
    

  * 接下来用 tokenizer.texts_to_sequences 将文本数据转化为数字向量 X，如 `X[0] = [6, 4528, 73, 9, 19, 142, 2873, 2107, 4, 750, 317, 323, 25, 4529, 2108, 4, 35, 5846, 2109, 6808, 4, 406, 25, 2874, 5121]`。
  * 再用 pad_sequences 将每个短语都补充成长度为 `sequence_length = 52` 的序列，对于长度不足的数据会在前部用 0 补充。
  * X 处理完后，用 get_dummies 得到情感标签的 one-hot 向量表示，如 y[0] = [0, 0, 0, 1, 0]。

    
    
    max_features = 20000 
    
    tokenizer = Tokenizer(num_words=max_features, split=' ', oov_token='<unw>')
    tokenizer.fit_on_texts(data['Phrase'].values)
    
    X = tokenizer.texts_to_sequences(data['Phrase'].values)
    
    X = pad_sequences(X, sequence_length)
    
    y = pd.get_dummies(data['Sentiment']).values
    

这样模型所需要的数据和标签都准备好了，接下来将处理好的数据划分为训练集和测试集。

    
    
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)
    print("test set size " + str(len(X_test)))
    # test set size 3536
    

#### **4\. 建立 CNN 模型**

**下面我们来搭建第一个模型 CNN-Random embedding model（CNN-rand）：**
即每个词的词向量都是随机初始化的，词嵌入矩阵会随着模型的训练过程不断地优化。

按照论文中的模型结构添加各个层即可：

  * 输入层 inputs；
  * 卷积层有三个 conv_0、conv_1、conv_2，卷积窗口分别为 3、4、5；
  * 池化层 maxpool_0、maxpool_1、maxpool_2，即在每个卷积层上作用一个最大池化层；
  * 用 Concatenate 和 Flatten 将结果串联；
  * 进入带有 dropout 的全连接层；
  * 再由 softmax 预测各个类别的概率。

在论文中 embedding_dim 是 300 维的，由于我们在后面几个模型中会使用大小为 200 的 GloVe
词向量，为了统一比较，这个模型的词向量维度也设置为 200。

    
    
    embedding_dim = 200 
    num_filters = 100
    
    inputs = Input(shape=(sequence_length,), dtype='int32')
    
    # 文本数据的随机词嵌入
    embedding_layer = Embedding(input_dim=max_features, output_dim=embedding_dim, input_length=sequence_length)(inputs)
    
    reshape = Reshape((sequence_length, embedding_dim, 1))(embedding_layer)
    
    # 论文中提到了激活函数用 ReLU，约束为 3 的 L2 正则化，卷积核窗口是作用在整个 embedding_dim = 300 上的
    conv_0 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)
    conv_1 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)
    conv_2 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape)
    
    # 每个卷积上作用 max pooling
    maxpool_0 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0)
    maxpool_1 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1)
    maxpool_2 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2)
    
    # 合并，展平
    concatenated_tensor = Concatenate(axis=1)([maxpool_0, maxpool_1, maxpool_2])
    flatten = Flatten()(concatenated_tensor)
    
    # 采用 dropout，并预测
    dropout = Dropout(0.5)(flatten)
    output = Dense(units=5, activation='softmax')(dropout)
    
    model = Model(inputs=inputs, outputs=output)
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    print(model.summary())
    

观察模型中各层的维度可以更清晰地了解结构：

![](https://images.gitbook.cn/ef9d1da0-7f3f-11ea-8ec2-752fc54f41de)

**训练模型：**

    
    
    batch_size = 32
    history = model.fit(X_train, y_train, epochs=30, batch_size=batch_size, verbose=1, validation_split=0.1, shuffle=True)
    

**用训练好的模型在测试集上进行预测，计算准确率：**

    
    
    y_hat = model.predict(X_test)
    accuracy_score(list(map(lambda x: np.argmax(x), y_test)), list(map(lambda x: np.argmax(x), y_hat)))
    

**画出训练集和测试集的准确率和损失：**

可以看到 epoch 选择 6 比较好，不过准确率还是有点低。

    
    
    plt.plot(history.history['acc'])
    plt.plot(history.history['val_acc'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()
    
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()
    

![](https://images.gitbook.cn/ffd3a8b0-7f3f-11ea-aef6-539c3826c714)

**接下来构建论文中的第二种模型是 Static word2vec（CNN-static）：** 输入单词的词向量会使用 Google 的
Word2Vector 预训练结果，并且在 CNN 模型训练过程中是固定不变的。

首先去这里[下载 GloVe 词向量](https://www.kaggle.com/rtatman/glove-global-vectors-for-
word-representation)，有 50100200 三种维度的可以选择，我们这里使用 glove.6B.200d.txt，论文中是 300d
的，可以去[这里下载数据](https://nlp.stanford.edu/projects/glove/)。

glove.6B.200d.txt 中包括 400000 个单词的词向量，每个词向量的长度是 200，文件中每一行的第一个是单词，后面 200
个是单词的词向量数值。

**首先读取文件构建“单词—词向量”的字典** embeddings_index，key 是单词，value 是词向量。

    
    
    embeddings_index = {}
    f = open('glove.6B.200d.txt')
    
    for line in f:
        values = line.split()
        word = values[0]
        coefs = np.asarray(values[1:], dtype='float32')
        embeddings_index[word] = coefs
    f.close()
    
    print('Found %s word vectors.' % len(embeddings_index))
    

**然后获得 word_index，即每个单词在语料库中对应的序号** ，这里我们一共有 14046 个单词，每个单词都对应着一个编号，如：

    
    
     'credit': 995,
     'called': 996,
     'final': 997,
     'usually': 998,
     'nicely': 999,
     'overcome': 1000,
    
    
    
    word_index = tokenizer.word_index
    print('Found %s unique tokens.' % len(word_index))
    # Found 14045 unique tokens.
    

**接下来得到 embedding_matrix** ，维度是 (14046, 200)，这一步就是要在上一步由 GloVe 得到的
embeddings_index 中，找到 word_index 的 14046 个单词的词向量，如果索引中找不到某个单词，就随机生成大小为 200
的词向量。

    
    
    num_words = min(max_features, len(word_index)) + 1
    print(num_words)
    # 14046
    
    # 初始化词嵌入矩阵
    embedding_matrix = np.zeros((num_words, embedding_dim))
    
    # 找到 word_index 每个单词的词向量，添加到词嵌入矩阵中
    for word, i in word_index.items():
        if i > max_features:
            continue
        embedding_vector = embeddings_index.get(word)
        if embedding_vector is not None:
            embedding_matrix[i] = embedding_vector
        else:
            # 如果没找到的话，就随机初始化一个词向量
            embedding_matrix[i] = np.random.randn(embedding_dim)
    
    embedding_matrix
    
    
    
     array([[ 0.        ,  0.        ,  0.        , ...,  0.        ,
             0.        ,  0.        ],
           [ 1.08179143,  1.95990525,  0.17293852, ..., -0.07549042,
             0.16186819, -0.92289775],
           [-0.071549  ,  0.093459  ,  0.023738  , ...,  0.33616999,
             0.030591  ,  0.25577   ],
           ...,
           [-0.34463999,  0.4181    ,  0.42930999, ...,  0.045373  ,
            -1.15499997,  0.21414   ],
           [-0.30818   , -0.48212999,  0.24180999, ...,  0.076972  ,
            -0.51968998, -1.19210005],
           [ 0.034607  ,  0.94485003, -0.18629999, ..., -0.23799001,
             0.69398999, -0.74716997]])
    

**第二个模型的整体结构和第一个是一样的** ，区别就在于 embedding_layer，在这里我们多了一些条件：

    
    
    embeddings_initializer=Constant(embedding_matrix)
    

由 embedding_matrix 作为词嵌入矩阵，并且在模型的训练过程中是不随之变化的 `trainable=False`。

    
    
    inputs_2 = Input(shape=(sequence_length,), dtype='int32')
    
    embedding_layer_2 = Embedding(num_words,
                                embedding_dim,
                                embeddings_initializer=Constant(embedding_matrix),
                                input_length=sequence_length,
                                trainable=False)(inputs_2)
    
    reshape_2 = Reshape((sequence_length, embedding_dim, 1))(embedding_layer_2)
    
    conv_0_2 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_2)
    conv_1_2 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_2)
    conv_2_2 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_2)
    
    maxpool_0_2 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0_2)
    maxpool_1_2 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1_2)
    maxpool_2_2 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2_2)
    
    concatenated_tensor_2 = Concatenate(axis=1)([maxpool_0_2, maxpool_1_2, maxpool_2_2])
    flatten_2 = Flatten()(concatenated_tensor_2)
    
    dropout_2 = Dropout(0.5)(flatten_2)
    output_2 = Dense(units=5, activation='softmax')(dropout_2)
    
    model_2 = Model(inputs=inputs_2, outputs=output_2)
    model_2.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    print(model_2.summary())
    

![](https://images.gitbook.cn/68cde560-7f40-11ea-9bbe-b79e71d58acf)

**训练模型：**

    
    
    batch_size = 32
    history_2 = model_2.fit(X_train, y_train, epochs=30, batch_size=batch_size, verbose=1, validation_split=0.2)
    
    
    
    plt.plot(history_2.history['acc'])
    plt.plot(history_2.history['val_acc'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()
    
    plt.plot(history_2.history['loss'])
    plt.plot(history_2.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()
    

![](https://images.gitbook.cn/7f1ec960-7f40-11ea-b497-6b28b57af19c)

**论文中的第三个模型 CNN-non-static：** 它和 static 一样是使用 Google 的 Word2Vector
预训练结果，但是会在训练过程中被微调。

这个模型结构和第二个几乎一样，只是在 Embedding 的 trainable=True 那里有所不同，其他都是一样的。

    
    
    inputs_3 = Input(shape=(sequence_length,), dtype='int32')
    embedding_layer_3 = Embedding(num_words,
                                embedding_dim,
                                embeddings_initializer=Constant(embedding_matrix),
                                input_length=sequence_length,
                                trainable=True)(inputs_3)
    
    reshape_3 = Reshape((sequence_length, embedding_dim, 1))(embedding_layer_3)
    
    conv_0_3 = Conv2D(num_filters, kernel_size=(3, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)
    conv_1_3 = Conv2D(num_filters, kernel_size=(4, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)
    conv_2_3 = Conv2D(num_filters, kernel_size=(5, embedding_dim), activation='relu', kernel_regularizer=regularizers.l2(3))(reshape_3)
    
    maxpool_0_3 = MaxPool2D(pool_size=(sequence_length - 3 + 1, 1), strides=(1,1), padding='valid')(conv_0_3)
    maxpool_1_3 = MaxPool2D(pool_size=(sequence_length - 4 + 1, 1), strides=(1,1), padding='valid')(conv_1_3)
    maxpool_2_3 = MaxPool2D(pool_size=(sequence_length - 5 + 1, 1), strides=(1,1), padding='valid')(conv_2_3)
    
    concatenated_tensor_3 = Concatenate(axis=1)([maxpool_0_3, maxpool_1_3, maxpool_2_3])
    flatten_3 = Flatten()(concatenated_tensor_3)
    
    dropout_3 = Dropout(0.5)(flatten_3)
    output_3 = Dense(units=5, activation='softmax')(dropout_3)
    
    model_3 = Model(inputs=inputs_3, outputs=output_3)
    model_3.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    print(model_3.summary())
    

![](https://images.gitbook.cn/a2f0dc70-7f40-11ea-bee5-61432feb39ea)

**训练模型：**

    
    
    batch_size = 32
    history_3 = model_3.fit(X_train, y_train, epochs=30, batch_size=batch_size, verbose=1, validation_split=0.2)
    
    
    
    plt.plot(history_3.history['acc'])
    plt.plot(history_3.history['val_acc'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()
    
    plt.plot(history_3.history['loss'])
    plt.plot(history_3.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()
    

![](https://images.gitbook.cn/b0315900-7f40-11ea-a205-47a8f48db192)

可以看到第三个模型采用 14 epoch 比较好，不过准确率要比前两个好一些。

**下面要构建论文中的第四个模型 CNN-multichannel：**

原文中是同时使用了 CNN-static 和 CNN-non-static 两种词向量，这里我们简化一下，用第一种模型的随机方式构建 3 个
channel，大家如果感兴趣，可以尝试换成和论文中一样的词嵌入方式。

下面的代码最后会构建出如下图所示的模型：

  * 其中每个 channel 的搭建都和第一个模型是类似的，包括 input-embedding-conv-drop-pool-flat；
  * 三个单独的 channel 建立好后要进行合并 `merged = concatenate([flat1, flat2, flat3])`；
  * 然后再由 dense 层得到输出层。

由于是三个管道，所以 model 的输入也是三个：

    
    
    input：model = Model(inputs=[inputs1, inputs2, inputs3], outputs=output)
    

当然在训练时，这三个 input 所接收的数据都是一样的 X_train。

![](https://images.gitbook.cn/de1ba690-7f40-11ea-b7b0-e1d3106a8702)

    
    
    from pickle import load
    from numpy import array
    from keras.preprocessing.text import Tokenizer
    from keras.preprocessing.sequence import pad_sequences
    from keras.utils.vis_utils import plot_model
    from keras.models import Model
    from keras.layers import Input
    from keras.layers import Dense
    from keras.layers import Flatten
    from keras.layers import Dropout
    from keras.layers import Embedding
    from keras.layers.convolutional import Conv1D
    from keras.layers.convolutional import MaxPooling1D
    from keras.layers.merge import concatenate
    
    # channel 1
    inputs1 = Input(shape=(sequence_length,), dtype='int32')
    embedding1 = Embedding(max_features, 200)(inputs1)
    conv1 = Conv1D(num_filters, kernel_size=4, activation='relu')(embedding1)
    drop1 = Dropout(0.5)(conv1)
    pool1 = MaxPooling1D(pool_size=2)(drop1)
    flat1 = Flatten()(pool1)
    
    # channel 2
    inputs2 = Input(shape=(sequence_length,))
    embedding2 = Embedding(max_features, 200)(inputs2)
    conv2 = Conv1D(num_filters, kernel_size=6, activation='relu')(embedding2)
    drop2 = Dropout(0.5)(conv2)
    pool2 = MaxPooling1D(pool_size=2)(drop2)
    flat2 = Flatten()(pool2)
    
    # channel 3
    inputs3 = Input(shape=(sequence_length,))
    embedding3 = Embedding(max_features, 200)(inputs3)
    conv3 = Conv1D(num_filters, kernel_size=8, activation='relu')(embedding3)
    drop3 = Dropout(0.5)(conv3)
    pool3 = MaxPooling1D(pool_size=2)(drop3)
    flat3 = Flatten()(pool3)
    
    # 合并
    merged = concatenate([flat1, flat2, flat3])
    
    # 输出层
    dense1 = Dense(10, activation='relu')(merged)
    output = Dense(5, activation='softmax')(dense1)
    
    
    model = Model(inputs=[inputs1, inputs2, inputs3], outputs=output)
    
    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
    
    print(model.summary())
    plot_model(model, show_shapes=True, to_file='multichannel.png')
    

**训练模型：**

    
    
    batch_size = 32 
    history = model.fit([X_train,X_train,X_train], y_train, epochs=10, batch_size=batch_size, verbose=1, validation_split=0.1, shuffle=True)
    
    
    
    plt.plot(history.history['acc'])
    plt.plot(history.history['val_acc'])
    plt.title('model accuracy')
    plt.ylabel('accuracy')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()
    
    plt.plot(history.history['loss'])
    plt.plot(history.history['val_loss'])
    plt.title('model loss')
    plt.ylabel('loss')
    plt.xlabel('epoch')
    plt.legend(['train', 'validation'], loc='upper left')
    plt.show()
    

![](https://images.gitbook.cn/ee28b0a0-7f40-11ea-9bbe-b79e71d58acf)

可以看出多通道的这个模型可以达到第三种模型差不多的准确率，而且只要一次训练就可以有同样的效果。

在这一节中我们复现了 Kim 的论文，大家如果感兴趣可以用 RNN 处理一下这个数据集，和 CNN 的对比一下效果。

* * *

参考文献：

  * Yoon Kim，2014，[Convolutional Neural Networks for Sentence Classification](https://arxiv.org/abs/1408.5882)
  * Hamish，[CNN for Sentence Classification](https://www.kaggle.com/hamishdickson/cnn-for-sentence-classification-by-yoon-kim/notebook)
  * Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. [GloVe: Global Vectors for Word Representation](https://nlp.stanford.edu/pubs/glove.pdf)
  * WILDML，[Understanding Convolutional Neural Networks for NLP](http://www.wildml.com/2015/11/understanding-convolutional-neural-networks-for-nlp/)
  * Thushan Ganegedara，Natural Language Processing with TensorFlow

