今天我们要学习机器翻译，机器翻译就是让计算机来帮我们将一种语言自动翻译成另一种语言。

现在机器翻译几乎已经成为我们生活中不可缺少的工具了，当我们在看英文文档时，可以直接把不懂的单词、句子，甚至段落，复制到谷歌翻译等工具中就可以立刻得到翻译结果。

现在还有一些 App 是用手机对着外语拍照，就可以直接进行翻译，或者对着手机讲话，就能有语音翻译，好像带着一个私人翻译一样。

有了机器翻译，我们的生活变得更加便利。世界上有 5000
多种语言，以前如果一个人需要和另一个国家的人流畅地进行沟通，就需要掌握这门外语，但是学习一种新的语言是需要很多成本的，需要付出很多时间和精力还不一定能学得很好。

现在我们可以将翻译这个任务交给机器，在一定程度上降低了沟通成本，也让我们的视野变得更广，当我们在查找某个问题的资料时可能除了英语还会查到日语俄语等语言的内容，这个时候不需要去特意的学习这些语言，只要输入到软件里就可以得到差不多准确的结果。

那么今天我们就来学习一下，我们生活中常用的机器翻译系统背后的原理是什么。

### 机器翻译的历史

首先让我们先来简单地看一下机器翻译技术的发展过程：

![](https://images.gitbook.cn/373f1420-7f30-11ea-b497-6b28b57af19c)

[图片来源](https://www.slideshare.net/IconicTranslation/machine-translation-the-
neural-frontier-74363623)

目前为止机器翻译主要经历了三个阶段的发展：

  1. RBMT：Rule based machine translation 基于规则的机器翻译
  2. SMT：Statistical machine translation 统计机器翻译
  3. NMT：Neural machine translation 神经机器翻译

#### **1\. 基于规则的机器翻译**

这种技术是指要提前建立很多很多语言规则，还要建立每一对语言的数百万词对的双语词典。

第一个有记录的项目是 IBM 的 Georgetown，它将六十多个俄语句子翻译成英语。

最开始的技术是基于单词的翻译，也就是用一个双语的字典将单词映射到单词。

**主要步骤**
是首先需要对文本进行解析，然后建立一个过渡表示用来生成翻译结果，此外需要建立很多复杂的规则集，包括单词形态、句法、语义信息等等，最后利用原来的语法结构将文本翻译成目标语言。

这种方法存在很多局限：

  1. 其中需要的语言规则非常复杂，词典数量庞大。
  2. 单词之间的映射不全是一对一的，例如一个俄语单词可能对应着多个英语单词。
  3. 而且这种方法没有考虑到语境，那么翻译的结果肯定不是特别准确。

这个时期的机器翻译结果还不怎么准确，而且非常慢，和人工翻译比起来消耗的资源更多，这些局限性造成机器翻译的技术发展一直比较缓慢。

#### **2\. 统计机器翻译**

统计机器翻译是指用统计模型从大量的语料库数据中学习如何将文本翻译成目标语言，这个时候最早的模型是 IBM 的 Brown/Mercer。

统计机器翻译的数学基础是贝叶斯定理，我们已知源语言的句子 f，想要得到它的目标语言翻译 e，通过贝叶斯定理，我们将目标 P(e|f) 进行转化：

![](https://images.gitbook.cn/7e47e4a0-7f30-11ea-9456-634f43a6106e)

[图片来源](http://nlp.postech.ac.kr/research/previous_research/smt/)

所以问题就变成了求解 P(f|e)P(e) 的最大值，其中，P(e) 用语言模型得到，P(f|e)
是翻译模型，再经过解码器求得最大值的解，就可以得到目标语句。

统计机器翻译将基于单词的翻译发展成基于短语的翻译，意思是最小翻译单元是短语，而不是只限于单个单词，这使得翻译的质量大大提高。

从上面的流程中我们可以看到这种方法的目标是要学习一个翻译模型，即给定一个原始句子，学习出几个不同候选翻译结果的概率分布。

这种方法也存在一定的局限：

  1. 虽然不需要定义大量的语法语义规则，但是它需要很多数据，需要维护两种语言的短语语料库，所以数据量比较大。
  2. 还有一个局限是它的解码过程比较困难，因为一个短语也会对应多个短语表达，于是就有了基于句法的翻译模型。

**基于句法的翻译：**

在这种方法中，原始句子会由一个语义树表示：

![](https://images.gitbook.cn/9160cf20-7f30-11ea-9e50-d754b71d2fe2)

例如上图所示，其中 NP 表示名词短语，VP 表示动词短语，S 表示一个句子。

用这种结构，通过改变树的节点顺序就可以改变主谓宾的顺序，这样就可以根据不同的目标语言的语法来调整短语之间的顺序，进而使翻译的质量提高。不过在将最终的叶子节点翻译成目标语言时，是使用单词对单词的方式。

#### **3\. 神经机器翻译**

神经机器翻译系统是在 2014 年流行起来，是指用神经网络模型去学习机器翻译中的统计模型。

**它的优势**
就是只需要一个端对端的系统，就可以直接得到原语言到目标语言之间的翻译，不再需要统计机器翻译中所需要的多个组件，而且省去了短语配对，语义树等复杂的特征工程，经过短短几年的时间，神经机器翻译系统的效果就超过了以往的翻译技术。

神经机器翻译最核心的模型就是 Encoder-Decoder 结构，它可以处理可变长度的输入输出序列：

![](https://images.gitbook.cn/a5a9d530-7f30-11ea-9e50-d754b71d2fe2)

编码器读取一个句子，并输出一个固定长度的向量，再由解码器翻译成目标语句。

我们前面提到过编码器解码器结构存在一定的问题，就是不论多长的句子都会被编码成一个固定长度的向量表示，当遇到比较长的输入时这种方法是有局限的，所以引入了注意力机制，它可以使翻译更加有效，在翻译每个单词时只去关注相关的信息，这样可以又快速又准确地进行翻译。

### Encoder-Decoder 用于机器翻译的原理

人类在翻译的时候，听到一个句子，先总结一下这句话表达了什么含义思想，然后再翻译成另一种语言，神经机器翻译系统也是同样的原理。

模型最早由 Sutskever 等人在论文 [_Sequence to Sequence Learning with Neural
Networks_](https://arxiv.org/pdf/1409.3215.pdf) 中提出。

Encoder-Decoder 的结构如下图所示：

![](https://images.gitbook.cn/bff2a750-7f30-11ea-a205-47a8f48db192)

  * 编码器会接收一句话，输出一个上下文向量
  * 解码器会接收上下文向量，输出最终的翻译
  * 最终的目标是要最大化这个对数似然函数：$ \frac{1}{N} \sum_{i=1}^N \log P (y_T | x_S) $
  * $y_T$ 是目标翻译的候选集合，我们要找到最好的结果，就是要满足这个式子：$y_T^{best} = argmax_{y \in Y_T} P(y_T | x_S^{infer}) = argmax_{y \in Y_T} \prod_{i=1}^M P(y_T^i | x_S^{infer})$

下面我们更详细地来看各个时间步是如何工作的。

![](https://images.gitbook.cn/e4490590-7f30-11ea-a918-478f642cdd64)

[图片来源](https://www.amazon.com/Natural-Language-Processing-TensorFlow-language-
ebook/dp/B077Q3VZFR)

**Encoder**

Encoder 中的 LSTM 单元我们可以记作 $LSTM_{enc}$：

  * 初始的 $c_0$ 和 $h_0$ 是零向量；
  * 输入序列为 $x_s^1, x_s^2 ... x_s^L$；
  * 每一个时间步经过一个 LSTM 单元：
    * 在第一个时间步，编码器接收第一个单词，输出隐藏状态 h1；
    * 在第二个时间步，编码器接收第二个单词和隐藏状态 h1，输出隐藏状态 h2；
    * 在第三个时间步，编码器接收第三个单词和隐藏状态 h2，输出隐藏状态 h3；
    * ……
  * 在编码器的最后一步得到最终的单元状态 $c_L$ 和隐藏状态 $h_L$。

**Context Vector**

  * Encoder 的最后一步也就是上下文向量 $v_c = c_L$、$v_h = h_L$，Encoder 和 Decoder 之间连接的关键就是这个向量。

**Decoder**

Decoder 中的 LSTM 记作 $LSTM_{dec}$，虽然 Encoder 和 Decoder 可以共享权重参数，但最好用不同的网络。

  * Decoder 初始状态是上下文向量 $c_0 = v_c$、$h_0 = v_h$；
  * 翻译结果由这个公式得到：$c_m, h_m = LSTM_{dec} (y_T^{m-1} | v, y_T^1, y_T^2 ... y_T^{m-2})$ 
    * 在第四个时间步，解码器接收 v，输出翻译的第一个单词；
    * 第五个时间步输出第二个单词；
    * 第六个时间步输出第三个单词；
    * ……
  * 由解码器得到 $h_m $后，再由 softmax 计算出 $y_T^m =softmax( w_{softmax} × h_m + b_{softmax} )$。

### 注意力机制用于机器翻译的原理

还记得我们之前学的注意力机制么：

![](https://images.gitbook.cn/bc896f30-7f31-11ea-a205-47a8f48db192)

在这里我们将注意力机制用在解码器中：

![](https://images.gitbook.cn/cadfa180-7f31-11ea-b497-6b28b57af19c)

  * 首先解码器会观察它接收到的所有隐藏状态 h1~h3，注意编码器的每个隐藏状态都和输入序列中相应的某个单词有较多的关联性；
  * 然后给每个隐藏状态打分；
  * 每个隐藏状态乘以它的 Softmax 分数，这样可以放大得分高的隐藏状态的影响，削弱得分低的隐藏状态的影响；
  * 加权平均得到的结果将作为第四步的上下文向量。

![](https://images.gitbook.cn/e8bb8570-7f31-11ea-9e50-d754b71d2fe2)

[图片来源](http://jalammar.github.io/visualizing-neural-machine-translation-
mechanics-of-seq2seq-models-with-attention/)

  * 在应用注意力机制的解码器中，编码器会将其中产生的所有隐藏状态都传递给解码器，而不是只传递最后一个状态；
  * 解码器接收 end 标记的词嵌入向量，和一个初始状态；
  * 在第四个时间步，解码器的这个 RNN 单元会生成一个输出和一个隐藏状态 h4；
  * 然后就要经过注意力这一步，我们会用编码器的隐藏状态和 h4 来计算一个上下文向量 c4；
  * 再将 h4 和 c4 串联成一个向量；
  * 最后将这个向量传递给一个前馈神经网络，这个前馈神经网络的输出结果就对应着解码器的这一个时间步的翻译结果。

### 模型的训练

知道了模型的具体原理，我们来看训练过程是怎样的。

  * 训练模型所用的数据是成对的源语言句子和目标翻译句子，例如： `( Ich ging nach Hause , I went home)`
  * 此外，还需要两个特殊的记号，开始标记和结尾标记 `<s>, </s>` 所以输入的数据就变成了这个样子：

    
    
    (<s> Ich ging nach Hause </s> , <s> I went home </s>)
    

#### **数据预处理**

然后用结尾标记补齐句子，虽然 LSTM
可以处理可变长度的句子，没有必要一定将句子都变成相同长度的，但是将长度固定住，可以方便进行批次数据处理，使输入句子的长度为 L，输出句子的长度为 M、L 和
M 可以不相等，如果句子的长度大于 L 和 M 则会被截断：

    
    
    (<s> Ich ging nach Hause </s> </s> </s> , <s> I went home </s> </s> </s>) 
    

然后对训练数据取 token：

    
    
    (['<s>' , 'Ich' , 'ging' , 'nach' , 'Hause' , '</s>' , '</s>' , '</s>'] , ['<s>' , 'I' , 'went' , 'home' , '</s>' , '</s>' , '</s>']) 
    

#### **翻转句子**

翻转这个小技巧可以有效地提高模型的表现，尤其是当两种语言有相同的语法结构时效果更好。

例如我们想要翻译 ABC，那么首先将它翻转顺序为 CBA：

    
    
    (['</s>' , '</s>' , '</s>' , 'Hause' , 'nach' , 'ging' , 'Ich' , '<s>'] , ['<s>' , 'I' , 'went' , 'home' , '</s>' , '</s>' , '</s>']) 
    

这样做可以使编码器和解码器之间有更好的信息连接，因为原句子和目标句子原封不动地连接起来后，这样相对应的两个单词之间的距离是一样的，但如果将句子翻转过来，相应的第一个单词之间距离就会非常近，在翻译系统中，起点有比较好的信息连接是非常重要的，可以使翻译质量向较好的方向发展。

经过上面的预处理后，正式进入训练步骤：

![](https://images.gitbook.cn/445a91a0-7f32-11ea-aef6-539c3826c714)

[图片来源](https://www.amazon.com/Natural-Language-Processing-TensorFlow-language-
ebook/dp/B077Q3VZFR)

  * 将 x 投入到编码器中，计算在 x 条件下的上下文向量 v；
  * 将 v 输入到解码器；
  * 从解码器得到预测结果 y，第 M 个预测值的计算公式是：$y_T^m =softmax( w_{softmax} × h_m + b_{softmax} ) $；
  * 再得到第 M 的位置最好的预测单词: $ w_T^m =argmax_{w_m \in V } P( \hat{y}_T^{(m,w_m)} | v, \hat{y}_T^1, ... , \hat{y}^{m−1})$；
  * 计算损失，损失函数为预测单词 $\hat{y}_T^m$ 和实际单词 $y_T^m$ 之间的 categorical cross-entropy；
  * 在这个损失函数下，优化编码器，解码器和 Softmax 层，

### 模型的推断

推断和训练过程稍微不同，前面三步是一样的，从第四步开始就有所不同：

![](https://images.gitbook.cn/69e02110-7f32-11ea-b7e8-9964a7b45d0e)

[图片来源](https://www.amazon.com/Natural-Language-Processing-TensorFlow-language-
ebook/dp/B077Q3VZFR)

在推断的时候，因为数据中没有目标句子，所以在编码器的结尾需要用一种方法来触发解码器，在这里，我们令解码器的第一个输入为 `<s>` 和 v：

  * 将 x 投入到编码器中；
  * 计算在 x 条件下的上下文向量 v；
  * 将 v 输入到解码器；
  * 解码器根据初始输入 `<s>` 和 v 预测 $\hat{y}_T^2$；
  * 在接下来的时间步，以 v 和前面所得结果为条件，预测下一个结果 $\hat{y}_T^m$。

### 模型的评估：BLEU 分数

BLEU 的意思是 Bilingual Evaluation Understudy，是一种自动评价翻译系统的方法。

它的公式是：

![](https://images.gitbook.cn/87184af0-7f32-11ea-bee5-61432feb39ea)

**下面我们通过一个例子来看如何计算 BLEU。**

例如我们有两个句子，一个是真实的翻译，一个是机器翻译的结果：

> Reference 1: The cat sat on the mat
>
> Candidate 1: The cat is on the mat

我们想要通过一个指标来看机器翻译的质量到底如何，首先想到一种指标是精确率，也就是在机器翻译的结果中有多少单词是真实翻译中也有的：

$$ Precision = \frac{ \sum_{unigram ∈ Candidate} Is Found In Ref (unigram)
}{|Candidate|} $$

所以 Candidate 1 机器翻译结果的精确率为 5/6。

**但是精确率这个指标有一个问题** ，就是如果我们的翻译结果中重复出现某个正确单词，那么就会严重的影响指标的可靠性，

例如有这样一个翻译结果：

> Candidate 2: The the the cat cat cat

很明显这句话的质量非常差，但是它在精确率下的得分却是 1，所以单纯用精确率来评估是不可靠的。

**所以需要对精确率的公式进行修正** ：

$$ Precision = \frac{ \sum_{unigram ∈ Candidate} Min(Occurences(unigram),
unigram_{max}) }{|Candidate|} $$

于是第一个翻译结果的准确率是 `(1 + 1 + 1 + 1 + 1)/ 6 = 5/6`，第二个翻译结果的准确率变为 `(2 + 1) / 6 =
3/6`，可见第二个结果已经不如第一个结果的分数，可见这种修正起到了效果。

另外这种修正还可以拓展到 n-gram。

此外精确率比较倾向于短小的句子，如果真实的翻译比较长，而机器翻译的结果比较短的话，这个结果仍然会得到一个比较高的分数，所以 **需要引入一个简短处罚** ：

$$BP = \begin{cases}1, & if & c > r \\\e^{1-r/c}, & if & c \leq r\end{cases}$$

其中 c 是指机器翻译结果的长度，r 是指真实翻译的句子长度，那么上面例子中，两个机器翻译结果的简短处罚都等于 1：

$$candidate 1 = e^{(1−(6/6))} = e^0 =1 $$ $$candidate 2 = e^{(1−(6/6))} = e^0
=1 $$

知道了精确率和简短处罚的公式，我们就可以得到最终的 BLEU 计算公式：

$$BLEU = BP \times exp (\sum_1^N w_n P_n) $$

首先计算 N 个 n-gram 精确率，然后计算它们的加权平均，最后再作用一个简短处罚，这样这个分数就可以避免毫无意义的句子却有较高分值的问题。

* * *

通过上面的内容，我们知道了神经翻译系统的模型原理、训练过程，如何预测结果，以及模型的评估，在下一篇我们将实际搭建一个神经机器翻译系统。

