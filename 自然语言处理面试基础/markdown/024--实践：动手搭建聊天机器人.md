### 什么是聊天机器人？

聊天机器人是一种人工智能系统，可以用文字或者语音和人类交流互动。﻿简单的如询问现在的天气怎么样、最新的新闻是什么，复杂一点的如手机出问题了询问一下要如何解决等等。

不过聊天机器人现在并不仅仅是只会这种简单的对话，也开始有变现的途径，比如一些实用场景：如辅助客服人员，可以查看资讯，可以帮忙下订单。很多零售业厂商的客服聊天机器人能够帮助客户解答
80% 的常见问题，极大地提高效率节省成本。

还可以植入广告，比如你在咨询一个厨师专业的聊天机器人，那么当你说怎么做 Pasta 呀，它在告诉你一系列流程后，就会顺便弹出一个文章链接“这五道用‘xx
辣椒酱’烹饪的菜，吃后绝对让你惊叹不已”，这款 Pasta
配这个调料不错哦。其中某辣椒酱就是赞助内容，这种原生广告的效果比传统的横幅广告的效果要好很多，所以未来可能会在聊天机器人里更常看到这种广告形式。

或者做用户调研，例如以前都是一些调研人员去大街上找人填问卷非常费时费力的工作，那么现在直接手机来条信息，就是机器人在以对话的形式进行询问，用户回答后，机器人还可以根据用户的回答智能地提问下一个问题，提高效率。

总之聊天机器人可以做很多事情：提供个性化的援助，分享品牌或产品更新推荐，提供促销信息，流程自动化，互动讲故事等等。

### 聊天机器人的商业应用举例：

**个人银行：**[Chip](https://getchip.uk/)

![](https://images.gitbook.cn/ecf87a90-7f2b-11ea-aef6-539c3826c714)

这是一款基于 AI 的自动理财 App，它可以连接到用户的银行账户，分析消费习惯，计算用户可以节省多少钱。在 App
的首页是个聊天功能，可以和机器人提出需求对话。

**医疗咨询：**[Babylon Health](https://www.babylonhealth.com/)

![](https://images.gitbook.cn/01515f20-7f2c-11ea-a918-478f642cdd64)

分析用户的病症，帮用户查找解决方案，并提出建议是否需要看医生做进一步检查。

**智能助理：**[Dunzo](https://angel.co/projects/558310-dunzo)

![](https://images.gitbook.cn/13d116e0-7f2c-11ea-9456-634f43a6106e)

基于 AI 的日常任务管理 App，通过对话形式的界面建立个人或者团队协作的任务清单。

**零售推荐：**[Mezi](https://mezi.com/)

![](https://images.gitbook.cn/513221f0-7f2c-11ea-aef6-539c3826c714)

可以智能分析用户的需求，提供旅行、时尚、礼物上的个性化推荐方案。

### 为什么需要聊天机器人？

我们以客服机器人为例，因为我们每个用户在接触任何一个产品，任何一个公司时，任何时间、任何地点、任何场合，都会有很多很多问题想问，而这时候就需要客服来提供解答和服务。例如电商、互联网金融、在线教育、游戏娱乐等领域，就对客服的需求很大。刚到国外的留学生可能还会听到前辈们说，觉得自己口语不好的话，就打客服电话，找他们练，一般不会不耐烦的，可见客服的哥哥姐姐们会有多辛苦。而如果一些常见的简单的问题，可以被聊天机器人自动处理的话，那应该会解放出一批劳动力了。

**为什么 AI 在客服机器人领域开花结果？**

客服是伴随着交易出现就自然产生的，近年来一些新兴企业，尤其是互联网企业增长飞速，他们的交易又大多依赖线上平台，就对客服软件有了巨大的市场需求。智能云客服软件的涌现，又让许许多多的中小企业能够通过简单的方式，定制自己的客服系统，目前，电商、互联网金融、在线教育、游戏娱乐等，对客服的需求都很大。

**客服聊天机器人的基本功能？**

针对不同企业，客服机器人要处理的问题也是不同的。一方面，客服机器人系统可以把企业的 FAQ
列表快速导入知识库，另一方面可以在特定行业中积累语料，扩充企业的知识库。

有一些客服机器人也已经具备学习能力，它可以在不断地使用过程中，根据新问题出现的频率来主动扩充知识库。例如一家以前做电视的企业，推出手机产品后，因为客服问题中频繁出现手机，智能客服机器人就会主动将“手机”加入知识库中。

另外客服机器人不像微软小冰、Siri 一样致力于和人建立情感联系，比如之前人类用户与小冰的平均每次对话轮数可达到 23
轮，但是在客服上，如果机器人要与前来咨询的客户对话 23 轮才能给出答案，你可以想象客户的反应会是什么。

### 如何搭建聊天机器人？

一个聊天机器人一般包括四个基本处理阶段：Parse 解析、Analyze 分析、Generate 生成、Execute 执行。

  * Parse：用来从自然语言的文本中提取特征。这部分涉及的内容有分词、正则表达、标签、命名实体识别、提取信息、降维等。﻿﻿
  * Analyze：从情感、语法、语义的角度上生成和组合特征。﻿包括检查拼写、检查语法、分析情绪、分析样式﻿等。
  * Generate：用模板、搜索或者语言模型生成回复语句。﻿主要有 MCMC（Markov Chain Monte Carlo）、LSTM﻿ 等模型。
  * Execute：根据对话历史和主题选择下一个回复句。﻿这部分要做的事情有归纳和分类、更新模型、更新目标、更新对话计划、选择回复等。

要将一个完整的聊天机器人系统讲出来需要很多篇幅，这里我们只看如何用 seq2seq 模型来搭建聊天机器人。

在这个实战中我们使用的数据是 [Cornell
电影对话语料库](https://www.cs.cornell.edu/~cristian/Cornell_Movie-
Dialogs_Corpus.html)，可以看到有 ZIP 数据下载的链接，解压到项目文件夹中，这个数据集是非常有名的语料库，包括 10292
对电影角色的 249911 条对话，我们用这个来训练一个简易聊天机器人。

#### **1\. 加载数据**

    
    
    import numpy as np 
    import pandas as pd 
    import tensorflow as tf
    import re
    import time
    import os
    from keras.preprocessing.sequence import pad_sequences
    from keras.utils import plot_model
    from keras.models import Model
    from keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, concatenate
    from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
    
    lines = open('movie_lines.txt', encoding='utf-8', errors='ignore').read().split('\n')
    conv_lines = open('movie_conversations.txt', encoding='utf-8', errors='ignore').read().split('\n')
    

先看一下对话数据是什么样的。

movie_lines.txt 的格式是对话 line 的 ID、说这话的角色 ID、电影的 ID、角色的名字、line 中的文本，其中 `+++$+++`
是分隔符：

    
    
    lines[:5]
    
    
    
    ['L1045 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ They do not!',
     'L1044 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ They do to!',
     'L985 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ I hope so.',
     'L984 +++$+++ u2 +++$+++ m0 +++$+++ CAMERON +++$+++ She okay?',
     "L925 +++$+++ u0 +++$+++ m0 +++$+++ BIANCA +++$+++ Let's go."]
    

movie_conversations.txt 的数据格式是第一个角色的 ID、第二个角色的 ID、对话所在电影的 ID、最后是 line ID
的列表，角色和电影的信息可以在 movie_characters_metadata.txt 和 movie_titles_metadata.txt 中找到：

    
    
    conv_lines[:5]
    
    
    
    ["u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L194', 'L195', 'L196', 'L197']",
     "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L198', 'L199']",
     "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L200', 'L201', 'L202', 'L203']",
     "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L204', 'L205', 'L206']",
     "u0 +++$+++ u2 +++$+++ m0 +++$+++ ['L207', 'L208']"]
    

建立一个字典，将 id 和它的文本对应起来：

    
    
    id2line = {}
    for line in lines:
        _line = line.split(' +++$+++ ')
        if len(_line) == 5:
            id2line[_line[0]] = _line[4]
    
    
    
    'L3488': "Your father blamed you for your mother's blindness?",...
    

建立一个表包含将 lines 和它包含的所有的 id 对应起来：

    
    
    convs = []
    for line in conv_lines[:-1]:
        _line = line.split(' +++$+++ ')[-1][1:-1].replace("'","").replace(" ","")
        convs.append(_line.split(','))
    
    
    
    ['L6511', 'L6512', 'L6513', 'L6514', 'L6515'],...
    

可以打印 convs 中一个例子看看数据形式：

    
    
    for k in convs[110]:
        print (k, id2line[k])
    
    
    
    L231 Hey.
    L232 Are you lost?
    L233 Nope - just came by to chat
    L234 We don't chat.
    L235 Well, actually, I thought I'd run an idea by you.  You know, just to see if you're interested.
    L236 We're not.
    

#### **2\. 将对话文本整理成问答对的形式**

    
    
    questions = []
    answers = []
    
    for conv in convs:
        for i in range(len(conv)-1):
            if conv[i] not in id2line.keys(): 
              continue       
            questions.append(id2line[conv[i]])
            answers.append(id2line[conv[i+1]])
    
    
    
    questions[0]: 'Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.'
    answers[0]: "Well, I thought we'd start with pronunciation, if that's okay with you."
    
    
    
    print(len(questions))
    print(len(answers))
    
    
    
    221616
    221616
    

#### **3\. 数据预处理，将字母都变成小写，将不重要的字符删除**

    
    
    def clean_text(text):
    
        text = text.lower()
    
        text = re.sub(r"i'm", "i am", text)
        text = re.sub(r"he's", "he is", text)
        text = re.sub(r"she's", "she is", text)
        text = re.sub(r"it's", "it is", text)
        text = re.sub(r"that's", "that is", text)
        text = re.sub(r"what's", "that is", text)
        text = re.sub(r"where's", "where is", text)
        text = re.sub(r"how's", "how is", text)
        text = re.sub(r"\'ll", " will", text)
        text = re.sub(r"\'ve", " have", text)
        text = re.sub(r"\'re", " are", text)
        text = re.sub(r"\'d", " would", text)
        text = re.sub(r"\'re", " are", text)
        text = re.sub(r"won't", "will not", text)
        text = re.sub(r"can't", "cannot", text)
        text = re.sub(r"n't", " not", text)
        text = re.sub(r"n'", "ng", text)
        text = re.sub(r"'bout", "about", text)
        text = re.sub(r"'til", "until", text)
        text = re.sub(r"[-()\"#/@;:<>{}`+=~|.!?,]", "", text)
        text = " ".join(text.split())
        return text
    
    clean_questions = []
    for question in questions:
        clean_questions.append(clean_text(question))
    
    clean_answers = []    
    for answer in answers:
        clean_answers.append(clean_text(answer))
    

对比一下预处理前后的数据：

    
    
    r = np.random.randint(1,len(questions))
    print ('original text......')
    for i in range(r, r+3):
        print(questions[i])
        print(answers[i])
        print()
    print ('cleaned text......')
    for i in range(r, r+3):
        print(clean_questions[i])
        print(clean_answers[i])
        print()
    
    
    
    original text......
    My goodness, don't you open your presents until Christmas morning?
    No.
    
    No.
    We open ours on Christmas Eve. That's considered proper.
    
    We open ours on Christmas Eve. That's considered proper.
    Well, I guess we're not a very proper family.
    
    cleaned text......
    my goodness do not you open your presents until christmas morning
    no
    
    no
    we open ours on christmas eve that is considered proper
    
    we open ours on christmas eve that is considered proper
    well i guess we are not a very proper family
    

去掉 questions 和 answers 中单词数小于 1 和大于 20 的：

    
    
    min_line_length = 2
    max_line_length = 20
    
    short_questions_temp = []
    short_answers_temp = []
    
    for i, question in enumerate(clean_questions):
        if len(question.split()) >= min_line_length and len(question.split()) <= max_line_length:
            short_questions_temp.append(question)
            short_answers_temp.append(clean_answers[i])
    
    short_questions = []
    short_answers = []
    
    for i, answer in enumerate(short_answers_temp):
        if len(answer.split()) >= min_line_length and len(answer.split()) <= max_line_length:
            short_answers.append(answer)
            short_questions.append(short_questions_temp[i])
    
    print(len(short_questions))
    print(len(short_answers))
    
    
    
    138335
    138335
    

选择 60000 个样本作为训练集：

    
    
    num_samples = 60000  
    short_questions = short_questions[:num_samples]
    short_answers = short_answers[:num_samples]
    

给 answers 中的文本加上开始和结束的标记：

    
    
    short_answers2 = []
    for ans in short_answers:
        ans = '<SOS> ' + ans + ' <EOS>'
        short_answers2.append(ans)
    

建立 vocabulary 统计单词出现的次数：

    
    
    vocab = {}
    
    for question in short_questions:
        for word in question.split():
            if word not in vocab:
                vocab[word] = 1
            else:
                vocab[word] += 1
    
    for answer in short_answers2:
        for word in answer.split():
            if word not in vocab:
                vocab[word] = 1
            else:
                vocab[word] += 1
    
    
    
    'lawyer': 79,
     'shit': 674,
    

在 vocab 中只使用出现次数超过 20 的，稀有的单词将会被替换成 `<UNK>`，最后我们只使用 2559 个词汇：

    
    
    threshold = 20
    count = 0
    for k,v in vocab.items():
        if v >= threshold:
            count += 1
    
    print("Size of total vocab:", len(vocab))
    print("Size of vocab we will use:", count)
    
    
    
    Size of total vocab: 27223
    Size of vocab we will use: 2559
    

给词汇表中的单词建立数字索引，用来将文本数据转化为数字：

    
    
    vocab_to_int = {}
    
    word_num = 0
    for word, count in vocab.items():
        if count >= threshold:
            vocab_to_int[word] = word_num
            word_num += 1
    

给特殊字符添加索引：

    
    
    codes = ['<PAD>','<UNK>']
    for code in codes:
        code_int = len(vocab_to_int)
        vocab_to_int[code] = code_int
    
    # 将原本第一个索引为 0 的单词与 PAD 的索引交换    
    for i, v in vocab_to_int.items():
        if v == 0:
            vocab_to_int[i] = vocab_to_int['<PAD>']
    
    vocab_to_int['<PAD>'] = 0
    
    print (vocab_to_int['<PAD>'])
    print (vocab_to_int['<UNK>'])
    
    
    
    0
    2560
    

建立数字索引到单词的字典，用于将预测结果转化为文本：

    
    
    int_to_vocab = {v_i: v for v, v_i in vocab_to_int.items()}
    

接下来要将文本转化为数字，其中不在 vocab 考虑范围内的单词标记为 `<UNK>`：

    
    
    questions_int = []
    
    for question in short_questions:
        ints = []
        for word in question.split():
            if word not in vocab_to_int:
                ints.append(vocab_to_int['<UNK>'])
            else:
                ints.append(vocab_to_int[word])
        questions_int.append(ints)
    
    answers_int = []
    
    for answer in short_answers2:
        ints = []
        for word in answer.split():
            if word not in vocab_to_int:
                ints.append(vocab_to_int['<UNK>'])
            else:
                ints.append(vocab_to_int[word])
        answers_int.append(ints)
    
    print("First question:", questions_int[0])
    print("First answer:", answers_int[0])
    
    
    
    First question: [2559, 1, 2, 3, 4, 5, 6, 2560, 7, 8, 9, 10, 6, 11]
    First answer: [2557, 12, 13, 2560, 14, 2560, 14, 2560, 15, 16, 2558]
    

补齐：

    
    
    encoder_input_data = pad_sequences(questions_int, maxlen=max_line_length, value=vocab_to_int['<PAD>'], padding='post') # 补齐到 max_line_length
    decoder_input_data = pad_sequences(answers_int, maxlen=max_line_length+2, value=vocab_to_int['<PAD>'], padding='post') # 补齐到 max_line_length + start + end
    

构造 decoder 数据：

    
    
    decoder_target_data = np.zeros(
        (len(answers_int), max_line_length+2, len(vocab_to_int)), #memory error occurs after 3500
        dtype='float32')
    
    for i, target_seq in enumerate(answers_int):
        for t, seq in enumerate(target_seq):
            if t > 0:
                decoder_target_data[i, t - 1, seq] = 1.
    

#### **4\. 构造模型**

    
    
    embedding_size = 200
    
    encoder_inputs = Input(shape=(None,))
    en_x=  Embedding(len(vocab_to_int), embedding_size)(encoder_inputs)
    encoder = Bidirectional(LSTM(100, return_state=True))
    encoder_outputs, state_h_1, state_c_1, state_h_2, state_c_2 = encoder(en_x)
    
    state_h = concatenate([state_h_1, state_h_2], axis=1)
    state_c = concatenate([state_c_1, state_c_1], axis=1)
    encoder_states = [state_h, state_c]
    
    decoder_inputs = Input(shape=(None,))
    dex=  Embedding(len(vocab_to_int), embedding_size)
    final_dex= dex(decoder_inputs)
    decoder_lstm = LSTM(200, return_sequences=True, return_state=True)
    decoder_outputs, _, _ = decoder_lstm(final_dex,
                                         initial_state=encoder_states)
    decoder_dense = Dense(len(vocab_to_int), activation='softmax')
    decoder_outputs = decoder_dense(decoder_outputs)
    model = Model([encoder_inputs, decoder_inputs], decoder_outputs)
    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc']) 
    
    model.summary()
    

![](https://images.gitbook.cn/43ad1af0-7f2f-11ea-bee5-61432feb39ea)

#### **5\. 训练模型**

    
    
    mcp_save = ModelCheckpoint('Best_weights_movie_word.hdf5', save_best_only=True, monitor='val_loss', mode='min')
    
    model.fit([encoder_input_data, decoder_input_data], decoder_target_data,
              batch_size=128,
              epochs=40,
              validation_split=0.05,
              callbacks= [mcp_save])
    
    model.save('bot.h5')
    

#### **6\. 建立采样模型**

    
    
    encoder_model = Model(encoder_inputs, encoder_states)
    encoder_model.summary()
    
    decoder_state_input_h  = Input(shape=(200,))
    decoder_state_input_c = Input(shape=(200,))
    decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]
    
    final_dex2= dex(decoder_inputs)
    
    decoder_outputs2, state_h2, state_c2 = decoder_lstm(final_dex2, initial_state=decoder_states_inputs)
    decoder_states2 = [state_h2, state_c2]
    decoder_outputs2 = decoder_dense(decoder_outputs2)
    decoder_model = Model(
        [decoder_inputs] + decoder_states_inputs,
        [decoder_outputs2] + decoder_states2)
    
    
    
    def decode_sequence(input_seq):
        # 将 input 编码为状态向量
        states_value = encoder_model.predict(input_seq)
        # 生成空的目标序列
        target_seq = np.zeros((1,1))
        # 给目标序列加上开始标记
        target_seq[0, 0] = vocab_to_int['<SOS>']
    
        stop_condition = False
        decoded_sentence = ''
        while not stop_condition:
            output_tokens, h, c = decoder_model.predict(
                [target_seq] + states_value)
    
            # 采样一个 token
            sampled_token_index = np.argmax(output_tokens[0, -1, :])
            sampled_char = int_to_vocab[sampled_token_index]
            decoded_sentence += ' '+sampled_char
    
            if (sampled_char == '<EOS>' or
               len(decoded_sentence) > 52):
                stop_condition = True
    
            # 更新目标序列
            target_seq = np.zeros((1,1))
            target_seq[0, 0] = sampled_token_index
    
            # 更新状态
            states_value = [h, c]
    
        return decoded_sentence
    

可以看看模型的效果：

    
    
    for i in range(50):
        seq_index = np.random.randint(1, len(encoder_input_data))
        input_seq = encoder_input_data[seq_index: seq_index + 1]
        decoded_sentence = decode_sequence(input_seq)
        print('-')
        print('Input sentence:', short_answers[seq_index: seq_index + 1])
        print('Decoded sentence:', decoded_sentence)
    

![](https://images.gitbook.cn/5c1c2400-7f2f-11ea-a205-47a8f48db192)

可以看到有些 answer 还是意思上还能过得去，有些觉得答非所问。

以上就是用 seq2seq
模型搭建的简易聊天机器人，关于聊天机器人的论文和模型有很多，还可以做很多优化，例如让计算效率更高，用注意力模型让对话更准确，让回答内容更有内涵有感情等等，感兴趣的朋友可以进行拓展。

* * *

参考文献：

  * Hobson Lane, Hannes Hapke, Cole Howard, _Natural Language Processing in Action_

