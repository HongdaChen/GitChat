今天这篇文章是本专栏的最后一篇，感谢大家可以坚持学习到这里！

在“ **第 1 部分：深度学习基础知识** ”中，我们知道了自然语言处理的几大热门研究领域，每个主题介绍了一些经典的研究论文，让大家可以对 NLP
有一个全局的了解。介绍了神经网络的基础知识，用 3W 方法解释了深度学习的常用核心概念，还有应用深度学习的 8 个基本步骤，并学习了 TensorFlow
1&2、Keras 的使用。这些都是我们开展后续模型理论和实战应用学习的基础。

![](https://images.gitbook.cn/84f5dfe0-a6e1-11ea-b3af-85060ac145d6)

**第 2 部分：循环神经网络基础** ，这一部分我们对 RNN 和 LSTM
的模型结构，数学原理，模型的前向计算和反向传播的推导，模型的思想都做了非常详细的讲述。很多人可能会觉得现在是 Attention
的时代，这些模型都要过时了，其实不然，每个模型都有它适用的问题范围，在面对一个问题时，可以尝试所有适合做这个问题的模型，再找到其中最优的。另外，我觉得最有价值的是这些模型用到的基础数学工具，比如仅仅将一个简单的线性函数和一个简单的非线性函数组合起来就能起到控制信号记忆信息的作用，它们就像是一个
LEGO 零件一样，在我们需要建立模型解决自己的实际问题时，就可以把具有相应功能的组件用起来。

![](https://images.gitbook.cn/ae35df40-a6e1-11ea-a44e-eb0e47a99655)

**第 3 部分：词嵌入** 中，我们也细致地学习了 CBOW、Skip-gram、GloVe
的模型理论和如何应用。词嵌入是NLP中非常重要的技术，它能够让模型更好地理解文本中单词短语句子的含义，就像我们人类在学会理解之前，先要学习文字的意义一样。模型对单词的理解越好，做出来的任务也会越优秀。所以词嵌入也是现在
NLP 重要的研究方向，这一部分只是讲了经典的模型，大家可以继续探索。

![](https://images.gitbook.cn/d203d3a0-a6e1-11ea-a2e8-dd9884b5f53f)

**第 4 部分：循环神经网络的改进** ，这一部分我们讲了几个 LSTM
的改进模型、Peephole、GRU，还有双向、深层网络，这里除了文章中讲到的原理和实战，我觉得拓展的思想也值得学习，水平方向拓展到双向，垂直方向拓展到深层，还有化繁为简的
GRU 等等，这些都可以应用到未来的模型构建中。

![](https://images.gitbook.cn/1bf06320-a6e2-11ea-9f5c-7744995f9c7d)

**第 5 部分：Seq2seq 和 Attention** ，在这里我们学习了 Encoder–Decoder 结构和注意力机制，这些概念也都是现在
Transformer 和 BERT 等优秀模型的基础。 Encoder–Decoder 结构比之前的 RNN 又有了突破，一个 Encoder
编码器可以将输入的文本序列转化为一个固定维度的“上下文向量”，一个 Decoder 解码器又将这个“上下文向量”预测为目标序列。而且在 Encoder 和
Decoder 上还都可以做双向和深层的拓展，模型的性能也越来越强。不过凡事都有两面性，seq2seq
强大的同时也依然存在着瓶颈，于是就有了注意力机制来改善这些问题。在课程中我们学习了 Bahdanau attention 和 Luong attention
两种常用的注意力模型，其实还有一些其他比较著名的模型，大家可以用类似的方法去研究它们的原理。

![](https://images.gitbook.cn/2e420600-a6e2-11ea-868f-cdbdfc614a23)

**第 6 部分：卷积神经网络的应用** ，这一部分我们先是学习了 CNN 的几个基本结构，NLP 和 CV
其实有很多模型是可以通用的，我们要学会跨界思维，将另外一个领域的研究思想用到自己的领域中会碰撞出不一样的火花。将一维的文本数据转化成二维的形式，就能让
CNN
像处理图像数据一样处理文本。卷积层可以一小块区域地提取特征，不同深度的还可以提取出不同级别的特征；池化层可以一定程度地降维，高效地使用最主要的特征；全连接层可以学习到全局的信息；可见这些技术的思想在
NLP 中也是可以拿过来用的。

![](https://images.gitbook.cn/153ce200-a6e3-11ea-bf7b-853a805636f7)

**第 7 部分：Transformer、BERT、XLNet**
，这里我们介绍了三种当下火热并且在多个自然语言处理任务上有着卓越表现的模型，通过对它们的原理和实战的学习，有了基础的认识，再去研究一些相关的新模型就会顺畅很多。

![](https://images.gitbook.cn/26f970d0-a6e3-11ea-bf7b-853a805636f7)

自从 2018 年预训练语言模型突破了自然语言理解和生成任务的极限，这些模型开始逐渐成为主导，NLP 领域也有了新的趋势，尤其是近两年“预训练 +
微调”的技术非常火热， **ULMFiT、CoVe、ELMo、OpenAI GPT、BERT、OpenAI
GPT-2、XLNet、RoBERTa、ALBERT** 这些模型都将 NLP
推向了新的高度，当然它们也伴随着数据量大计算成本大等问题，不过相信这些在未来也会一步步得到解决，此外由于越来越多的企业需要智能聊天机器人的服务、强化学习、语义搜索、认知交流等技术也是比较重要的研究方向。大家如果对这些方面感兴趣，下一步就可以继续进行更全面的学习和应用了。

我将本专栏涉及的主要模型之间的关系做成了下面这个图，大家在复习的时候也可以按照这个路线去回顾，也可以以此为基础搭建自己的更系统的 NLP 知识体系。

![](https://images.gitbook.cn/4b8b7880-a6e3-11ea-b3af-85060ac145d6)

变的是模型，不变的是底层逻辑，我们要懂得过去，也要能展望未来。

在本专栏中，我们有对重要模型的数学原理和思想做了详尽的推导，有大量的实战，并且代码中涉及到的知识点也做了详细的解释，我们有从零搭建模型，有复现论文模型，有
Kaggle 实战，还对常见面试题的知识点做了讲解，希望这里用到的每个技能都能起到抛砖引玉的作用，让您能够将其用到更广的地方。

希望这个专栏可以给大家的学习带来一些启发，对您的工作和学习有所帮助！也希望大家将这些知识用起来，如果暂时没有实战的机会，那么就多多参加 Kaggle
等平台的 NLP 竞赛项目，积累更多的实战经验和技巧。

**非常感谢为这个专栏辛苦付出的各位 GitChat 编辑！**

**最后由衷地感谢大家的订阅、喜欢、耐心的等待和支持！**

