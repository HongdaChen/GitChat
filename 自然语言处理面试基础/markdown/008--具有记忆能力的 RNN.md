在导读部分我们提到了序列数据，它的一个重要特性是具有顺序，而 RNN 的记忆性使它可以处理序列数据。

现实世界有很多问题的研究对象都是序列数据，尤其是自然语言处理中几乎所有问题都和序列数据相关。段落是句子的序列，句子是单词的序列，单词是字母的序列，音频视频也是由帧组成的序列。

在所有这些应用中，序列数据的顺序是很重要的，顺序发生了变化，含义也会发生变化。

简单的前馈神经网络处理这类问题是非常受限制的，因为它们假设输入数据是独立的，而这样显然会丢失掉数据的很多模式。

下面让我们具体看看 RNN 的结构和计算原理，来看它是如何具有处理序列数据所需的记忆能力的。

**本文将讲述以下内容：**

  1. RNN 模型 
  2. RNN 的前向计算
    * 为什么说 RNN 具有记忆功能？
  3. 损失函数
  4. 反向传播：BPTT 
    * RNN 存在梯度消失／爆炸问题的原因是什么？
    * 梯度消失／爆炸的解决方案有什么？ 
  5. 应用举例: 
    * 用基本的 RNN 识别垃圾邮件

### 1\. RNN 模型

RNN 译为循环神经网络，即这种网络的神经元之间形成了一个有向循环。

1\. 那首先来看一下基本 RNN 的结构：

![FOvRsK](https://images.gitbook.cn/FOvRsK.jpg)

我们可以以命名实体识别问题为例来看结构图，即识别一句话中的人名、地名等实体名词。

在时刻 t，输入为 $x^{<t>}$，即读入句子的一个单词， 然后计算出 $a^{<t>}$，再计算
$\hat{y}^{<t>}$，当对应单词是人名时值为 1，否则为 0。

和一般的前馈神经网络的区别在于，在计算 $a^{<t>}$ 时，不仅仅用到了 $x^{<t>}$，还用到了 $a^{<t－1>}$，这正是循环所在。

隐藏层的激活值也被称为 state 状态，为了保持结构的统一，我们在序列的最初时刻加一个激活值 $a^{<0>}$，一般初始化为零向量或者随机生成。

2\. 将模型表示为每个时间步的通用公式为：

$$a^{<t>}=f(W_aa^{<t-1>}+W_xx^{<t>}+b_a)$$

$$\hat{y}^{<t>} = g( W_{y} a^{<t>} + b_{y})$$

其中，连接输入层和隐藏层的参数用 $W_{x}$ 来表示，在每个时间步的输入和隐藏层之间都是相同的 $W_{x}$，水平方向的激活值到隐藏层的参数用
$W_{a}$ 表示，在每个时间步也都相同，隐藏层到输出值的参数用 $W_{y}$ 表示，同样在每个时间步也都是相同的。

也就是说每个时间步的参数是共享的：

![](https://images.gitbook.cn/46e9f160-2f99-11ea-b6a8-fbb27a899f8c)

了解了 RNN 的结构，下面看如何训练 RNN。

神经网络的训练主要分为下面几个步骤：

  1. 初始化 weights 和 biases
  2. 前向计算：计算出 $a^{<t>}$，用 Sigmoid、Softmax 或 linear function 等得到了每个时间步的输出 $\hat{y}^{<t>}$
  3. 计算损失：用预测标签和实际的标签进行比较，计算损失函数
  4. 反向传播：求出损失函数对参数 W、b 的梯度
  5. 然后通过梯度下降等方法进行梯度更新，重复第 2 到第 4 步使损失越来越小直到最小值。

### 2\. RNN 的前向计算

通过前向计算可以得到每个时间步的激活值 $a^{<t>}$ 和输出值 $\hat{y}^{<t>}$：

![aikrDG](https://images.gitbook.cn/aikrDG.jpg)

具体过程是，RNN 按照从左到右的顺序读取 input 序列，将第一个单词 $x^{<1>}$ 投入到网络中：

  * 先计算 $a^{<1>}$ ：这时并不是只用 $x^{<1>}$，还需要初始的激活函数，即 $a^{<0>}$ 也会被传递到第一个时间步。此处的激活函数常用 Tanh，有时用 ReLU，当然选 Tanh 会存在梯度消失问题，不过可以通过其他方法解决。
  * 再计算 $\hat{y}^{<1>}$：这里用的激活函数可以是和计算 a 时用不同的，二分类用 Sigmoid，多分类用 Softmax。例如，在命名实体识别问题的话，这个输出的值 $\hat{y}^{<1>}$ 表示输入的单词是否是人名。

这样就完成了由 $a^{<0>}$ 和 $x^{<0>}$ 计算 $a^{<1>}$ 和 $\hat{y}^{<1>}$ 的过程。

以此类推，从左到右，进行前向计算，得到每个时间步的 $a^{<t>}$ 和 $\hat{y}^{<t>}$。

一直到最后一个时间步，输入最后一个单词 $x^{<T_x>}$ 并预测 $\hat{y}^{<T_y>}$，RNN 都会将前一步的激活值传递给当前步。

#### 2.1 为什么说 RNN 具有记忆功能？

由前向计算的过程可知，RNN 在每个时间步都会将前一步的激活值传递给当前步。

例如当我们在预测时间步 3 的 $\hat{y}^{<3>}$ 时，用到的不仅仅是 $x^{<3>}$，还有 $x^{<1>}$ 和 $x^{<2>}$
的信息，前面时间步的 x 的信息会通过水平方向传递给当前步。

通过公式来看就是，因为 $a^{<t>}$ 和 $\hat{y}^{<t>}$ 的公式如下：

$\hat{y}^{<t>} = g( W_{y} a^{<t>} ) $

$a^{<t>} = f( W_{x} x^{<t>} + W_{a} a^{<t-1>} )$

当我们计算时间步 t 的输出 $\hat{y}^{<t>}$ 时，将激活值 $a^{<t>}$ 进行替换，可以看到公式中包含前面所有时间步的
$x^{<t>}$ 的信息：

$\hat{y}^{<t>} = g( W_{y} a^{<t>} )$

$= g( W_{y} f( W_{x} {x^{<t>}} + W_{a} a^{<t-1>} ) )$

$= g( W_{y} f( W_{x} {x^{<t>}} + W_{a} f( W_{x} {x^{<t-1>}} + W_{a} a^{<t-2>}
) )$

$= g( W_{y} f( W_{x} { x^{<t>} } + W_{a} f( W_{x} {x^{<t-1>}} + W_{a} f( W_{x}
{x^{<t-2>}} + ... ) ) )$

即 RNN 的状态取决于当前输入和先前的状态，而先前状态又取决于它的输入和它之前的状态，因此状态可以间接访问序列的所有先前输入，RNN
正是以这种方式保存着过去的记忆。

RNN
的这种激活值水平传递的循环结构可以将信息存储在网络中，这样就可以利用过去的信息，对具有高度时间依赖性的数据进行处理和预测，如手写识别，语音识别等序列数据任务都可以用
RNN 来解决。

### 3\. 损失函数

由前向计算得到网络的预测输出值 $\hat{y}^{<t>}$ 后，需要定义损失函数来衡量预测值和实际值之间的差距：

在分类问题中，损失函数常用交叉熵损失函数，回归问题中常用均方差函数。

以命名实体识别问题为例，单个时间步 t 的损失函数对应的是一个单词的损失：

$ {L}^{<t>} (\hat{y}^{<t>}, {y}^{<t>} ) = -{y}^{<t>} \log \hat{y}^{<t>} -
(1-{y}^{<t>}) \log(1-\hat{y}^{<t>}) $

如果这个单词是名字，那么实际的 ${y}^{<t>}$ 就是 1，神经网络输出的 $\hat{y}^{<t>}$ 就是它是名字的概率。

t 从 1 开始，到 $T_x$ 或者 $T_y$（在命名实体识别例子中，$T_x = T_y$），每一步的 $\hat{y}^{<t>}$
都可以得到一个损失 ${L}^{<t>}$，那么整体的损失，就是把这些都加起来：

$ L = \sum_{t=1}^T {L}^{<t>} $

### 4\. 反向传播

定义了损失函数，接下来由反向传播计算出损失对参数的梯度，为下一步优化算法做梯度更新准备。

RNN 的反向传播算法是 Backpropagation Through Time，BPTT
算法，基本原理和一般的反向传播算法是一样的，区别在于反向传播算法是按照层进行反向传播，BPTT 是按照时间 t 进行反向传播，所以是 Through
Time。

这一步将对损失函数求出 $W_{x}$、$W_{a}$、$W_{y}$，以及 $b_{a}$、$b_{y}$ 的偏导。

我们以 t3 时刻为例，计算此时的 $W_{x}$、$W_{a}$、$W_{y}$ 的偏导，其他时刻的计算原理是一样的。

1\. 首先按前向计算，将三个时刻的 $a^{<t>}$ 和 $\hat{y}^{<t>}$ 的计算式写出来：

![enter image description
here](https://images.gitbook.cn/562be540-31f4-11ea-924d-0fd6db928ace)

![Oi6xqG](https://images.gitbook.cn/Oi6xqG.jpg)

单个神经元的 a 的计算如上图黄色箭头所示，y 的计算如上图红色箭头所示。

2\. t = 3 时的损失函数为：

$ {L}^{<3>} (\hat{y}^{<3>}, {y}^{<3>} ) = -{y}^{<3>} \log \hat{y}^{<3>} -
(1-{y}^{<3>}) \log(1-\hat{y}^{<3>}) $

整体损失为每一时刻的损失值累加：

$$L = \sum_{t=1}^T {L}^{<t>}$$

3\. 然后对 $W_{x}$、$W_{a}$、$W_{y}$ 求偏导，根据链式法可得：

${ \partial L^{<3>} \over \partial W_{y} } = { \partial L^{<3>} \over \partial
y^{<3>}} { \partial y^{<3>} \over \partial W_{y}}$

${ \partial L^{<3>} \over \partial W_{a} } = { \partial L^{<3>} \over \partial
y^{<3>}} { \partial y^{<3>} \over \partial a_{3}} { \partial a^{<3>} \over
\partial W_{a}} +$ ${ \partial L^{<3>} \over \partial y^{<3>}} { \partial
y^{<3>} \over \partial a_{3}} { \partial a^{<3>} \over \partial a_{2}} {
\partial a^{<2>} \over \partial W_{a}} +$ ${ \partial L^{<3>} \over \partial
y^{<3>}} { \partial y^{<3>} \over \partial a_{3}} { \partial a^{<3>} \over
\partial a_{2}} { \partial a^{<2>} \over \partial a_{1}} { \partial a^{<1>}
\over \partial W_{a}} $

${ \partial L^{<3>} \over \partial W_{x} } = { \partial L^{<3>} \over \partial
y^{<3>}} { \partial y^{<3>} \over \partial a_{3}} { \partial a^{<3>} \over
\partial W_{x}} +$ ${ \partial L^{<3>} \over \partial y^{<3>}} { \partial
y^{<3>} \over \partial a_{3}} { \partial a^{<3>} \over \partial a_{2}} {
\partial a^{<2>} \over \partial W_{x}} +$ ${ \partial L^{<3>} \over \partial
y^{<3>}} { \partial y^{<3>} \over \partial a_{3}} { \partial a^{<3>} \over
\partial a_{2}} { \partial a^{<2>} \over \partial a_{1}} { \partial a^{<1>}
\over \partial W_{x}} $

![t2hNqJ](https://images.gitbook.cn/t2hNqJ.jpg)

由上面的图可以更直观地看出三个参数的求导过程， $W_{y}$ 的求导过程为上图绿色箭头路径，$W_{x}$ 的为上图紫色路径，$W_{a}$
的为上图蓝色路径。

计算出各参数的梯度后，就可以用随机梯度下降法等优化算法来更新参数，不断迭代调整它们以使 L 尽可能达到最小。

#### 4.1 RNN 存在梯度消失/爆炸问题的原因是什么？

基本 RNN 虽然在处理序列数据上比前馈神经网络有优势，但是它自身也存在一些问题，容易出现梯度消失，梯度爆炸问题。

梯度消失是指在做反向传播，计算损失函数对权重的梯度时，随着越向后传播，梯度变得越来越小，这就意味着在网络的前面一些层的神经元，会比后面的训练要慢很多，甚至不会变化。致使结果不准确，训练时间非常长。

由前面的推导可以看出损失函数对于 $W_{y}$ 求偏导并没有长期依赖，对于 $W_{x}$、$W_{a}$ 求偏导，会随着时间产生长期依赖。

那我们再来看 $W_{x}$、$W_{a}$ 在任意时刻的梯度公式：

$ { \partial L^{<t>} \over \partial W_{x} } = { \partial L^{<t>} \over
\partial y^{<t>}}{ \partial y^{<t>} \over \partial a_{t}} ( { \partial a^{<t>}
\over \partial W_{x}} + \sum_{k=1}^t$ $({\prod_{j={k+1}}^t { \partial a^{<j>}
\over \partial a_{j-1}}})$ ${ \partial a^{<k>} \over \partial w_{x}} )$

$ { \partial L^{<t>} \over \partial W_{a} } = { \partial L^{<t>} \over
\partial y^{<t>}}{ \partial y^{<t>} \over \partial a_{t}} ( { \partial a^{<t>}
\over \partial W_{a}} + \sum_{k=1}^t$ $( { \prod_{j={k+1}}^t { \partial
a^{<j>} \over \partial a_{j-1}}})$ ${ \partial a^{<k>} \over \partial w_{a}}
)$

对这个梯度的结果影响很大的就是中间这些项：

$$\prod_{j={k+1}}^t{ \partial a^{<j>} \over \partial a_{j-1}}$$

例如 a 的激活函数为 Tanh：

$$a^{<j>} = tanh ( W_{x} x^{<j>} + W_{a} a^{<j-1>} + b_{a} )$$

中间项即为：

$$\prod_{j={k+1}}^t{ \partial a^{<j>} \over \partial a_{j-1}}
=\prod_{j={k+1}}^ttanh^\prime \;W_{a} $$

因为 Tanh 的导数是 <＝1 的，并且在训练过程中，大部分情况 Tanh 的导数是 <1 的，很少出现 ＝1，同时当我们使用均值为 0，方差为 1
的高斯分布初始化参数 w，则有 |w|<1。

那么当 t 很大时，这一项就会指数级趋近于 0，这就是 RNN 中梯度消失的原因。

同理，当|w|>1，这一项呈指数递增，最后造成梯度爆炸。

#### 4.2 梯度消失/爆炸的解决方案有什么？

我们已经知道了 $W_{x}$、$W_{a}$ 在任意时刻的梯度公式：

$ { \partial L^{<t>} \over \partial W_{x} } = { \partial L^{<t>} \over
\partial y^{<t>}}{ \partial y^{<t>} \over \partial a_{t}} ( { \partial a^{<t>}
\over \partial W_{x}} + \sum_{k=1}^t$ $({\prod_{j={k+1}}^t{ \partial a^{<j>}
\over \partial a_{j-1}}})$ ${ \partial a^{<k>} \over \partial w_{x}} )$

$ { \partial L^{<t>} \over \partial W_{a} } = { \partial L^{<t>} \over
\partial y^{<t>}}{ \partial y^{<t>} \over \partial a_{t}} ( { \partial a^{<t>}
\over \partial W_{a}} + \sum_{k=1}^t$ $({ \prod_{j={k+1}}^t{ \partial a^{<j>}
\over \partial a_{j-1}}})$ ${ \partial a^{<k>} \over \partial w_{a}} )$

那么可以从激活函数、网络结构、权重、时间步等角度出发去考虑解决方案：

梯度爆炸的可用解决方案 | 梯度消失的可用解决方案  
---|---  
TBPTT | Long Short-Term Memory  
对 $W_{a}$ 进行 L1/L2 正则化 | Gated Recurrent Unit  
Teacher Forcing | 正交初始化 Orthogonal initialization  
梯度剪切 Clipping Gradients |  
  
梯度爆炸的可用解决方案有下面几个：

  1. TBPTT（Truncated Backpropagation Through Time）：这个方法限制了反向传播时的时间步数，即设定一个最大值 n，只将 error 传播到 t−n 步。可以避免梯度在 n 步后指数级变化，但是有个缺点是牺牲了可以学习长时间信息的能力。
  2. 对 $W_{a}$ 进行 L1/L2 正则化：这个方法可以使 $W_{a}$ 的谱半径不会超过 1，可以避免梯度爆炸。缺点是所有的输入都必须在时间上以指数级快速消亡，不能用于训练生成型模型，也会牺牲对长期依赖性的学习能力。
  3. Teacher Forcing：这个方法尝试在正确的空间区域内初始化模型，它可以用来训练生成模型或者具有无限记忆长度的模型，缺点需要在每个时间步把目标定义好。
  4. 梯度剪切 Clipping Gradients：这个方法会在梯度超出给定阈值时重新缩放梯度，使它们的范数小于或等于设定的阈值，进而来防止梯度爆炸。缺点是引入了额外的超参数——阈值。

梯度消失的可用解决方案有：

  1. Long Short-Term Memory：LSTM 的精细单元内通过门控的机制来控制进出单元的信息流，关门时，这些单元能够创建线性的自循环，使信息可以无限时间步地流动，从而克服梯度消失的问题。
  2. Gated Recurrent Unit：GRU 只有两个门控单元，用来调节单元内部的信息流，与 LSTM 相比减少了限制的同时仍然具有信息流可以不限时间步地流动的能力，并克服梯度消失的问题。
  3. 正交初始化 Orthogonal initialization：使用正交矩阵初始化权重可以使权重矩阵 W 具有绝对值为 1 的首特征值，这样当 t→∞ 时，$λ^t_1 = 1$，首特征值 $λ^t_1$ 不会对长期的整体梯度值产生不利的影响。这种正交初始化就可以避免梯度消失和梯度爆炸问题，不过这种方法不会单独使用，并且通常与其他更高级的结构如 LSTM 等结合使用效果更佳。

### 5\. 应用举例：用基本的 RNN 识别垃圾邮件

接下来我们就用基本的 RNN 来处理一个任务，就是判断邮件是否为垃圾邮件。

垃圾邮件识别问题是一个常见的分类问题，在这个问题中，我们会收到一些电子邮件，它们被标记为垃圾邮件或非垃圾邮件，用这些数据训练模型，然后在一组新的电子邮件，即测试集数据上预测每封邮件是否为垃圾邮件。

所用的数据集来自 UCI 的 SMS 垃圾邮件数据集，可以从下面地址下载：

> <https://www.kaggle.com/uciml/sms-spam-collection-dataset>

这个数据有两列，第一列为标记，值为 ham 或者 spam，第二列为邮件文本数据。一共有 5574 条记录，其中 spam 747，ham 4827。

![23a5on](https://images.gitbook.cn/23a5on.jpg)

首先，判断邮件是否为垃圾邮件这个问题属于分类问题。所以输出 y 可以设为 0 或 1，1 代表 spam 垃圾邮件，0 代表 ham 非垃圾邮件。

其次，因为要处理的输入数据 x 是文本数据，是序列数据，所以可以建立一个多对一的 RNN 模型来解决这个问题，接下来让我们具体看如何建立并训练 RNN。

训练神经网络一般可以分为下面几个步骤：

  1. 加载包
    1. 建立会话 session，
    2. 设定 RNN 模型参数
  2. 导入数据
    2. 数据预处理
    3. 分为训练集和测试集
    4. 将输入数据做词嵌入 Embedding
  3. 建立 RNN 模型
  4. 定义损失函数和准确率函数
  5. 选择优化算法
  6. 训练模型
  7. 可视化损失函数 loss 和准确率 accuracy

#### 1\. 加载所需要的包

    
    
    import os
    import re
    import io
    import requests
    import numpy as np
    import matplotlib.pyplot as plt
    import tensorflow as tf
    from zipfile import ZipFile
    from tensorflow.python.framework import ops
    
    ops.reset_default_graph() 
    

其中 `ops.reset_default_graph` 函数用于将 default graph 重新初始化，保证内存中没有其他的
Graph，相当于清空所有的张量。

#### 1.1 为 graph 建立会话 session

    
    
    sess = tf.Session()
    

#### 1.2 设定 RNN 模型的参数

    
    
    epochs = 30
    batch_size = 250
    max_sequence_length = 25
    rnn_size = 10
    embedding_size = 50
    min_word_frequency = 10
    learning_rate = 0.0005
    dropout_keep_prob = tf.placeholder(tf.float32)
    

上面各参数的含义为： 执行 30 代， `batch_size` 为 250， 我们考虑的每个文本的最大长度为 25 个单词，这样会将较长的文本剪切为 25
个，不足的用零填充。 `rnn_size` 表示 RNN 有 10 个单元， 每个单词都将被嵌入到一个长度为 50 的可训练向量中， 只考虑我们的
vocabulary 中出现 10 次以上的单词， 学习率设置为 0.0005 dropout 先由一个占位符定义，我们可以在训练时将其设置为 0.5，
或在评估时设置为 1.0。

#### 2\. 导入数据

接下来下载并存储数据

首先检查 SMS 文本数据是否已经下载过了，如果下载了，就读进文件， 没有的话就从 UCI machine learning data repository
下载， 存在 temp 目录下的 `text_data.txt` 文件。

设定路径：

    
    
    data_dir = 'temp'
    data_file = 'text_data.txt'
    if not os.path.exists(data_dir):
        os.makedirs(data_dir)
    

直接下载 zip 格式的数据集：

    
    
    if not os.path.isfile(os.path.join(data_dir, data_file)):
        zip_url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip'
        r = requests.get(zip_url)
        z = ZipFile(io.BytesIO(r.content))
        file = z.read('SMSSpamCollection')
    
        # 格式化数据
        text_data = file.decode()
        text_data = text_data.encode('ascii', errors='ignore')
        text_data = text_data.decode().split('\n')
    
        # 将数据存储到 text 文件
        with open(os.path.join(data_dir, data_file), 'w') as file_conn:
            for text in text_data:
                file_conn.write("{}\n".format(text))
    else:
        # 从 text 文件打开数据
        text_data = []
        with open(os.path.join(data_dir, data_file), 'r') as file_conn:
            for row in file_conn:
                text_data.append(row)
        text_data = text_data[:-1]
    

#### 2.1 数据预处理

  1. 首先，将数据中的标签和邮件文本分开，得到 `text_data_target 和 text_data_train`

    
    
    text_data = [x.split('\t') for x in text_data if len(x) >= 1]
    [text_data_target, text_data_train] = [list(x) for x in zip(*text_data)]
    

  2. 为了减少 vocabulary, 先清理文本，移除特殊字符，删掉多余的空格，将文本都换成小写

    
    
    # 创建一个文本清理函数
    def clean_text(text_string):
        text_string = re.sub(r'([^\s\w]|_|[0-9])+', '', text_string)
        text_string = " ".join(text_string.split())
        text_string = text_string.lower()
        return text_string
    
    # 调用 clean_text 清理文本
    text_data_train = [clean_text(x) for x in text_data_train]
    

  3. 接下来将文本转为词的 ID 序列

用 TensorFlow 中一个内置的 VocabularyProcessor 函数来处理文本， `max_document_length`:
是文本的最大长度。如果文本的长度大于这个值，就会被剪切，小于这个值的地方用 0 填充。 `min_frequency`:
是词频的最小值。当单词的出现次数小于这个词频，就不会被收录到词表中。

    
    
    vocab_processor = tf.contrib.learn.preprocessing.VocabularyProcessor(max_sequence_length, min_frequency=min_word_frequency)
    text_processed = np.array(list(vocab_processor.fit_transform(text_data_train)))
    

例如，

    
    
    max_document_length = 4
    x_text =[
        'i love you',
        'me too'
    ]
    

则

    
    
    vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)
    x = np.array(list(vocab_processor.fit_transform(x_text)))
    

的结果为：

    
    
    [[1 2 3 0]
     [4 5 0 0]]
    

#### 2.2 分为训练集和测试集

预处理后，就可以将数据进行 shuffle，然后分为训练集和测试集。

  1. shuffle，可以打乱数据行序，使数据随机化

这样参数就不易陷入局部最优，模型更容易达到收敛， 用 `np.random.permutation()` 来对行索引进行一次全排列。

    
    
    text_processed = np.array(text_processed)
    text_data_target = np.array([1 if x == 'ham' else 0 for x in text_data_target])
    shuffled_ix = np.random.permutation(np.arange(len(text_data_target)))
    x_shuffled = text_processed[shuffled_ix]
    y_shuffled = text_data_target[shuffled_ix]
    

  2. shuffle 数据后，将数据集分为 80% 训练集和 20% 测试集 如果想做交叉验证 cross-validation ，可以将 测试集 进一步分为测试集和验证集来调参。

    
    
    ix_cutoff = int(len(y_shuffled)*0.80)
    x_train, x_test = x_shuffled[:ix_cutoff], x_shuffled[ix_cutoff:]
    y_train, y_test = y_shuffled[:ix_cutoff], y_shuffled[ix_cutoff:]
    vocab_size = len(vocab_processor.vocabulary_)
    print("Vocabulary Size: {:d}".format(vocab_size))
    print("80-20 Train Test split: {:d} -- {:d}".format(len(y_train), len(y_test)))
    

输出为

    
    
    Vocabulary size: 933
    80-20 Train Test split: 4459 -- 1115
    

#### 2.3 将输入数据做词嵌入 Embedding

  1. 首先，建立 x 和 y 的 placeholders，

`x_data` 的大小为 [None, max _sequence_ length]， `y_output` 是一个整数，值为 0 或 1, 分别表示
ham 和 spam，

    
    
    x_data = tf.placeholder(tf.int32, [None, max_sequence_length])
    y_output = tf.placeholder(tf.int32, [None])
    

  2. 接下来，建立 embedding

在前面的预处理中，生成了同样长度的序列，这些序列的元素是整数，对应为单词的索引。

然后用 embedding 将索引转化为特征，这是一种特征学习方法，可以用来自动学习数据集中各个单词的显著特征。

它可以将单词映射到固定长度为 `embedding_size` 大小的向量，和 one hot
相比，就可以用有限长度的向量来表示，这样就减少特征空间的维数来减少维数诅咒的影响。向量的值是实数值，不是必须是整数。而且特征的提取是可训练的。

建立一个嵌入层需要两步：

  1. 首先建立一个嵌入矩阵 `embedding_mat`，它是一个 tensor 变量，大小为 `[vocab_size × embedding_size]`，将它的元素随机初始化为 [-1, 1] 之间。
  2. 然后用 `tf.nn.embedding_lookup` 函数来找嵌入矩阵 `embedding_mat` 中与 `x_data` 的每个元素所对应的行，也就是将每个单词的整数索引，映射到这个可训练的嵌入矩阵 `embedding_mat` 的某一行。

    
    
    embedding_mat = tf.Variable(tf.random_uniform([vocab_size, embedding_size], -1.0, 1.0))
    embedding_output = tf.nn.embedding_lookup(embedding_mat, x_data)
    

### 3\. 建立 RNN 模型

这时就要建立 RNN 了，

  1. 首先定义 RNN cell，
  2. 然后用 dynamic_rnn 建立 RNN 序列， 
  3. 再为 RNN 添加 dropout，

#### 3.1 定义 RNN cell

    
    
    if tf.__version__[0] >= '1':
        cell = tf.contrib.rnn.BasicRNNCell(num_units=rnn_size)
    else:
        cell = tf.nn.rnn_cell.BasicRNNCell(num_units=rnn_size)
    

`BasicRNNCell` 是 RNN 的基础类， 激活函数默认是 tanh， 每调用一次，就相当于在时间上“推进了一步”， 它的参数
`num_units` 就是隐藏层神经元的个数，

`BasicRNNCell` 和 `BasicLSTMCell` 都是抽象类 RNNCell 的两个子类， 每个 RNNCell 都有一个 call
方法，使用为 `(output, next_state) = call(input, state)`。 其中输入数据的形状为 `(batch_size,
input_size)`， 得到的隐层状态形状 `(batch_size, state_size)`，输出形状为 `(batch_size,
output_size)`。

用图来解释一下就是， 输入一个初始状态 `h0` 和输入 `x1`，调用 `call(x1, h0)` 后就可以得到 `(output1, h1)`，
再调用一次 `call(x2, h1)` 就可以得到 `(output2, h2)`。

#### 3.2 用 `tf.nn.dynamic_rnn` 建立 RNN 序列

单个的 RNNCell，调用一次就只是在序列时间上前进了一步。 所以需要使用 `tf.nn.dynamic_rnn` 函数， 它相当于调用了 n 次
RNNCell，即通过 `{h0,x1, x2, …., xn}` 直接得到
`{h1,h2…,hn}`，`{output1,output2…,outputn}`。

    
    
    output, state = tf.nn.dynamic_rnn(cell, embedding_output, dtype=tf.float32)
    

`tf.nn.dynamic_rnn` 以前面定义的 cell 为基本单元建立一个展开的 RNN 序列网络， 将词嵌入和初始状态输入其中，返回了
outputs，还有最后的 state， output 是一个三维的 tensor，是 `time_steps` 步的所有的输出，形状为
`(batch_size, time_steps, cell.output_size)`， state 是最后一步的隐状态，形状为
`(batch_size, cell.state_size)`。

#### 3.3 再为 RNN 添加 dropout

    
    
    output = tf.nn.dropout(output, dropout_keep_prob)
    

`tf.nn.dropout` 用来减轻过拟合。 Dropout
是指在深度学习网络的训练过程中，按照一定的概率将一部分神经网络单元暂时从网络中“丢弃”，相当于从原始的网络中找到一个更瘦的网络.
`dropout_keep_prob` 是保留比例，是神经元被选中的概率，和输入一样，也是一个占位符，取值为 (0,1] 之间。

#### 3.4 为了得到预测，需要重新安排 output 的维度

    
    
    output = tf.transpose(output, [1, 0, 2])
    

`tf.transpose` 用于将张量进行转置，张量的原有维度顺序是 [0, 1, 2], 则 [1, 0, 2] 是告诉 tf 要将 0 和 1
维转置， 0 代表三维数组的高，1 代表二维数组的行，2 代表二维数组的列。 即将输出 output 的维度 `[batch_size,
time_steps, cell.output_size]` 变成 `[time_steps, batch_size, cell.output_size]`

#### 3.5 切掉最后一个时间步的输出作为预测值

    
    
    last = tf.gather(output, int(output.get_shape()[0]) - 1)
    

`tf.gather` 用于将向量中某些索引值提取出来，得到新的向量。

#### 3.6 将 output 传递给一个全连接层，来得到 logits_out

为了完成 RNN 的分类预测，通过一个全连接层 将 `rnn_size` 长度的输出变为二分类输出，

这个全连接层的核心操作就是矩阵向量乘积：`y = Wx＋b`， 在 RNN 中，全连接层可以将 embedding 空间拉到隐层空间，将隐层空间转回
label 空间。

首先定义变量 weight 和 bias：

    
    
    weight = tf.Variable(tf.truncated_normal([rnn_size, 2], stddev=0.1))
    bias = tf.Variable(tf.constant(0.1, shape=[2]))
    

`tf.truncated_normal` 用来从截断的正态分布中随机抽取值，即生成的值服从指定平均值和标准偏差的正态分布，
但是如果生成的值与均值的差值大于两倍的标准差，即在区间`（μ-2σ，μ+2σ）`之外，则丢弃并重新进行选择，这样可以保证生成的值都在均值附近。

其中参数 shape 表示生成的张量的维度是 `[rnn_size, 2]`， mean 均值默认为 0， stddev 标准差设置为 0.1。

用上面的 weight 和 bias 定义 logits output：

    
    
    logits_out = tf.matmul(last, weight) + bias
    

logits 是这个全连接层的输出，作为 softmax 的输入，在接下来定义损失函数时用到。

### 4\. 定义损失函数和准确率函数

#### 4.1 下面定义损失函数，

    
    
    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=logits_out, labels=y_output)
    

这里用 `tf.nn.sparse_softmax_cross_entropy_with_logits`，

    
    
    sparse_softmax_cross_entropy_with_logits(
        _sentinel=None,
        labels=None,
        logits=None,
        name=None
    )
    

其中参数 `logits`：是神经网络最后一层的输出，如果有 batch 的话，形状是 `[batch_size, num_classes]`，
`labels`：是实际的标签，形状是 `[batch_size, 1]`， 每个 label 的取值是 [0, num_classes)
的离散值，是哪一类就标记哪个类对应的 label。

这个函数将 softmax 和 cross_entropy 放在一起计算， 先对网络最后一层的输出做一个 softmax 求取输出属于某一类的概率， 然后
softmax 的输出向量再和样本的实际标签做一个交叉熵
`cross_entropy`，来计算的两个概率分布之间的距离，这是分类问题中使用比较广泛的损失函数之一。

上面损失函数的返回值是一个向量，是一个 batch 中每个样本的 loss，不是一个数， 需要通过 `tf.reduce_mean` 对向量求均值，计算
batch 内的平均 loss。

    
    
    loss = tf.reduce_mean(losses)
    

#### 4.2 定义准确率函数：

    
    
    accuracy = tf.reduce_mean(tf.cast( tf.equal( tf.argmax(logits_out, 1), tf.cast(y_output, tf.int64) ), tf.float32 ))
    

此处 `tf.argmax` 用来返回最大值 1 所在的索引位置，因为标签向量是由 0,1 组成，因此返回了预测类别标签， 再用 `tf.equal`
来检测预测与真实标签是否匹配，返回一个布尔数组, 用 `tf.cast` 将布尔值转换为浮点数 [1,0,1,1…] 最后用
`tf.reduce_mean` 计算出平均值即为准确率，

### 5\. 选择优化算法

建立 training op，优化算法为 RMSPropOptimizer：

    
    
    optimizer = tf.train.RMSPropOptimizer(learning_rate)
    train_step = optimizer.minimize(loss)
    

RMSProp 是 Geoff Hinton 提出的一种自适应学习率方法，为了解决 Adagrad 学习率急剧下降问题的：

![kUWNWA](https://images.gitbook.cn/kUWNWA.jpg)

这种方法很好的解决了深度学习中过早结束的问题，适合处理非平稳目标，对于RNN效果很好。

### 6\. 训练模型

#### 6.1 首先初始化计算图中的所有变量

    
    
    init  =  tf..global_variables_initializerglobal_ ()
    sess.run(init)
    

#### 6.2. 建立一些空 list

用来追踪每一代 epoch 的 training loss, testing loss, training accuracy, testing
accuracy：

    
    
    train_loss = []
    test_loss = []
    train_accuracy = []
    test_accuracy = []
    

#### 6.3. 接下来就可以开始训练模型了

训练的流程为：

  * Shuffle 训练数据：在每一代 epoch 遍历时都 shuffle 一下数据可以避免过度训练
  * 选择训练集, 训练每个 batch
  * 计算训练集和测试集的损失 loss 和准确率 accuracy

    
    
    # 开始训练
    for epoch in range(epochs):
    
        # Shuffle 训练集
        shuffled_ix = np.random.permutation(np.arange(len(x_train)))
        x_train = x_train[shuffled_ix]
        y_train = y_train[shuffled_ix]
        # 用 Mini batch 梯度下降法
        num_batches = int(len(x_train)/batch_size) + 1
    
        for i in range(num_batches):    
            # 选择每个 batch 的训练数据            
            min_ix = i * batch_size
            max_ix = np.min([len(x_train), ((i+1) * batch_size)])
            x_train_batch = x_train[min_ix:max_ix]
            y_train_batch = y_train[min_ix:max_ix]
    
            # 进行训练： 用 Session 来 run 每个 batch 的训练数据，逐步提升网络的预测准确性
            train_dict = {x_data: x_train_batch, y_output: y_train_batch, dropout_keep_prob:0.5}
            sess.run(train_step, feed_dict=train_dict)
    
        # 将训练集每一代的 loss 和 accuracy 加到整体的损失和准确率中去
        temp_train_loss, temp_train_acc = sess.run([loss, accuracy], feed_dict=train_dict)
        train_loss.append(temp_train_loss)
        train_accuracy.append(temp_train_acc)
    
        # 同时计算并记录测试集每一代的损失和准确率
        test_dict = {x_data: x_test, y_output: y_test, dropout_keep_prob:1.0}
        temp_test_loss, temp_test_acc = sess.run([loss, accuracy], feed_dict=test_dict)
        test_loss.append(temp_test_loss)
        test_accuracy.append(temp_test_acc)
        print('Epoch: {}, Test Loss: {:.2}, Test Acc: {:.2}'.format(epoch+1, temp_test_loss, temp_test_acc))
    

上面代码输出为：

![KsZp6Q](https://images.gitbook.cn/KsZp6Q.jpg)

### 7\. 可视化损失函数 loss 和准确率 accuracy

现在让我们画出训练集和测试集的每一代的 loss 和 accuracy， 观察损失在每次迭代中是如何随着时间推移变化的：

    
    
    %matplotlib inline
    
    # 损失随着时间的变化
    epoch_seq = np.arange(1, epochs+1)
    plt.plot(epoch_seq, train_loss, 'k--', label='Train Set')
    plt.plot(epoch_seq, test_loss, 'r-', label='Test Set')
    plt.title('Softmax Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Softmax Loss')
    plt.legend(loc='upper left')
    plt.show()
    
    # 准确率随着时间的变化
    plt.plot(epoch_seq, train_accuracy, 'k--', label='Train Set')
    plt.plot(epoch_seq, test_accuracy, 'r-', label='Test Set')
    plt.title('Test Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend(loc='lower right')
    plt.show()
    

![lfbIOl](https://images.gitbook.cn/lfbIOl.jpg)

![9QoztP](https://images.gitbook.cn/9QoztP.jpg)

通过上面这个例子我们用基本的 RNN 做个一个分类问题，我们还可以进一步对 epoch、learning rate、embedding size、rnn
size、batch size 调参来提高准确率。

在梯度消失解决方案那里提到了 LSTM，下一篇就主要讲 LSTM 网络。

