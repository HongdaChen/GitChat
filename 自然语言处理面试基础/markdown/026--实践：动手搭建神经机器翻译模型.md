æˆ‘ä»¬ç°åœ¨å‡ ä¹æ¯å¤©éƒ½ä¼šç”¨åˆ°ç¿»è¯‘è½¯ä»¶ï¼Œæ— è®ºæ˜¯çœ‹è®ºæ–‡çœ‹æºç çœ‹æ–°é—»ï¼Œæ€»æ˜¯ä¼šé‡è§ä¸€äº›ä¸ç†Ÿæ‚‰ä¸è®¤è¯†çš„å•è¯ï¼Œå…³äºæœºå™¨ç¿»è¯‘èƒŒåçš„åŸç†æˆ‘ä»¬åœ¨å‰ä¸€ç¯‡æ–‡ç« ä¸­å·²ç»è®²åˆ°äº†ï¼Œä»Šå¤©å°±æ¥åŠ¨æ‰‹å®è·µä¸€ä¸‹ã€‚

åœ¨è¿™ä¸ªä¾‹å­ä¸­æˆ‘ä»¬ä¼šç”¨ä¸€ä¸ªå¾ˆå°çš„æ•°æ®é›†æ¥ç†è§£ä¸€éæµç¨‹ï¼Œæ¥ä¸‹æ¥ä¸»è¦ä¼šæœ‰ä¸¤å¤§æ­¥éª¤ï¼Œä¸€ä¸ªæ˜¯ NMT çš„è®­ç»ƒï¼Œä¸€ä¸ªæ˜¯ NMT çš„æ¨æ–­é¢„æµ‹ã€‚

**NMT çš„è®­ç»ƒæ­¥éª¤ï¼š**

  1. åºåˆ—çš„è¡¨ç¤ºï¼šä¸»è¦æ˜¯å°†æ–‡æœ¬è½¬åŒ–ä¸ºåºåˆ—æ•°æ®ã€‚
  2. è¯åµŒå…¥ï¼šç”¨ä¸€ä¸ªä½ç»´å‘é‡æµ“ç¼©è¡¨è¾¾æ¯ä¸ªå•è¯ï¼Œä½¿ç”¨åµŒå…¥å±‚æ¥å®ç°ã€‚
  3. è®­ç»ƒ Encoderï¼šè¾“å…¥æ•°æ®è¿›å…¥ç¼–ç å™¨ï¼Œå­¦ä¹ å‡ºéšè—çŠ¶æ€ã€‚
  4. è¿æ¥ Encoder å’Œ Decoderï¼šEncoder çš„æœ€ç»ˆéšè—çŠ¶æ€ä½œä¸º Decoder çš„åˆå§‹çŠ¶æ€ã€‚
  5. è®­ç»ƒ Decoderï¼šDecoder çš„ä½œç”¨æ˜¯ç»™å‡ºç›®æ ‡åºåˆ—çš„ä¸€ä¸ªè¯ï¼Œå¯ä»¥é¢„æµ‹å‡ºä¸‹ä¸€ä¸ªè¯ã€‚

ä¸è¿‡è®­ç»ƒå¥½çš„ NMT ä¸èƒ½ç›´æ¥ç”¨æ¥ç¿»è¯‘æ–°çš„æ•°æ®ï¼Œå› ä¸ºè¿™æ—¶å¹¶ä¸çŸ¥é“ç›®æ ‡å¥å­æ˜¯ä»€ä¹ˆï¼Œæ‰€ä»¥ç¿»è¯‘çš„æµç¨‹å’Œè®­ç»ƒçš„æµç¨‹æœ‰äº›ä¸åŒã€‚

**NMT çš„æ¨æ–­é¢„æµ‹ï¼š**

  1. åºåˆ—çš„è¡¨ç¤ºï¼šä¸»è¦æ˜¯å°†æ–‡æœ¬è½¬åŒ–ä¸ºåºåˆ—æ•°æ®ã€‚
  2. è¯åµŒå…¥ï¼šç”¨ä¸€ä¸ªä½ç»´å‘é‡æµ“ç¼©è¡¨è¾¾æ¯ä¸ªå•è¯ï¼Œä½¿ç”¨åµŒå…¥å±‚æ¥å®ç°ã€‚
  3. Encoderï¼šå°†é¢„å¤„ç†å¥½çš„æ•°æ®è¾“å…¥è¿›ç¼–ç å™¨å­¦ä¹ éšè—çŠ¶æ€ã€‚
  4. è¿æ¥ Encoder å’Œ Decoderï¼šEncoder çš„æœ€ç»ˆéšè—çŠ¶æ€ä½œä¸º Decoder çš„åˆå§‹çŠ¶æ€ã€‚
  5. Decoder é¢„æµ‹ï¼šè§£ç å™¨çš„åˆå§‹è¾“å…¥é™¤äº†ä¸Šé¢çš„çŠ¶æ€å‘é‡ï¼Œè¿˜æœ‰ä¸€ä¸ªå¼€å§‹æ ‡è®° `<start>`ï¼Œç„¶åå¯ä»¥é¢„æµ‹ä¸‹ä¸€ä¸ªå‘é‡ã€‚
  6. é€‰æ‹©é¢„æµ‹å€¼ï¼šé¢„æµ‹å‡ºçš„ç»“æœå¦‚æœé€‰æ‹©æ¦‚ç‡æœ€å¤§çš„é‚£ä¸ªï¼Œå°±æ˜¯ greedy searchï¼Œä¹Ÿå¯ä»¥ä» n ä¸ªæœ€å¤§çš„å€™é€‰å€¼é€‰æ‹©ï¼Œè¿™æ—¶æ˜¯ beam searchã€‚
  7. å½“ç¿»è¯‘ç»“æœé‡åˆ°ç»“å°¾æ ‡è®° `<end>` æˆ–è€…å¥å­é•¿åº¦è¾¾åˆ°è®¾ç½®çš„æ•°å€¼å°±å¯ä»¥å®Œæˆè¿™ä¸€å¥çš„ç¿»è¯‘ã€‚

### æ•°æ®é›†

åœ¨è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨ [ManyThings.org](http://www.manythings.org/anki/)
çš„æ•°æ®ï¼Œè¿™ä¸ªé¡¹ç›®ç»å¸¸è¢«ç”¨æ¥åšæœºå™¨ç¿»è¯‘çš„æºæ•°æ®ï¼Œä¸Šé¢æœ‰è‹±è¯­ä¸å…¶ä»–ä¸€ç™¾ä¸‰åå¤šç§è¯­è¨€é…å¯¹çš„æ•°æ®é›†ï¼Œ æˆ‘ä»¬è¿™ä¸ªä¾‹å­ä¸­ç”¨è‹±è¯­-
æ³•è¯­çš„æ•°æ®é›†ï¼Œå¤§å®¶åœ¨å­¦ä¹ å®Œè¿™ä¸€ç« åå¯ä»¥æ‹“å±•åˆ°æ„Ÿå…´è¶£çš„å¦ä¸€ç§è¯­è¨€ä¸Šã€‚

ä¸ºäº†æ›´å¿«åœ°æ‰§è¡Œä»£ç çœ‹åˆ°ç»“æœï¼Œæˆ‘ä»¬å…ˆåªç”¨å…¶ä¸­çš„ 20 å¥æ¥è¯´æ˜ç¥ç»æœºå™¨ç¿»è¯‘çš„åŸç†ã€‚è¦åšçš„ä»»åŠ¡å°±æ˜¯å°†è‹±è¯­ç¿»è¯‘æˆæ³•è¯­ï¼Œæ‰€ä»¥ raw_data
å°±ç”±â€œè‹±è¯­å¥å­â€”æ³•è¯­å¥å­â€æˆå¯¹ç»„æˆã€‚

    
    
    import tensorflow as tf
    import numpy as np
    import unicodedata
    import re
    
    raw_data = (
        ('What a ridiculous concept!', 'Quel concept ridicule !'),
        ('Your idea is not entirely crazy.', "Votre idÃ©e n'est pas complÃ¨tement folle."),
        ("A man's worth lies in what he is.", "La valeur d'un homme rÃ©side dans ce qu'il est."),
        ('What he did is very wrong.', "Ce qu'il a fait est trÃ¨s mal."),
        ("All three of you need to do that.", "Vous avez besoin de faire cela, tous les trois."),
        ("Are you giving me another chance?", "Me donnez-vous une autre chance ?"),
        ("Both Tom and Mary work as models.", "Tom et Mary travaillent tous les deux comme mannequins."),
        ("Can I have a few minutes, please?", "Puis-je avoir quelques minutes, je vous prie ?"),
        ("Could you close the door, please?", "Pourriez-vous fermer la porte, s'il vous plaÃ®t ?"),
        ("Did you plant pumpkins this year?", "Cette annÃ©e, avez-vous plantÃ© des citrouillesâ€¯?"),
        ("Do you ever study in the library?", "Est-ce que vous Ã©tudiez Ã  la bibliothÃ¨que des fois ?"),
        ("Don't be deceived by appearances.", "Ne vous laissez pas abuser par les apparences."),
        ("Excuse me. Can you speak English?", "Je vous prie de m'excuser ! Savez-vous parler anglais ?"),
        ("Few people know the true meaning.", "Peu de gens savent ce que cela veut rÃ©ellement dire."),
        ("Germany produced many scientists.", "L'Allemagne a produit beaucoup de scientifiques."),
        ("Guess whose birthday it is today.", "Devine de qui c'est l'anniversaire, aujourd'hui !"),
        ("He acted like he owned the place.", "Il s'est comportÃ© comme s'il possÃ©dait l'endroit."),
        ("Honesty will pay in the long run.", "L'honnÃªtetÃ© paye Ã  la longue."),
        ("How do we know this isn't a trap?", "Comment savez-vous qu'il ne s'agit pas d'un piÃ¨ge ?"),
        ("I can't believe you're giving up.", "Je n'arrive pas Ã  croire que vous abandonniez."),
    )
    

### æ•°æ®é¢„å¤„ç†

æœ‰äº†æ•°æ®åï¼Œå…ˆå¯¹æ•°æ®è¿›è¡Œä¸€äº›é¢„å¤„ç†ï¼š

  1. å°† unicode è½¬æ¢ä¸º asciiï¼›
  2. å°†å­—ç¬¦ä¸²è§„èŒƒåŒ–ï¼Œé™¤äº† `(a-z, A-Z, ".", "?", "!", ",")` ä¹‹å¤–ï¼Œå°†ä¸éœ€è¦çš„æ ‡è®°æ›¿æ¢æˆç©ºæ ¼ï¼›
  3. åœ¨ç´§è·Ÿå•è¯åé¢çš„æ ‡ç‚¹å‰é¢åŠ ä¸Šç©ºæ ¼ï¼Œä¾‹å¦‚ï¼š`"he is a boy." => "he is a boy ."`ã€‚

    
    
    def unicode_to_ascii(s):
        return ''.join(
            c for c in unicodedata.normalize('NFD', s)
            if unicodedata.category(c) != 'Mn')
    
    def normalize_string(s):
        s = unicode_to_ascii(s)
        s = re.sub(r'([!.?])', r' \1', s)
        s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)    # é™¤äº† (a-z, A-Z, ".", "?", "!", ",") ä¹‹å¤–ï¼Œå°†ä¸éœ€è¦çš„æ ‡è®°æ›¿æ¢æˆç©ºæ ¼
        s = re.sub(r'\s+', r' ', s)                # åœ¨ç´§è·Ÿå•è¯åé¢çš„æ ‡ç‚¹å‰é¢åŠ ä¸Šç©ºæ ¼ï¼Œeg: "he is a boy." => "he is a boy ."
        return s
    

æˆ‘ä»¬åœ¨å‰é¢æåˆ°å¦‚ä½•ç”¨ seq2seq ç»“æ„å¤„ç†æœºå™¨ç¿»è¯‘é—®é¢˜ï¼Œè¿™é‡Œå›é¡¾ä¸€ä¸‹ï¼Œ

![](https://images.gitbook.cn/d7bd67d0-7f35-11ea-b497-6b28b57af19c)

é¦–å…ˆè¦å°†åŸå¥æ•°æ®è¾“å…¥ç»™ Encoderï¼Œå¾—åˆ°ä¸€ä¸ªçŠ¶æ€å‘é‡ï¼Œä½œä¸ºåˆå§‹å‘é‡ä¼ é€’ç»™ Decoderï¼ŒDecoder ä¸€æ­¥ä¸€æ­¥ç”Ÿæˆç¿»è¯‘ç»“æœã€‚

è®­ç»ƒå¥½çš„ Decoder ç”¨æ¥é¢„æµ‹æ•°æ®æ—¶ï¼Œå…¶ä¸­æœ‰ä¸€ä¸ªåˆå§‹è¾“å…¥æ˜¯ `<start>` æ ‡è®°ï¼Œå¦ä¸€ä¸ªæ˜¯ Encoder ç”Ÿæˆçš„çŠ¶æ€å‘é‡ï¼Œå½“ Decoder é‡åˆ°
`<end>` æ ‡è®°æ—¶å°±ä¼šåœæ­¢ç¿»è¯‘ã€‚

æˆ‘ä»¬åœ¨è®­ç»ƒ seq2seq æ¨¡å‹æ—¶ï¼Œå°±è¦ç»™ç¿»è¯‘å¥çš„å‰ååŠ ä¸Šè¿™ä¸¤ä¸ªæ ‡è®°ï¼š

  * å°†æ•°æ®ä¸­çš„è‹±è¯­å’Œæ³•è¯­å¥å­åˆ†åˆ«æ‹¿å‡ºæ¥ï¼Œå½¢æˆåŸå¥å’Œç¿»è¯‘å¥ä¸¤ä¸ªéƒ¨åˆ†ï¼›
  * å¹¶åœ¨ç¿»è¯‘å¥çš„å¼€å§‹å’Œç»“å°¾æ ‡è®° `<start>` ä¸ `<end>`ã€‚

    
    
    raw_data_en, raw_data_fr = list(zip(*raw_data))
    raw_data_en, raw_data_fr = list(raw_data_en), list(raw_data_fr)
    
    raw_data_en = [normalize_string(data) for data in raw_data_en]
    raw_data_fr_in = ['<start> ' + normalize_string(data) for data in raw_data_fr]
    raw_data_fr_out = [normalize_string(data) + ' <end>' for data in raw_data_fr]
    

æ¥ä¸‹é‡Œæˆ‘ä»¬éœ€è¦ç”¨ Tokenizer å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºæ•´æ•°åºåˆ—ï¼š

  * Tokenizer é‡Œé¢çš„ filters æŒ‡çš„æ˜¯è¦å»æ‰æ‰€æœ‰æ ‡ç‚¹ç¬¦å·ï¼Œå› ä¸ºå‰é¢å·²ç»è¿›è¡Œäº†é¢„å¤„ç†ï¼Œè¿™é‡Œå°±è®¾ç½®ä¸ºç©ºã€‚
  * fit_on_textsï¼šæ˜¯æ ¹æ®ä¼ å…¥æ–‡æœ¬ä¸­æ¯ä¸ªå•è¯å‡ºç°çš„é¢‘æ•°ï¼Œç»™å‡ºå¯¹åº”å•è¯çš„ç´¢å¼•å·ï¼Œä¾‹å¦‚ `"The cat sat on the mat."`ï¼Œthe å‡ºç°äº†ä¸¤æ¬¡æ’åœ¨ç¬¬ä¸€ä½ï¼Œæ‰€ä»¥ `word_index["the"] = 1`ï¼Œç´¢å¼•è¶Šå°çš„è¯´æ˜è¿™ä¸ªå•è¯å‡ºç°çš„é¢‘æ•°è¶Šå¤§ã€‚

    
    
    en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')
    en_tokenizer.fit_on_texts(raw_data_en)
    
    print(en_tokenizer.word_index)
    
    
    
    {
     '.': 1, 'you': 2, '?': 3, 'the': 4, 'a': 5, 'is': 6, 'he': 7,
     'what': 8, 'in': 9, 'do': 10, 'can': 11, 't': 12, 'did': 13, 'giving': 14
     ...
    }
    

  * å†ç”¨ texts_to_sequences å°†æ–‡æœ¬æ•°æ®è½¬åŒ–ä¸ºæ•´æ•°åºåˆ—æ•°æ®ï¼Œä¹Ÿå°±æ˜¯å°†å•è¯æ›¿æ¢æˆ word_index ä¸­ç›¸åº”çš„ç´¢å¼•å·ã€‚
  * æ­¤å¤–è¿˜éœ€è¦å°†å¥å­é•¿åº¦è¡¥é½ï¼Œä½¿å®ƒä»¬å…·æœ‰ç›¸åŒçš„é•¿åº¦ï¼Œåœ¨åé¢åˆ›å»º tf.data.Dataset æ—¶å€™ä¼šç”¨åˆ°ï¼Œå› ä¸ºåˆ°æ—¶éœ€è¦å¯¹è¾“å…¥åºåˆ—ä½¿ç”¨è¯åµŒå…¥ï¼Œå¯¹è¾“å‡ºåºåˆ—è¿›è¡Œçƒ­ç¼–ç ç­‰æ“ä½œã€‚

    
    
    data_en = en_tokenizer.texts_to_sequences(raw_data_en)
    data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en, padding='post')
    
    print(data_en[:3])
    
    
    
    [[ 8  5 21 22 23  0  0  0  0  0]
     [24 25  6 26 27 28  1  0  0  0]
     [ 5 29 30 31 32  9  8  7  6  1]]
    

æˆ‘ä»¬å†ç”¨åŒæ ·çš„é¢„å¤„ç†æ­¥éª¤å°†ç›®æ ‡è¯­è¨€çš„å¥å­è¿›è¡Œå¤„ç†ï¼š

    
    
    fr_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')
    
    fr_tokenizer.fit_on_texts(raw_data_fr_in)
    fr_tokenizer.fit_on_texts(raw_data_fr_out)
    
    data_fr_in = fr_tokenizer.texts_to_sequences(raw_data_fr_in)
    data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in,
                                                               padding='post')
    
    data_fr_out = fr_tokenizer.texts_to_sequences(raw_data_fr_out)
    data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(data_fr_out,
                                                               padding='post')
    

**å»ºç«‹æ•°æ®é›† Datasetï¼š** ç”¨ from_tensor_slices å¯ä»¥ä¿æŒä¸‰ä¸ªé›†åˆçš„ç‹¬ç«‹æ€§ï¼Œshuffle éšæœºæ‰“ä¹±æ•°æ®é›†ä¸­çš„æ•°æ®ï¼ŒBatch
å°†æ•°æ®é›†åˆ’åˆ†æˆå‡ ä»½ã€‚

    
    
    dataset = tf.data.Dataset.from_tensor_slices(
        (data_en, data_fr_in, data_fr_out))
    dataset = dataset.shuffle(20).batch(5)
    

### å»ºç«‹æ¨¡å‹

æˆ‘ä»¬å…ˆå»ºç«‹ä¸€ä¸ªåŸºç¡€çš„ Seq2Seq æ¨¡å‹ï¼Œæ¨¡å‹çš„ç»“æ„å°±æ˜¯ Encoder-Decoder LSTMã€‚

**é¦–å…ˆå»ºç«‹ Encoder éƒ¨åˆ†ï¼š**

  * Encoder æ¥æ”¶åŸå¥çš„è¾“å…¥åºåˆ—å’Œåˆå§‹çŠ¶æ€ï¼Œè¿”å›è¾“å‡ºåºåˆ—å’ŒçŠ¶æ€å‘é‡ã€‚
  * æ•°æ®åœ¨è¿›å…¥ç¼–ç å™¨ä¹‹å‰å…ˆè¿›è¡Œ Embeddingï¼Œåœ¨å‰é¢æˆ‘ä»¬ä¹Ÿè®²è¿‡è¯åµŒå…¥çš„çŸ¥è¯†ï¼Œè¿™é‡Œç®€å•å›é¡¾ä¸€ä¸‹ï¼ŒEmbedding ç›¸å½“äºå»ºç«‹ä¸€ä¸ªç»´åº¦æ˜¯ `(vocab_size, embedding_size)` çš„æŸ¥è¯¢çŸ©é˜µã€‚
  * ç»è¿‡è¯åµŒå…¥åæ¯ä¸ªå•è¯å¯ä»¥ç”¨ä¸€ä¸ªé•¿åº¦ä¸º `embedding_size` çš„å‘é‡ä»£è¡¨è‡ªå·±ï¼Œä¾‹å¦‚ `â€œsee you againâ€` è¿™å¥è¯çš„æ•°å­—åºåˆ—æ˜¯ `[100, 21, 24]`ï¼Œé‚£ä¹ˆ see çš„åµŒå…¥å‘é‡å°±æ˜¯åµŒå…¥çŸ©é˜µçš„ç¬¬ 100 è¡Œï¼Œyou çš„åµŒå…¥å‘é‡å°±æ˜¯åµŒå…¥çŸ©é˜µçš„ç¬¬ 21 è¡Œï¼Œagain çš„åµŒå…¥å‘é‡å°±æ˜¯åµŒå…¥çŸ©é˜µçš„ç¬¬ 24 è¡Œã€‚

![](https://images.gitbook.cn/6ea52660-7f36-11ea-a918-478f642cdd64)

æˆ‘ä»¬ä½¿ç”¨çš„åŸºæœ¬å•å…ƒæ˜¯ LSTM å•å…ƒï¼Œå®ƒçš„ return_state è®¾ç½®ä¸º Trueï¼Œå› ä¸ºåœ¨ Decoder ä¸­è¦ä½¿ç”¨è¿™ä¸ªçŠ¶æ€å‘é‡ã€‚

    
    
    class Encoder(tf.keras.Model):
        def __init__(self, vocab_size, embedding_size, lstm_size):
            super(Encoder, self).__init__()
            self.lstm_size = lstm_size
            self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)
            # Encoder ä¸­ LSTM å•å…ƒçš„ return_state è®¾ç½®ä¸º Trueï¼Œå› ä¸ºåœ¨ Decoder ä¸­è¦ä½¿ç”¨è¿™ä¸ªçŠ¶æ€å‘é‡ã€‚
            self.lstm = tf.keras.layers.LSTM(
                lstm_size, return_sequences=True, return_state=True)
    
        def call(self, sequence, states):
            embed = self.embedding(sequence)
            output, state_h, state_c = self.lstm(embed, initial_state=states)
    
            return output, state_h, state_c
    
        def init_states(self, batch_size):
            return (tf.zeros([batch_size, self.lstm_size]),
                    tf.zeros([batch_size, self.lstm_size]))
    

è¿™é‡Œçš„ LSTM å•å…ƒè¿˜å¯ä»¥æ¢æˆ GRU ç­‰å•å…ƒï¼š

    
    
        self.gru = tf.keras.layers.GRU(self.enc_units,     # è¿™é‡Œç”¨çš„ gru å•å…ƒ
                                       return_sequences=True,
                                       return_state=True,
                                       recurrent_initializer='glorot_uniform') 
    

**æ¥ä¸‹æ¥å»ºç«‹ Decoder éƒ¨åˆ†ï¼š**

Decoder å’Œ Encoder çš„ç»“æ„å·®ä¸å¤šï¼Œåªæ˜¯å¤šäº†ä¸€ä¸ª Dense å±‚ï¼Œç”¨æ¥å°† Decoder çš„è¾“å‡ºæ˜ å°„åˆ°ç›®æ ‡è¯­è¨€çš„è¯æ±‡è¡¨ç©ºé—´ï¼ŒDense
å±‚çš„æ¿€æ´»å‡½æ•°å°±æ˜¯ Softmax å‡½æ•°ï¼Œåœ¨ Decoder ä¸­ä¼šè¿”å› `lstm_out, state_h,
state_c`ï¼Œè¿™é‡Œçš„ä¸¤ä¸ªçŠ¶æ€åœ¨è®­ç»ƒæ—¶ä¸ä¼šç”¨åˆ°ï¼Œä½†æ˜¯åœ¨é¢„æµ‹ç¿»è¯‘æ—¶ä¼šç”¨åˆ°ï¼Œæ‰€ä»¥éœ€è¦ä¿ç•™ã€‚

    
    
    class Decoder(tf.keras.Model):
        def __init__(self, vocab_size, embedding_size, lstm_size):
            super(Decoder, self).__init__()
            self.lstm_size = lstm_size
            self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)
            self.lstm = tf.keras.layers.LSTM(
                lstm_size, return_sequences=True, return_state=True)
            self.dense = tf.keras.layers.Dense(vocab_size)
    
        def call(self, sequence, state):
            embed = self.embedding(sequence)
            # åœ¨ Decoder ä¸­ä¼šè¿”å› lstm_out, state_h, state_cï¼Œè¿™é‡Œçš„ä¸¤ä¸ªçŠ¶æ€åœ¨è®­ç»ƒæ—¶ä¸ä¼šç”¨åˆ°ï¼Œä½†æ˜¯åœ¨é¢„æµ‹ç¿»è¯‘æ—¶ä¼šç”¨åˆ°ï¼Œæ‰€ä»¥éœ€è¦ä¿ç•™ã€‚
            lstm_out, state_h, state_c = self.lstm(embed, state)
            logits = self.dense(lstm_out)
    
            return logits, state_h, state_c
    

### å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨

æŸå¤±å‡½æ•°ä½¿ç”¨ SparseCategoricalCrossentropyã€‚

    
    
    def loss_func(targets, logits):
        crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(
            from_logits=True)
        mask = tf.math.logical_not(tf.math.equal(targets, 0))
        mask = tf.cast(mask, dtype=tf.int64)
        loss = crossentropy(targets, logits, sample_weight=mask)
    
        return loss
    
    optimizer = tf.keras.optimizers.Adam()
    

**1\. è¿™é‡Œæˆ‘ä»¬å…ˆç®€å•ä»‹ç»ä¸€ä¸‹ä¸¤ç§æŸå¤±å‡½æ•° categorial_crossentropy å’Œ
sparse_categorial_crossentropy çš„åŒºåˆ«ã€‚**

categorial_crossentropy çš„è®¡ç®—å…¬å¼æ˜¯è¿™æ ·çš„ï¼š

$$J(\textbf{w}) = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \text{log}(\hat{y}_i)
+ (1-y_i) \text{log}(1-\hat{y}_i) \right]$$

$ğ°$ æ˜¯ç¥ç»ç½‘ç»œçš„æƒé‡å‚æ•°ï¼Œ$y_i$ æ˜¯çœŸå®çš„æ ‡ç­¾ï¼Œ$\hat{y}_i$ æ˜¯é¢„æµ‹çš„æ ‡ç­¾ã€‚

ä¸¤ç§æŸå¤±å‡½æ•°çš„è®¡ç®—å…¬å¼æ˜¯ä¸€æ ·çš„ï¼ŒåŒºåˆ«åœ¨äºçœŸå®çš„æ ‡ç­¾ $y_i$ çš„å½¢å¼ï¼š

  * å½“å®ƒæ˜¯ one-hot å½¢å¼æ—¶ï¼Œä¾‹å¦‚ [1,0,0]ã€[0,1,0]ã€[0,0,1]ï¼Œä½¿ç”¨ categorial_crossentropyï¼›
  * å½“å®ƒæ˜¯æ•´æ•°æ—¶ï¼Œä¾‹å¦‚ [1]ã€[2]ã€[3]ï¼Œä½¿ç”¨ sparse_categorial_crossentropyï¼Œåœ¨è®¡ç®—æ—¶ sparse æ¶ˆè€—çš„èµ„æºæ›´å°‘ä¸€äº›ï¼Œå› ä¸ºå®ƒä¸ç”¨è®¡ç®—æ•´ä¸ªå‘é‡ï¼Œåªéœ€è®¡ç®—æ•´æ•°ã€‚

æ­¤å¤–ï¼Œå› ä¸ºæˆ‘ä»¬å‰é¢åœ¨æ•°æ®é¢„å¤„ç†æ—¶ä¸ºäº†æ›´æ–¹ä¾¿æ„é€ æ‰¹æ¬¡æ•°æ®ï¼Œæœ‰å¯¹åºåˆ—ç”¨ 0 å¡«å……æˆç»Ÿä¸€é•¿åº¦çš„æ“ä½œï¼Œè¿™ç§å¡«å……åœ¨è®¡ç®—æŸå¤±å’Œåå‘ä¼ æ’­æ—¶ä¼šå¯¹æ¨¡å‹çš„æ€§èƒ½é€ æˆä¸€å®šçš„å½±å“ã€‚

**2\. ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜å°±éœ€è¦åœ¨è®¡ç®—æŸå¤±æ—¶ç”¨ä¸€ä¸ª padding maskã€‚**

å®ƒä¼šåœ¨åå‘ä¼ æ’­ä¹‹å‰å°†ä»»ä½•å¡«å……è¿‡çš„åºåˆ—æ‰€å¸¦æ¥çš„æŸå¤±å˜ä¸º 0ï¼Œç„¶åå†å»æ›´æ–°æƒé‡å‚æ•°ï¼Œå³å¡«å……çš„æ•°æ® mask ä¸º 0ï¼Œæ²¡å¡«å……çš„æ•°æ® mask ä¸º 1ã€‚

å…·ä½“ä¼šå¸¦æ¥ä»€ä¹ˆå½±å“æˆ‘ä»¬å¯ä»¥ç”¨ä¸€ä¸ªæœ€ç®€å•çš„å°ä¾‹å­æ¥çœ‹ä¸€ä¸‹ï¼Œä¾‹å¦‚ï¼Œæœ‰è¿™æ ·ä¸€ä¸ªç®€å•çš„æ¨¡å‹ï¼š

    
    
    max_sentence_length = 5
    character_number = 2
    
    input_tensor = Input(shape=(max_sentence_length, character_number))
    masked_input = Masking(mask_value=0)(input_tensor)
    output = LSTM(3, return_sequences=True)(masked_input)
    model = Model(input_tensor, output)
    model.compile(loss='mae', optimizer='adam')
    

X æ•°æ®å¦‚ä¸‹ï¼š

    
    
    X = np.array([[[0, 0], [0, 0], [1, 0], [0, 1], [0, 1]],
                  [[0, 0], [0, 1], [1, 0], [0, 1], [0, 1]]])
    

é‚£ä¹ˆçœŸå®çš„æ ‡ç­¾æ˜¯ï¼š

    
    
    y_true = np.ones((2, max_sentence_length, 3))
    

é¢„æµ‹çš„æ ‡ç­¾ä¸ºï¼š

    
    
    y_pred = model.predict(X)
    
    print(y_pred)
    [[[ 0.          0.          0.        ]
      [ 0.          0.          0.        ]
      [-0.11980877  0.05803877  0.07880752]
      [-0.00429189  0.13382857  0.19167568]
      [ 0.06817091  0.19093043  0.26219055]]
    
     [[ 0.          0.          0.        ]
      [ 0.0651961   0.10283815  0.12413475]
      [-0.04420842  0.137494    0.13727818]
      [ 0.04479844  0.17440712  0.24715884]
      [ 0.11117355  0.21645413  0.30220413]]]
    

å¦‚æœä¸å¯¹æŸå¤±å‡½æ•°è¿›è¡Œ maskï¼Œè®¡ç®—å…¬å¼æ˜¯è¿™æ ·çš„ï¼š

    
    
    unmasked_loss = np.abs(1 - y_pred).mean()
    

å¦åˆ™æ˜¯è¿™æ ·çš„ï¼š

    
    
    masked_loss = np.abs(1 - y_pred[y_pred != 0]).mean()
    

æ‰“å°å‡ºä¸¤ç§æŸå¤±å¯ä»¥çœ‹åˆ°æ²¡æœ‰è¿›è¡Œ loss masking çš„æŸå¤±è¦æ¯”çœŸå®çš„æŸå¤±å¤§ä¸€äº›ã€‚

    
    
    print(model.evaluate(X, y_true))
    0.881977558136
    
    print(masked_loss)
    0.881978
    
    print(unmasked_loss)
    0.917384
    

### NMT è®­ç»ƒ

æˆ‘ä»¬ç”¨ tf.function è£…é¥°å™¨æ¥è¿›è¡Œé™æ€å›¾å½¢è®¡ç®—ï¼Œç½‘ç»œçš„è®¡ç®—éœ€è¦æ”¾åœ¨ tf.GradientTape() ä¸‹ï¼Œç”¨æ¥è¿½è¸ªæ¢¯åº¦ã€‚

å°† input è¾“å…¥ç»™ Encoderï¼Œç„¶åè¿”å› en_outputs å’Œ en_statesï¼Œè¿™ä¸ªéšå±‚çŠ¶æ€åŒæ—¶ä¹Ÿæ˜¯ Decoder çš„éšå±‚çŠ¶æ€ã€‚
Decoder çš„è¾“å…¥æœ‰åˆå§‹çš„éšå±‚çŠ¶æ€å’Œ target_seq_inï¼ŒDecoder çš„è¾“å‡ºç»“æœå³ä¸ºé¢„æµ‹å€¼ã€‚

è®¡ç®—æŸå¤±ï¼Œç„¶åè®¡ç®—æ¢¯åº¦å¹¶å°†å…¶åº”ç”¨äºä¼˜åŒ–å™¨å’Œåå‘ä¼ æ’­ä¸­ã€‚

å…¶ä¸­ Encoder çš„è¾“å‡ºæœ‰ä¸‰ä¸ª `encoder_output, en_state_h, en_state_c`ï¼Œæˆ‘ä»¬ä¸éœ€è¦ç”¨
encoder_outputï¼Œåªç”¨ Encoder çš„çŠ¶æ€å‘é‡ `en_state_h, en_state_c`ï¼Œ æ‰€ä»¥ `en_states =
en_outputs[1:]` è¿™ä¸ª en_states å°±ä½œä¸º de_states ä¼ é€’ç»™ Decoderã€‚

    
    
    @tf.function
    def train_step(source_seq, target_seq_in, target_seq_out, en_initial_states):
        with tf.GradientTape() as tape:
            en_outputs = encoder(source_seq, en_initial_states)
            # æˆ‘ä»¬ä¸éœ€è¦ç”¨åˆ° encoder_outputï¼Œåªç”¨ Encoder çš„çŠ¶æ€å‘é‡ en_state_h, en_state_cï¼Œ
            en_states = en_outputs[1:]
            de_states = en_states
    
            de_outputs = decoder(target_seq_in, de_states)
            logits = de_outputs[0]
            loss = loss_func(target_seq_out, logits)
    
        variables = encoder.trainable_variables + decoder.trainable_variables
        gradients = tape.gradient(loss, variables)
        optimizer.apply_gradients(zip(gradients, variables))
    
        return loss
    

### NMT æ¨æ–­é¢„æµ‹

åœ¨æ–‡ç« å¼€å¤´æˆ‘ä»¬çŸ¥é“äº† NMT é¢„æµ‹çš„æµç¨‹ï¼Œè¿™éƒ¨åˆ†å°±æ˜¯æŒ‰ç…§æµç¨‹å†™ä¸‹æ¥å°±å¯ä»¥ã€‚

é¦–å…ˆå°†æµ‹è¯•æ–‡æœ¬è½¬åŒ–æˆæ•´æ•°åºåˆ—æ•°æ®ï¼Œæ¥ç€è¾“å…¥åˆ° Encoderï¼ŒEncoder ç”Ÿæˆçš„çŠ¶æ€å‘é‡ä½œä¸º Decoder çš„åˆå§‹å‘é‡ï¼ŒåŒæ—¶ Decoder
çš„è¾“å…¥è¿˜æœ‰å¼€å§‹æ ‡è®° `<start>`ï¼ŒDecoder æ¯ä¸€æ­¥çš„è¾“å‡ºç»“æœ
de_outputï¼Œéƒ½é€‰æ‹©æ¦‚ç‡æœ€å¤§çš„ä½œä¸ºé¢„æµ‹å€¼ï¼Œå¹¶æ›¿æ¢æˆå¯¹åº”çš„å•è¯ï¼Œé™„åŠ åˆ°è¾“å‡ºçš„ç¿»è¯‘åºåˆ— out_words ä¸­ï¼Œå¹¶ä¸” Decoder
æ¯ä¸€æ­¥çš„é¢„æµ‹ç»“æœè¦ä½œä¸ºä¸‹ä¸€æ­¥çš„è¾“å…¥ç»§ç»­é¢„æµ‹ï¼Œä¸€ç›´åˆ°é‡åˆ° `<end>` æ ‡è®°æˆ–è€…ç»“æœçš„é•¿åº¦è¶…è¿‡è®¾å®šé•¿åº¦ä¸º 20 å°±åœæ­¢ç”Ÿæˆç»“æœã€‚

    
    
    def predict():
        test_source_text = raw_data_en[np.random.choice(len(raw_data_en))]
        print(test_source_text)
        # é¦–å…ˆå°†æµ‹è¯•æ–‡æœ¬è½¬åŒ–æˆæ•´æ•°åºåˆ—æ•°æ®ï¼Œ
        test_source_seq = en_tokenizer.texts_to_sequences([test_source_text])
        print(test_source_seq)
    
        # æµ‹è¯•æ•°æ®è¾“å…¥ Encoderï¼Œè¿”å›ç»“æœä¸­åªå–çŠ¶æ€å‘é‡
        en_initial_states = encoder.init_states(1)
        en_outputs = encoder(tf.constant(test_source_seq), en_initial_states)
    
        # Decoder çš„è¾“å…¥å°±æ˜¯å¼€å§‹æ ‡è®° <start>ï¼ŒEncoder ç”Ÿæˆçš„çŠ¶æ€å‘é‡ä½œä¸º Decoder çš„åˆå§‹å‘é‡
        de_input = tf.constant([[fr_tokenizer.word_index['<start>']]])
        de_state_h, de_state_c = en_outputs[1:]        # è¿”å›ç»“æœä¸­åªå–çŠ¶æ€å‘é‡
        out_words = []
    
        # Decoder è¾“å‡ºçš„ç»“æœ de_outputï¼Œé€‰æ‹©æ¦‚ç‡æœ€å¤§çš„ä½œä¸ºé¢„æµ‹å€¼ï¼Œå¹¶æŸ¥æ‰¾ç›¸åº”çš„ç´¢å¼•ï¼Œæ›¿æ¢æˆå¯¹åº”çš„å•è¯ï¼Œé™„åŠ åˆ°è¾“å‡ºçš„ç¿»è¯‘åºåˆ— out_words ä¸­
        # å‰ä¸€æ­¥çš„é¢„æµ‹å€¼ä½œä¸ºä¸‹ä¸€æ­¥çš„è¾“å…¥
        # é‡å¤è¿™ä¸ªæ­¥éª¤ä¸€ç›´é¢„æµ‹ï¼Œç›´åˆ°é‡åˆ° <end> æ ‡è®°æˆ–è€…ç»“æœçš„é•¿åº¦è¶…è¿‡ 20ã€‚
        while True:
            de_output, de_state_h, de_state_c = decoder(
                de_input, (de_state_h, de_state_c))
            de_input = tf.argmax(de_output, -1)
            out_words.append(fr_tokenizer.index_word[de_input.numpy()[0][0]])
    
            if out_words[-1] == '<end>' or len(out_words) >= 20:
                break
    
        print(' '.join(out_words))
    

### æ¨¡å‹è®­ç»ƒ

å‰é¢çš„æ¯ä¸€æ­¥éƒ½å®Œæˆåï¼Œå°±å¯ä»¥æ‰§è¡Œè®­ç»ƒäº†ï¼Œåœ¨æ¯ä¸€ä¸ª epoch ä¼šæ‰“å°å‡ºæ¨¡å‹çš„æŸå¤±ã€‚

    
    
    EMBEDDING_SIZE = 32
    LSTM_SIZE = 64
    
    en_vocab_size = len(en_tokenizer.word_index) + 1
    encoder = Encoder(en_vocab_size, EMBEDDING_SIZE, LSTM_SIZE)
    
    fr_vocab_size = len(fr_tokenizer.word_index) + 1
    decoder = Decoder(fr_vocab_size, EMBEDDING_SIZE, LSTM_SIZE)
    
    NUM_EPOCHS = 250
    BATCH_SIZE = 5
    
    for e in range(NUM_EPOCHS):
        en_initial_states = encoder.init_states(BATCH_SIZE)
    
        for batch, (source_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):
            loss = train_step(source_seq, target_seq_in,
                              target_seq_out, en_initial_states)
    
        print('Epoch {} Loss {:.4f}'.format(e + 1, loss.numpy()))
    
        try:
            predict()
        except Exception:
          continue   
    

æˆ‘ä»¬å¯ä»¥çœ‹åˆ°åœ¨æ¨¡å‹è®­ç»ƒçš„æœ€å¼€å§‹ï¼Œæ‰“å°å‡ºæ¥çš„ç»“æœæ˜¯æ²¡æœ‰ä»€ä¹ˆå®é™…æ„ä¹‰çš„ï¼Œéšç€æ¨¡å‹è®­ç»ƒæ¬¡æ•°çš„å¢åŠ ï¼Œç»“æœå°±ä¼šè¶Šæ¥è¶Šå‡†ç¡®ï¼Œå½“ç„¶è¿™ä¸ªå°ä¾‹å­æ˜¯è¿‡æ‹Ÿåˆçš„äº†ã€‚

    
    
    Epoch 1 Loss 3.2892
    I can t believe you re giving up .
    [[16, 11, 12, 95, 2, 96, 14, 97, 1]]
    est est est est est comme comme comme mannequins <end>
    Epoch 2 Loss 3.4178
    Germany produced many scientists .
    [[73, 74, 75, 76, 1]]
    vous <end>
    Epoch 3 Loss 3.3415
    What a ridiculous concept !
    [[8, 5, 21, 22, 23]]
    vous <end>
    
    ...
    
    
    Epoch 248 Loss 0.1051
    Could you close the door please ?
    [[53, 2, 54, 4, 55, 18, 3]]
    pourriez vous fermer la s il vous plait ? <end>
    Epoch 249 Loss 0.1094
    What a ridiculous concept !
    [[8, 5, 21, 22, 23]]
    <end>
    Epoch 250 Loss 0.1079
    All three of you need to do that .
    [[35, 36, 37, 2, 38, 39, 10, 40, 1]]
    vous avez besoin de faire cela tous les trois . <end>
    

æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå°ä¾‹å­å·²ç»åŸºæœ¬ç†è§£ NMT çš„ä»£ç å®ç°ï¼Œå¤§å®¶æœ‰å…´è¶£å¯ä»¥å°è¯•ç¿»è¯‘å…¶ä»–è¯­ç§æ•°æ®ã€‚

* * *

å‚è€ƒæ–‡çŒ®ï¼š

  * TensorFlowï¼Œ[Neural machine translation with attention](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/nmt_with_attention.ipynb#scrollTo=umohpBN2OM94)

