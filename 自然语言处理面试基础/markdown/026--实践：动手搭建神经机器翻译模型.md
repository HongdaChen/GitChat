我们现在几乎每天都会用到翻译软件，无论是看论文看源码看新闻，总是会遇见一些不熟悉不认识的单词，关于机器翻译背后的原理我们在前一篇文章中已经讲到了，今天就来动手实践一下。

在这个例子中我们会用一个很小的数据集来理解一遍流程，接下来主要会有两大步骤，一个是 NMT 的训练，一个是 NMT 的推断预测。

**NMT 的训练步骤：**

  1. 序列的表示：主要是将文本转化为序列数据。
  2. 词嵌入：用一个低维向量浓缩表达每个单词，使用嵌入层来实现。
  3. 训练 Encoder：输入数据进入编码器，学习出隐藏状态。
  4. 连接 Encoder 和 Decoder：Encoder 的最终隐藏状态作为 Decoder 的初始状态。
  5. 训练 Decoder：Decoder 的作用是给出目标序列的一个词，可以预测出下一个词。

不过训练好的 NMT 不能直接用来翻译新的数据，因为这时并不知道目标句子是什么，所以翻译的流程和训练的流程有些不同。

**NMT 的推断预测：**

  1. 序列的表示：主要是将文本转化为序列数据。
  2. 词嵌入：用一个低维向量浓缩表达每个单词，使用嵌入层来实现。
  3. Encoder：将预处理好的数据输入进编码器学习隐藏状态。
  4. 连接 Encoder 和 Decoder：Encoder 的最终隐藏状态作为 Decoder 的初始状态。
  5. Decoder 预测：解码器的初始输入除了上面的状态向量，还有一个开始标记 `<start>`，然后可以预测下一个向量。
  6. 选择预测值：预测出的结果如果选择概率最大的那个，就是 greedy search，也可以从 n 个最大的候选值选择，这时是 beam search。
  7. 当翻译结果遇到结尾标记 `<end>` 或者句子长度达到设置的数值就可以完成这一句的翻译。

### 数据集

在这里我们使用 [ManyThings.org](http://www.manythings.org/anki/)
的数据，这个项目经常被用来做机器翻译的源数据，上面有英语与其他一百三十多种语言配对的数据集， 我们这个例子中用英语-
法语的数据集，大家在学习完这一章后可以拓展到感兴趣的另一种语言上。

为了更快地执行代码看到结果，我们先只用其中的 20 句来说明神经机器翻译的原理。要做的任务就是将英语翻译成法语，所以 raw_data
就由“英语句子—法语句子”成对组成。

    
    
    import tensorflow as tf
    import numpy as np
    import unicodedata
    import re
    
    raw_data = (
        ('What a ridiculous concept!', 'Quel concept ridicule !'),
        ('Your idea is not entirely crazy.', "Votre idée n'est pas complètement folle."),
        ("A man's worth lies in what he is.", "La valeur d'un homme réside dans ce qu'il est."),
        ('What he did is very wrong.', "Ce qu'il a fait est très mal."),
        ("All three of you need to do that.", "Vous avez besoin de faire cela, tous les trois."),
        ("Are you giving me another chance?", "Me donnez-vous une autre chance ?"),
        ("Both Tom and Mary work as models.", "Tom et Mary travaillent tous les deux comme mannequins."),
        ("Can I have a few minutes, please?", "Puis-je avoir quelques minutes, je vous prie ?"),
        ("Could you close the door, please?", "Pourriez-vous fermer la porte, s'il vous plaît ?"),
        ("Did you plant pumpkins this year?", "Cette année, avez-vous planté des citrouilles ?"),
        ("Do you ever study in the library?", "Est-ce que vous étudiez à la bibliothèque des fois ?"),
        ("Don't be deceived by appearances.", "Ne vous laissez pas abuser par les apparences."),
        ("Excuse me. Can you speak English?", "Je vous prie de m'excuser ! Savez-vous parler anglais ?"),
        ("Few people know the true meaning.", "Peu de gens savent ce que cela veut réellement dire."),
        ("Germany produced many scientists.", "L'Allemagne a produit beaucoup de scientifiques."),
        ("Guess whose birthday it is today.", "Devine de qui c'est l'anniversaire, aujourd'hui !"),
        ("He acted like he owned the place.", "Il s'est comporté comme s'il possédait l'endroit."),
        ("Honesty will pay in the long run.", "L'honnêteté paye à la longue."),
        ("How do we know this isn't a trap?", "Comment savez-vous qu'il ne s'agit pas d'un piège ?"),
        ("I can't believe you're giving up.", "Je n'arrive pas à croire que vous abandonniez."),
    )
    

### 数据预处理

有了数据后，先对数据进行一些预处理：

  1. 将 unicode 转换为 ascii；
  2. 将字符串规范化，除了 `(a-z, A-Z, ".", "?", "!", ",")` 之外，将不需要的标记替换成空格；
  3. 在紧跟单词后面的标点前面加上空格，例如：`"he is a boy." => "he is a boy ."`。

    
    
    def unicode_to_ascii(s):
        return ''.join(
            c for c in unicodedata.normalize('NFD', s)
            if unicodedata.category(c) != 'Mn')
    
    def normalize_string(s):
        s = unicode_to_ascii(s)
        s = re.sub(r'([!.?])', r' \1', s)
        s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)    # 除了 (a-z, A-Z, ".", "?", "!", ",") 之外，将不需要的标记替换成空格
        s = re.sub(r'\s+', r' ', s)                # 在紧跟单词后面的标点前面加上空格，eg: "he is a boy." => "he is a boy ."
        return s
    

我们在前面提到如何用 seq2seq 结构处理机器翻译问题，这里回顾一下，

![](https://images.gitbook.cn/d7bd67d0-7f35-11ea-b497-6b28b57af19c)

首先要将原句数据输入给 Encoder，得到一个状态向量，作为初始向量传递给 Decoder，Decoder 一步一步生成翻译结果。

训练好的 Decoder 用来预测数据时，其中有一个初始输入是 `<start>` 标记，另一个是 Encoder 生成的状态向量，当 Decoder 遇到
`<end>` 标记时就会停止翻译。

我们在训练 seq2seq 模型时，就要给翻译句的前后加上这两个标记：

  * 将数据中的英语和法语句子分别拿出来，形成原句和翻译句两个部分；
  * 并在翻译句的开始和结尾标记 `<start>` 与 `<end>`。

    
    
    raw_data_en, raw_data_fr = list(zip(*raw_data))
    raw_data_en, raw_data_fr = list(raw_data_en), list(raw_data_fr)
    
    raw_data_en = [normalize_string(data) for data in raw_data_en]
    raw_data_fr_in = ['<start> ' + normalize_string(data) for data in raw_data_fr]
    raw_data_fr_out = [normalize_string(data) + ' <end>' for data in raw_data_fr]
    

接下里我们需要用 Tokenizer 将字符串转换为整数序列：

  * Tokenizer 里面的 filters 指的是要去掉所有标点符号，因为前面已经进行了预处理，这里就设置为空。
  * fit_on_texts：是根据传入文本中每个单词出现的频数，给出对应单词的索引号，例如 `"The cat sat on the mat."`，the 出现了两次排在第一位，所以 `word_index["the"] = 1`，索引越小的说明这个单词出现的频数越大。

    
    
    en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')
    en_tokenizer.fit_on_texts(raw_data_en)
    
    print(en_tokenizer.word_index)
    
    
    
    {
     '.': 1, 'you': 2, '?': 3, 'the': 4, 'a': 5, 'is': 6, 'he': 7,
     'what': 8, 'in': 9, 'do': 10, 'can': 11, 't': 12, 'did': 13, 'giving': 14
     ...
    }
    

  * 再用 texts_to_sequences 将文本数据转化为整数序列数据，也就是将单词替换成 word_index 中相应的索引号。
  * 此外还需要将句子长度补齐，使它们具有相同的长度，在后面创建 tf.data.Dataset 时候会用到，因为到时需要对输入序列使用词嵌入，对输出序列进行热编码等操作。

    
    
    data_en = en_tokenizer.texts_to_sequences(raw_data_en)
    data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en, padding='post')
    
    print(data_en[:3])
    
    
    
    [[ 8  5 21 22 23  0  0  0  0  0]
     [24 25  6 26 27 28  1  0  0  0]
     [ 5 29 30 31 32  9  8  7  6  1]]
    

我们再用同样的预处理步骤将目标语言的句子进行处理：

    
    
    fr_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')
    
    fr_tokenizer.fit_on_texts(raw_data_fr_in)
    fr_tokenizer.fit_on_texts(raw_data_fr_out)
    
    data_fr_in = fr_tokenizer.texts_to_sequences(raw_data_fr_in)
    data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in,
                                                               padding='post')
    
    data_fr_out = fr_tokenizer.texts_to_sequences(raw_data_fr_out)
    data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(data_fr_out,
                                                               padding='post')
    

**建立数据集 Dataset：** 用 from_tensor_slices 可以保持三个集合的独立性，shuffle 随机打乱数据集中的数据，Batch
将数据集划分成几份。

    
    
    dataset = tf.data.Dataset.from_tensor_slices(
        (data_en, data_fr_in, data_fr_out))
    dataset = dataset.shuffle(20).batch(5)
    

### 建立模型

我们先建立一个基础的 Seq2Seq 模型，模型的结构就是 Encoder-Decoder LSTM。

**首先建立 Encoder 部分：**

  * Encoder 接收原句的输入序列和初始状态，返回输出序列和状态向量。
  * 数据在进入编码器之前先进行 Embedding，在前面我们也讲过词嵌入的知识，这里简单回顾一下，Embedding 相当于建立一个维度是 `(vocab_size, embedding_size)` 的查询矩阵。
  * 经过词嵌入后每个单词可以用一个长度为 `embedding_size` 的向量代表自己，例如 `“see you again”` 这句话的数字序列是 `[100, 21, 24]`，那么 see 的嵌入向量就是嵌入矩阵的第 100 行，you 的嵌入向量就是嵌入矩阵的第 21 行，again 的嵌入向量就是嵌入矩阵的第 24 行。

![](https://images.gitbook.cn/6ea52660-7f36-11ea-a918-478f642cdd64)

我们使用的基本单元是 LSTM 单元，它的 return_state 设置为 True，因为在 Decoder 中要使用这个状态向量。

    
    
    class Encoder(tf.keras.Model):
        def __init__(self, vocab_size, embedding_size, lstm_size):
            super(Encoder, self).__init__()
            self.lstm_size = lstm_size
            self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)
            # Encoder 中 LSTM 单元的 return_state 设置为 True，因为在 Decoder 中要使用这个状态向量。
            self.lstm = tf.keras.layers.LSTM(
                lstm_size, return_sequences=True, return_state=True)
    
        def call(self, sequence, states):
            embed = self.embedding(sequence)
            output, state_h, state_c = self.lstm(embed, initial_state=states)
    
            return output, state_h, state_c
    
        def init_states(self, batch_size):
            return (tf.zeros([batch_size, self.lstm_size]),
                    tf.zeros([batch_size, self.lstm_size]))
    

这里的 LSTM 单元还可以换成 GRU 等单元：

    
    
        self.gru = tf.keras.layers.GRU(self.enc_units,     # 这里用的 gru 单元
                                       return_sequences=True,
                                       return_state=True,
                                       recurrent_initializer='glorot_uniform') 
    

**接下来建立 Decoder 部分：**

Decoder 和 Encoder 的结构差不多，只是多了一个 Dense 层，用来将 Decoder 的输出映射到目标语言的词汇表空间，Dense
层的激活函数就是 Softmax 函数，在 Decoder 中会返回 `lstm_out, state_h,
state_c`，这里的两个状态在训练时不会用到，但是在预测翻译时会用到，所以需要保留。

    
    
    class Decoder(tf.keras.Model):
        def __init__(self, vocab_size, embedding_size, lstm_size):
            super(Decoder, self).__init__()
            self.lstm_size = lstm_size
            self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_size)
            self.lstm = tf.keras.layers.LSTM(
                lstm_size, return_sequences=True, return_state=True)
            self.dense = tf.keras.layers.Dense(vocab_size)
    
        def call(self, sequence, state):
            embed = self.embedding(sequence)
            # 在 Decoder 中会返回 lstm_out, state_h, state_c，这里的两个状态在训练时不会用到，但是在预测翻译时会用到，所以需要保留。
            lstm_out, state_h, state_c = self.lstm(embed, state)
            logits = self.dense(lstm_out)
    
            return logits, state_h, state_c
    

### 定义损失函数和优化器

损失函数使用 SparseCategoricalCrossentropy。

    
    
    def loss_func(targets, logits):
        crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(
            from_logits=True)
        mask = tf.math.logical_not(tf.math.equal(targets, 0))
        mask = tf.cast(mask, dtype=tf.int64)
        loss = crossentropy(targets, logits, sample_weight=mask)
    
        return loss
    
    optimizer = tf.keras.optimizers.Adam()
    

**1\. 这里我们先简单介绍一下两种损失函数 categorial_crossentropy 和
sparse_categorial_crossentropy 的区别。**

categorial_crossentropy 的计算公式是这样的：

$$J(\textbf{w}) = -\frac{1}{N} \sum_{i=1}^{N} \left[ y_i \text{log}(\hat{y}_i)
+ (1-y_i) \text{log}(1-\hat{y}_i) \right]$$

$𝐰$ 是神经网络的权重参数，$y_i$ 是真实的标签，$\hat{y}_i$ 是预测的标签。

两种损失函数的计算公式是一样的，区别在于真实的标签 $y_i$ 的形式：

  * 当它是 one-hot 形式时，例如 [1,0,0]、[0,1,0]、[0,0,1]，使用 categorial_crossentropy；
  * 当它是整数时，例如 [1]、[2]、[3]，使用 sparse_categorial_crossentropy，在计算时 sparse 消耗的资源更少一些，因为它不用计算整个向量，只需计算整数。

此外，因为我们前面在数据预处理时为了更方便构造批次数据，有对序列用 0 填充成统一长度的操作，这种填充在计算损失和反向传播时会对模型的性能造成一定的影响。

**2\. 为了解决这个问题就需要在计算损失时用一个 padding mask。**

它会在反向传播之前将任何填充过的序列所带来的损失变为 0，然后再去更新权重参数，即填充的数据 mask 为 0，没填充的数据 mask 为 1。

具体会带来什么影响我们可以用一个最简单的小例子来看一下，例如，有这样一个简单的模型：

    
    
    max_sentence_length = 5
    character_number = 2
    
    input_tensor = Input(shape=(max_sentence_length, character_number))
    masked_input = Masking(mask_value=0)(input_tensor)
    output = LSTM(3, return_sequences=True)(masked_input)
    model = Model(input_tensor, output)
    model.compile(loss='mae', optimizer='adam')
    

X 数据如下：

    
    
    X = np.array([[[0, 0], [0, 0], [1, 0], [0, 1], [0, 1]],
                  [[0, 0], [0, 1], [1, 0], [0, 1], [0, 1]]])
    

那么真实的标签是：

    
    
    y_true = np.ones((2, max_sentence_length, 3))
    

预测的标签为：

    
    
    y_pred = model.predict(X)
    
    print(y_pred)
    [[[ 0.          0.          0.        ]
      [ 0.          0.          0.        ]
      [-0.11980877  0.05803877  0.07880752]
      [-0.00429189  0.13382857  0.19167568]
      [ 0.06817091  0.19093043  0.26219055]]
    
     [[ 0.          0.          0.        ]
      [ 0.0651961   0.10283815  0.12413475]
      [-0.04420842  0.137494    0.13727818]
      [ 0.04479844  0.17440712  0.24715884]
      [ 0.11117355  0.21645413  0.30220413]]]
    

如果不对损失函数进行 mask，计算公式是这样的：

    
    
    unmasked_loss = np.abs(1 - y_pred).mean()
    

否则是这样的：

    
    
    masked_loss = np.abs(1 - y_pred[y_pred != 0]).mean()
    

打印出两种损失可以看到没有进行 loss masking 的损失要比真实的损失大一些。

    
    
    print(model.evaluate(X, y_true))
    0.881977558136
    
    print(masked_loss)
    0.881978
    
    print(unmasked_loss)
    0.917384
    

### NMT 训练

我们用 tf.function 装饰器来进行静态图形计算，网络的计算需要放在 tf.GradientTape() 下，用来追踪梯度。

将 input 输入给 Encoder，然后返回 en_outputs 和 en_states，这个隐层状态同时也是 Decoder 的隐层状态。
Decoder 的输入有初始的隐层状态和 target_seq_in，Decoder 的输出结果即为预测值。

计算损失，然后计算梯度并将其应用于优化器和反向传播中。

其中 Encoder 的输出有三个 `encoder_output, en_state_h, en_state_c`，我们不需要用
encoder_output，只用 Encoder 的状态向量 `en_state_h, en_state_c`， 所以 `en_states =
en_outputs[1:]` 这个 en_states 就作为 de_states 传递给 Decoder。

    
    
    @tf.function
    def train_step(source_seq, target_seq_in, target_seq_out, en_initial_states):
        with tf.GradientTape() as tape:
            en_outputs = encoder(source_seq, en_initial_states)
            # 我们不需要用到 encoder_output，只用 Encoder 的状态向量 en_state_h, en_state_c，
            en_states = en_outputs[1:]
            de_states = en_states
    
            de_outputs = decoder(target_seq_in, de_states)
            logits = de_outputs[0]
            loss = loss_func(target_seq_out, logits)
    
        variables = encoder.trainable_variables + decoder.trainable_variables
        gradients = tape.gradient(loss, variables)
        optimizer.apply_gradients(zip(gradients, variables))
    
        return loss
    

### NMT 推断预测

在文章开头我们知道了 NMT 预测的流程，这部分就是按照流程写下来就可以。

首先将测试文本转化成整数序列数据，接着输入到 Encoder，Encoder 生成的状态向量作为 Decoder 的初始向量，同时 Decoder
的输入还有开始标记 `<start>`，Decoder 每一步的输出结果
de_output，都选择概率最大的作为预测值，并替换成对应的单词，附加到输出的翻译序列 out_words 中，并且 Decoder
每一步的预测结果要作为下一步的输入继续预测，一直到遇到 `<end>` 标记或者结果的长度超过设定长度为 20 就停止生成结果。

    
    
    def predict():
        test_source_text = raw_data_en[np.random.choice(len(raw_data_en))]
        print(test_source_text)
        # 首先将测试文本转化成整数序列数据，
        test_source_seq = en_tokenizer.texts_to_sequences([test_source_text])
        print(test_source_seq)
    
        # 测试数据输入 Encoder，返回结果中只取状态向量
        en_initial_states = encoder.init_states(1)
        en_outputs = encoder(tf.constant(test_source_seq), en_initial_states)
    
        # Decoder 的输入就是开始标记 <start>，Encoder 生成的状态向量作为 Decoder 的初始向量
        de_input = tf.constant([[fr_tokenizer.word_index['<start>']]])
        de_state_h, de_state_c = en_outputs[1:]        # 返回结果中只取状态向量
        out_words = []
    
        # Decoder 输出的结果 de_output，选择概率最大的作为预测值，并查找相应的索引，替换成对应的单词，附加到输出的翻译序列 out_words 中
        # 前一步的预测值作为下一步的输入
        # 重复这个步骤一直预测，直到遇到 <end> 标记或者结果的长度超过 20。
        while True:
            de_output, de_state_h, de_state_c = decoder(
                de_input, (de_state_h, de_state_c))
            de_input = tf.argmax(de_output, -1)
            out_words.append(fr_tokenizer.index_word[de_input.numpy()[0][0]])
    
            if out_words[-1] == '<end>' or len(out_words) >= 20:
                break
    
        print(' '.join(out_words))
    

### 模型训练

前面的每一步都完成后，就可以执行训练了，在每一个 epoch 会打印出模型的损失。

    
    
    EMBEDDING_SIZE = 32
    LSTM_SIZE = 64
    
    en_vocab_size = len(en_tokenizer.word_index) + 1
    encoder = Encoder(en_vocab_size, EMBEDDING_SIZE, LSTM_SIZE)
    
    fr_vocab_size = len(fr_tokenizer.word_index) + 1
    decoder = Decoder(fr_vocab_size, EMBEDDING_SIZE, LSTM_SIZE)
    
    NUM_EPOCHS = 250
    BATCH_SIZE = 5
    
    for e in range(NUM_EPOCHS):
        en_initial_states = encoder.init_states(BATCH_SIZE)
    
        for batch, (source_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):
            loss = train_step(source_seq, target_seq_in,
                              target_seq_out, en_initial_states)
    
        print('Epoch {} Loss {:.4f}'.format(e + 1, loss.numpy()))
    
        try:
            predict()
        except Exception:
          continue   
    

我们可以看到在模型训练的最开始，打印出来的结果是没有什么实际意义的，随着模型训练次数的增加，结果就会越来越准确，当然这个小例子是过拟合的了。

    
    
    Epoch 1 Loss 3.2892
    I can t believe you re giving up .
    [[16, 11, 12, 95, 2, 96, 14, 97, 1]]
    est est est est est comme comme comme mannequins <end>
    Epoch 2 Loss 3.4178
    Germany produced many scientists .
    [[73, 74, 75, 76, 1]]
    vous <end>
    Epoch 3 Loss 3.3415
    What a ridiculous concept !
    [[8, 5, 21, 22, 23]]
    vous <end>
    
    ...
    
    
    Epoch 248 Loss 0.1051
    Could you close the door please ?
    [[53, 2, 54, 4, 55, 18, 3]]
    pourriez vous fermer la s il vous plait ? <end>
    Epoch 249 Loss 0.1094
    What a ridiculous concept !
    [[8, 5, 21, 22, 23]]
    <end>
    Epoch 250 Loss 0.1079
    All three of you need to do that .
    [[35, 36, 37, 2, 38, 39, 10, 40, 1]]
    vous avez besoin de faire cela tous les trois . <end>
    

我们通过一个小例子已经基本理解 NMT 的代码实现，大家有兴趣可以尝试翻译其他语种数据。

* * *

参考文献：

  * TensorFlow，[Neural machine translation with attention](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/tutorials/text/nmt_with_attention.ipynb#scrollTo=umohpBN2OM94)

