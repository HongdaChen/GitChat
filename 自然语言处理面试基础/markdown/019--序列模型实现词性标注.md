今天我们来看看 NLP 中一个很重要且基本的问题：POS。

### 什么是 POS？

**POS：Part-of-speech tagging，即词性标注**
，这是一种序列标注问题，就是输入一个句子，输出每个词在这句话中的词性是什么。例如，“我喜欢吃酸辣的热狗”这句话中，“我”和“热狗”是名词，“喜欢”和“吃”是动词，“酸辣的”是个形容词。

词性种类集合是预先定义好的，单词的词性也是取决于上下文的，即同样的词在不同的语境中词性可能会有所不同。

### 为什么要做 POS？

正确地识别词性会有助于理解一些语言问题，比如当我们遇到了有歧义的句子，有时 **通过标出词性就能知道真正的意思**
。知道了哪个是名词，我们就知道这句话涉及了哪些主体，知道了动词是什么，就知道主体之间要做什么，知道哪些是形容词，就可以了解事物的属性如何。

**词性标注还经常作为其他自然语言处理任务的特征** ，例如在 parsing 解析，relation extraction
关系提取，命名实体识别，情感分析，问答系统，自动生成文本等任务中都会用到词性标注。

**此外 POS 还经常和词形还原一起作为预处理的步骤，** 例如在情感分析任务中，如果我们想要识别讽刺性的话语，在预处理中就可以做下面几步：

  * 文字清洗：将对任务没有帮助的信息去掉，如去掉标点数字等字符。
  * 标记：将一串文本分解为单词，短语，符号等有意义的元素。
  * 词形还原：就是将一个词转化成它的基本型，这种转化是根据单词在句子中的词性，以及所在的上下文进行转化的，转换后的基本型是要符合语境含义的，词性标注就有助于理解当前语境下的单词含义，如 ‘Caring’ 根据上下文被识别为动词 -> 词形还原 -> ‘Care’；如果不考虑词性等因素，只是简单地去掉后缀就是词干还原，基本型就成了 ‘Caring’ -> 词干还原 -> ‘Car’ 成为名词，含义与原文就会相差甚远。

下面的代码中就是先进行合适的词性标注后，再根据这些标签进行词形还原，其中 POS 使用 `nltk.pos_tag` 来完成：

    
    
    import nltk
    from nltk.corpus import wordnet
    from nltk.stem import WordNetLemmatizer
    
    def get_wordnet_pos(word):
    
        # 将 POS 标签简化成一个大写字母传递给 lemmatize()
        tag = nltk.pos_tag([word])[0][1][0].upper()
        tag_dict = {"J": wordnet.ADJ,
                    "N": wordnet.NOUN,
                    "V": wordnet.VERB,
                    "R": wordnet.ADV}
    
        return tag_dict.get(tag, wordnet.NOUN)
    
    lemmatizer = WordNetLemmatizer()
    
    hl_lemmatized = []
    for tokens in hl_tokens:
        lemm = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in tokens]
        hl_lemmatized.append(lemm)
    
    # 比较转换前后的结果：
    word_1 = ['skyrim','dragons', 'are', 'having', 'parties']
    word_2 = [lemmatizer.lemmatize(w, get_wordnet_pos(w)) for w in word_1]
    print('Before lemmatization :\t',word_1)
    print('After lemmatization :\t',word_2)
    
    
    
    Before lemmatization :     ['skyrim', 'dragons', 'are', 'having', 'parties']
    After lemmatization :     ['skyrim', 'dragon', 'be', 'have', 'party']
    

进行这种预处理后，就可以继续进行后面模型的训练、准备数据、建立模型、模型训练验证和预测等，词性标注可以作为特征对模型的性能会有很大帮助。

### 都有哪些词性？

目前有很多词性标签库，比较流行的有 **Penn Treebank、Brown corpus** ，Penn Treebank 包括 45
种标签，Brown corpus 包括 87 种标签。虽然 PTB 和 Brown
中标签的类别算是比较多的，不过不能适用于中文，西班牙语，德语等，而且有些语言细节要比英语多一些，而有些细节要少一些。

![](https://images.gitbook.cn/57d4c340-70be-11ea-9d98-f7fceb2428d3)

[图片来源](https://www.researchgate.net/figure/The-Penn-Treebank-POS-
tagset_tbl1_220017637)

另外一个不错的库是 **Universal Dependencies**
，这个项目的目的是要建立一个可以跨多种语言的注释库，而且是为了只使用一种注释标准就能适用于几乎所有语言。UD
中的词性标注集合有三组：开放类标签，封闭类标签和“其他”：

![](https://images.gitbook.cn/71d9fcb0-70be-11ea-8e15-575ea6bad8cc)

[图片来源](https://www.semanticscholar.org/paper/Universal-Dependencies-v1%3A-A-
Multilingual-Treebank-Nivre-Marneffe/d115eceab7153d2a1dc6fbf6b99c3bdf1b0cdd46)

  * **开放类标签** ：包括名词、动词、形容词和副词，几乎所有语言都有这几种词性。除此以外还有两个开放类，分别是专有名词和感叹词。
  * **封闭类标签** ：包括助动词、并列连词、从属连词、代词、限定词、数词等。这些词有时被称为功能词，它们本身很少具有词汇含义，但是却对句子的组成有帮助。
  * **其他** ：包括标点符号和符号，标点如逗号、句号、冒号等，符号是可以有含义的，如美元、百分比符号、数学运算符、表情符号、网址等，以及标签 X，X 用于标记无法分配词性的词。

UD
有一个好处在于，现在社交媒体上在不断地涌出新的流行词，还有很多表情符号、URL、方言等等，这些是不能用简单的名词和动词来概括的，所以需要有这么一套新的标签系统来管理。

### 都有哪些标注器？

通过上面那个小例子可以看出在实际应用中，大多数情况下是直接使用现成的标注器，然后再去进行下游的任务。下面介绍几种常用的应用。

#### **nltk.pos_tag(word)**

我们可以使用 NLTK 内置的标注器 `nltk.pos_tag(word)`。

例如，"I love NLP and I will learn NLP in 2 month" 这句话。

    
    
    import nltk
    from nltk.corpus import stopwords
    from nltk.tokenize import word_tokenize, sent_tokenize
    
    stop_words = set(stopwords.words('english'))
    tokens = sent_tokenize(text)
    
    for i in tokens:
        words = nltk.word_tokenize(i)
        words = [w for w in words if not w in stop_words]
        #  POS-tagger.
        tags = nltk.pos_tag(words)
    
    tags
    
    
    
    Results:
    [('I', 'PRP'),
     ('love', 'VBP'),
     ('NLP', 'NNP'),
     ('I', 'PRP'),
     ('learn', 'VBP'),
     ('NLP', 'RB'),
     ('2month', 'CD')]
    

其中 PRP 为人称代词，VBP 动词一般时，NNP 专有名词，RB 副词，CD 基数。

在实际应用中，还有一种常见应用是提取出想要的词性所有词，例如提取出数据中所有的名词：

    
    
    tagged = nltk.pos_tag(word_tokenize(s))
    allnoun = [word for word,pos in tagged if pos in ['NN','NNP'] ]
    

#### **Stanford tagger**

也可以调用其他标注器，如 Stanford tagger：

    
    
    from nltk.tag.stanford import POSTagger
    import nltk
    
    stan_tagger = POSTagger('models/english-bidirectional-distdim.tagger','standford-postagger.jar')
    
    tokens = nltk.word_tokenize(s)
    stan_tagger.tag(tokens)
    

#### **序列标注器**

此外还有序列标注器，它们在标注词性时会考虑上下文，所以结果更准确：

    
    
    from nltk.tag import UnigramTagger
    from nltk.tag import DefaultTagger
    from nltk.tag import BigramTagger
    from nltk.tag import TrigramTagger
    
    train_data = brown_tagged_sents[:int(len(brown_tagged_sents) *
    0.9)]
    test_data = brown_tagged_sents[int(len(brown_tagged_sents) *
    0.9):]
    
    unigram_tagger = UnigramTagger(train_data,backoff=default_tagger)
    print unigram_tagger.evaluate(test_data)
    # 0.826195866853
    
    bigram_tagger = BigramTagger(train_data, backoff=unigram_tagger)
    print bigram_tagger.evaluate(test_data)
    # 0.835300351655
    
    trigram_tagger = TrigramTagger(train_data,backoff=bigram_tagger)
    print trigram_tagger.evaluate(test_data)
    # 0.83327713281
    

NLTK 中除了这些预训练好的标注器之外，自己也可以写适用于个性化任务的标注器。

### 如何标注词性？

词性标注的方法有很多种，总体来说可以分为两类：

**1\. 基于规则的：即手动设定一些规则，为单词标记上特定的标签。**

基于规则的方法，一般是通过上下文信息为未知或模糊的单词分配标签，这类方法通过分析单词自身的语言特征，以及它前面的单词，和后面的单词的特征，来达到消除歧义的目的。例如下面这样一条规则：如果未知单词
X 前面有一个限定词，并且后面跟着一个名词，则将其标记为形容词。

E. Brill Tagger
是最早广泛使用的英语词性标注器之一，它就是采用了基于规则的方法。不过它的规则不是手动制定的，而是通过语料库找到的，模型通过一组规则模版来生成新的特征，在数据集上训练模型，来找到一组能最小化误差的标签规则。

**2\. 随机标注器：任何一个利用和频率概率相关的模型来进行标注的都属于这一类。**

  * **最简单的一种方法是，为每个单词选择它最常见的标签。** 例如在 Universal Dependencies 标签库中，talk 这个词出现了 96 次，其中有 85 次都被标记为动词，那么我们就在一个新的 POS 任务中将它的标签预测为动词。如果是语料库中没有出现过的词，就将其标签预测为最常见的：名词。这种最简单粗暴的方法，在 Penn Treebank 上的准确率可以达到 92%。
  * **也可以用 n-gram** ，这种方法会连同该单词前面 n 个单词组成的序列一起考虑，出现概率最大的作为它的预测标签。和上一种方法相比，这里考虑了单词的语境。
  * **还有一种方法是将前面两类方法融合在一起** 。HMM——Hidden Markov Model 隐马尔可夫模型，既用到单词的频率，又用到序列的概率。此外还有 Viterbi 算法，动态规划等算法。
  * **目前越来越流行用机器学习技术来标注词性** ，比如 SVM，KNN，最大熵分类器等模型，其中 structured perceptron 结构化感知器在 Penn Treebank 上的准确率可以达到 97.1%。

在训练分类器之前，需要构造一些特征，一般包括内在特征和外在特征。

**内在特征** ，如单词本身、前缀、后缀等：

  * 当前的单词
  * 单词的前 2 个字符前缀
  * 单词的前 3 个字符前缀
  * 单词的后 2 个字符后缀
  * 单词的后 3 个字符后缀
  * 单词大写
  * 单词包括数字
  * 单词包括连字符

**外在特征** ，如上下文等：

  * 前 1 个单词的词性预测结果
  * 前 2 个单词的词性预测结果
  * 前面的 2 个单词
  * 后面的 1 个单词
  * 前面 1 个单词的前缀/后缀/是否大写/是否包含数字

此外单词聚类和词嵌入向量也是不错的特征，它们对于语料库中没有出现过的单词预测效果比较好，因为词性标签相似的词，它们的上下文也会更相似一些。

**除了上述模型，当然也少不了神经网络序列模型** ，[Plank et al.
(2016)](https://arxiv.org/pdf/1604.05529.pdf) 中使用双向 LSTM，在 UD
语料库中的二十二种语言上达到的平均准确率为 96.5％。

由前面的特征列表我们可以知道，在 POS 任务中前缀和后缀是很重要的特征，比如以 `ty` 结尾的词通常是名词，以 `-ed`
结尾的单词可能是动词的过去时，以 `un-` 开头的单词很可能是形容词，以大写字母开头的单词可能是专有名词等等。

传统方法中通常会使用一个字符窗口来捕捉前缀或者后缀，不过这样做只能考虑到固定长度的字符串，例如当设定窗口只查看每个单词的前5个字符时，在遇到较长单词时，就无法捕捉到后缀等特征。

这种情况下可以使用 **字符级的双向 RNN** ，单词的每个字符会被映射为一个嵌入向量，输入给模型，正向 RNN 可以捕捉到后缀，反向 RNN
可以捕捉到前缀，这样就可以同时抓住单词的整体特征，如大写、连字符、单词长度等，而不用受到固定长度字符串的限制。

![](https://images.gitbook.cn/1779b440-70cd-11ea-9d98-f7fceb2428d3)

除了字符级的还有 **单词级双向 RNN** ，例如我们使用 Deep biRNN 模型来做序列标注任务。

模型的输入是一个句子，包含单词 w1~wn，模型的输出是句子中每个单词 wi 的词性标签。

模型结构为：

![](https://images.gitbook.cn/3d6c3650-70cd-11ea-bf44-ef79e0369ea9)

首先，将每个单词转化为向量作为特征，例如用嵌入矩阵来实现。

[Plank et al. (2016)](https://arxiv.org/pdf/1604.05529.pdf) 的双向 LSTM
模型就采用了三种类型的嵌入：

  * fine-tuned：微调词嵌入，这种嵌入在训练期间会不断更新；
  * pre-trained：预训练好的词嵌入，这种嵌入不会进行更新，但有助于标记词汇表以外的单词；
  * character-based：基于字符的词嵌入，这种方法是通过对每个单词中的各个字符运行 LSTM 来计算的，所以可以捕获常见的拼写模式，比如前缀，后缀和大小写等。 

然后将嵌入向量串联起来，输入到模型中，得到输出向量 y1~yn。再经过一个 MLP 多层感知器，用 Softmax 预测出每个单词的标签向量，每个向量中第
k 个位置的数值，就表示当前单词属于第 k 个词性的概率。

用双向 RNN 得到的结果，不仅仅考虑每个位置的单词，还包含了这个位置上下文的信息，每个标签都是通过对整个输入句子进行条件概率预测的结果。

**目前在 Penn Treebank 上表现最好的模型是 Meta BiLSTM 模型** ，准确率达到 97.96，原始论文地址：

> [Morphosyntactic Tagging with a Meta-BiLSTM Model over Context Sensitive
> Token Encodings](https://arxiv.org/abs/1805.08237)

![](https://images.gitbook.cn/7e2c71f0-70cd-11ea-8e15-575ea6bad8cc)

[图片来源](http://nlpprogress.com/english/part-of-speech_tagging.html)

### 用序列模型实现 POS

下面我们来看如何用循环神经网络模型来进行序列标注。我们要做的是，输入一个句子，返回句子中每个单词相应的 POS 标签。例如： 输入：

    
    
    [The, cat, sat, on, the, mat, .]
    

得到输出：

    
    
    [DT, NN, VB, IN, DT, NN]
    

#### **导入依赖的库**

    
    
    import numpy as np
    np.random.seed(42)    
    from keras.layers.core import Activation, Dense, Dropout, RepeatVector, SpatialDropout1D
    from keras.layers.embeddings import Embedding
    from keras.layers.recurrent import GRU
    from keras.layers.wrappers import TimeDistributed
    from keras.models import Sequential
    from keras.preprocessing import sequence
    from keras.utils import np_utils
    from sklearn.model_selection import train_test_split
    import collections
    import os
    import nltk
    nltk.download('treebank')
    

#### **准备数据**

数据使用 NLTK 中的 Treebank 语料库，需要转化一下格式，建立两个文本，treebank_sents.txt 用来存句子中的单词，如：

    
    
    words = ['Trinity', 'said', 'it', 'plans', 'to', 'begin', 'delivery', 'in', 'the', 'first', 'quarter', 'of', 'next', 'year', '.']
    

treebank_poss.txt 为每句话中单词相应的 POS 标签，如：

    
    
    poss = ['NNP', 'VBD', 'PRP', 'VBZ', 'TO', 'VB', 'NN', 'IN', 'DT', 'JJ', 'NN', 'IN', 'JJ', 'NN', '.']
    
    
    
    DATA_DIR = "/content/data"
    
    fedata = open(os.path.join(DATA_DIR, "treebank_sents.txt"), "wt")
    ffdata = open(os.path.join(DATA_DIR, "treebank_poss.txt"), "wt")
    
    sents = nltk.corpus.treebank.tagged_sents()
    for sent in sents:
        words, poss = [], []
        for word, pos in sent:
            if pos == "-NONE-":
                continue
            words.append(word)
            poss.append(pos)
        fedata.write("{:s}\n".format(" ".join(words)))
        ffdata.write("{:s}\n".format(" ".join(poss)))
    
    fedata.close()
    ffdata.close()
    

#### **数据探索**

先简单地探索一下数据，这样我们可以决定后面的模型参数要设置成多少。

parse_sentences 这个函数是为了找到词汇表中唯一单词的数量，训练语料库中最长句子所含的单词数 maxlen 还有记录数 num_recs。

  * s_wordfreqs：每个单词出现的次数
  * t_wordfreqs：每个标签出现的次数

    
    
    def parse_sentences(filename):
        word_freqs = collections.Counter()
        num_recs, maxlen = 0, 0
        fin = open(filename, "r")
    
        for line in fin:
            words = line.strip().lower().split()
            for word in words:
                word_freqs[word] += 1
            if len(words) > maxlen:
                maxlen = len(words)
            num_recs += 1
        fin.close()
    
        return word_freqs, maxlen, num_recs
    
    DATA_DIR = "/content/data"
    
    s_wordfreqs, s_maxlen, s_numrecs = parse_sentences(os.path.join(DATA_DIR, "treebank_sents.txt"))
    t_wordfreqs, t_maxlen, t_numrecs = parse_sentences(os.path.join(DATA_DIR, "treebank_poss.txt"))
    
    print(len(s_wordfreqs), s_maxlen, s_numrecs, len(t_wordfreqs), t_maxlen, t_numrecs)
    
    # 10947 249 3914 45 249 3914
    

#### **设置参数**

由上面的结果可知，数据集中一共有 10947 个单词和 45 种 POS 标签，句子最长为 249，集合中的句子数量为 3914。

于是我们可以只考虑单词表的前 5000 个单词，标签就是所有这 45 种，并将 250 设置为句子的最大长度：

    
    
    max_sequence = 250
    sentence_max_features = 5000
    tag_max_features = 45
    

#### **建立单词和标签的索引**

下面分别构建单词和标签的序号索引表，用来将单词/标签转化为数字，以及将数字转化为单词/标签。s_word2index 会有 5002 个，除了我们前面设置的
5000 以外，还加入了用来填充的 PAD，未知的 UNK。

    
    
    # 单词索引
    s_vocabsize = min(len(s_wordfreqs), sentence_max_features) + 2
    s_word2index = {x[0]:i+2 for i, x in enumerate(s_wordfreqs.most_common(sentence_max_features))}
    s_word2index["PAD"] = 0
    s_word2index["UNK"] = 1
    s_index2word = {v:k for k, v in s_word2index.items()}
    
    # 标签索引
    t_vocabsize = len(t_wordfreqs) + 1
    t_word2index = {x[0]:i for i, x in enumerate(t_wordfreqs.most_common(tag_max_features))}
    t_word2index["PAD"] = 0
    t_index2word = {v:k for k, v in t_word2index.items()}
    

#### **构造数据**

这一步用 build_tensor 来生成数据集 X、Y，并用 train_test_split 生成 80% 训练集和 20% 测试集的数据。

在 treebank_sents.txt 上面应用 build_tensor，用来将输入的句子文本转换为长度为 `max_sequence = 250`
的向量，所以 X 的维度是 3914*250。

在 treebank_poss.txt 上，用来将标签数据构造为长度为 `tag_max_features + 1 = 46` 的 one-hot
向量，词和标签是对应的，所以它的最大长度也是 `max_sequence = 250`，即 Y 的维度是 3914*250*46。

    
    
    def build_tensor(filename, numrecs, word2index, maxlen, make_categorical = False, num_classes = 0):
        data = np.empty((numrecs, ), dtype=list)
        fin = open(filename, "r")
        i = 0
        for line in fin:
            wids = []
            for word in line.strip().lower().split():
                if word in word2index:
                    wids.append(word2index[word])
                else:
                    wids.append(word2index["UNK"])
            if make_categorical:
                data[i] = np_utils.to_categorical(wids, num_classes=num_classes)
            else:
                data[i] = wids
            i += 1
        fin.close()
        pdata = sequence.pad_sequences(data, maxlen=maxlen)
        return pdata
    
    X = build_tensor(os.path.join(DATA_DIR, "treebank_sents.txt"), s_numrecs, s_word2index, max_sequence)
    Y = build_tensor(os.path.join(DATA_DIR, "treebank_poss.txt"), t_numrecs, t_word2index, max_sequence, True, t_vocabsize)
    
    Xtrain, Xtest, Ytrain, Ytest = train_test_split(X, Y, test_size=0.2, random_state=42)
    

#### **建立序列模型**

我们首先尝试一下 seq2seq 模型，这个模型的具体原理我们会在后面课程中详细讲解。

![](https://images.gitbook.cn/56d169c0-70ce-11ea-8e15-575ea6bad8cc)

模型的输入张量形状为 (None, max_sequence, 1)，None 为批量大小尚未确定。

然后经过嵌入层，嵌入层将每个单词转换为 dense 向量，大小为 embedding_size，这一层最后输出的形状为 (None,
max_sequence, embedding_size)。

接着进入编码器 GRU 层，因为 `return_sequences = False`，所以大小为 max_sequence 的序列经过 GRU
后，返回单个上下文向量，即输出形状为 (None, hidden_size)。

然后使用 RepeatVector 层将该上下文向量复制成形状为 (None, max_sequence, hidden_size) 的张量。

TimeDistributed 用于保持输入层和输出层的一对一关系，并输入到解码器 GRU 层，继续进入一个 dense 层，产生形状为 (None,
max_sequence, t_vocab_size) 的张量。其中 dense 层上的激活函数为 Softmax。

输出张量的每一列对应该位置的单词，每一列的 argmax 就是预测的 POS 标签的索引。

损失函数用 categorical_crossentropy，优化器选择 Adam，评估方法用 Accuracy。

    
    
    embedding_size = 128
    hidden_size = 64
    batch_size = 32
    num_epochs = 1
    
    model = Sequential()
    model.add(Embedding(s_vocabsize, embedding_size, input_length=max_sequence))
    model.add(SpatialDropout1D(0.2))
    model.add(GRU(hidden_size, dropout=0.2, recurrent_dropout=0.2))
    model.add(RepeatVector(max_sequence))
    model.add(GRU(hidden_size, return_sequences=True))
    model.add(TimeDistributed(Dense(t_vocabsize)))
    model.add(Activation("softmax"))
    
    model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
    

#### **训练模型**

然后进行模型的训练和评估，我们可以先将 num_epochs 设置为 10 看一下模型的效果，不过我们可以从结果看到，在第一个 epoch
后就开始过拟合了，因为这个模型太丰富了，有许多参数。

    
    
    history1 = model.fit(Xtrain, Ytrain, batch_size=batch_size, epochs=num_epochs, validation_data=[Xtest, Ytest])
    
    score, acc = model.evaluate(Xtest, Ytest, batch_size=batch_size)
    print("Test score: %.3f, accuracy: %.3f" % (score, acc))
    
    
    
    import matplotlib.pyplot as plt
    
    plt.plot(history1.history['acc'])
    plt.plot(history1.history['val_acc'])
    plt.title('Model accuracy')
    plt.ylabel('Accuracy')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper left')
    plt.show()
    
    plt.plot(history1.history['loss'])
    plt.plot(history1.history['val_loss'])
    plt.title('Model loss')
    plt.ylabel('Loss')
    plt.xlabel('Epoch')
    plt.legend(['Train', 'Test'], loc='upper left')
    plt.show()
    

![](https://images.gitbook.cn/20f06cb0-70cf-11ea-ab1a-832c54b4f266)

#### **尝试其他模型**

我们可以将上述模型中 GRU 的部分全部替换成 LSTM，其他部分不变，不过二者的结果是差不多的。

    
    
    model = Sequential()
    model.add(Embedding(s_vocabsize, embedding_size, input_length=max_sequence))
    model.add(SpatialDropout1D(0.2))
    model.add(LSTM(hidden_size, dropout=0.2, recurrent_dropout=0.2))
    model.add(RepeatVector(max_sequence))
    model.add(LSTM(hidden_size, return_sequences=True))
    model.add(TimeDistributed(Dense(t_vocabsize)))
    model.add(Activation("softmax"))
    

我们还可以用 **双向 LSTM 模型** ，在 POS 这个任务中，每个单词的标签可能不仅取决于前面单词的标签，还有后面单词的标签。通过双向 RNN
我们可以捕捉到这样的信息，而且双向的结构一方面对序列的开头和结尾部分重视度相同，一方面还有助于增加训练数据。

双向 RNN 的原理在前面一篇文章中已经介绍了，简单来讲是两个堆叠在一起的 RNN，以相反的方向读取输入数据，一个 RNN 从左到右读取序列单词，另一个
RNN 从右到左读取序列单词，每个时间步的输出是基于两个 RNN 的隐层状态的。

在模型中，我们只需要通过一个双向的封装层 Bidirectional 就可以简单实现 LSTM 双向化：

    
    
    from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional
    
    model2 = Sequential()
    model2.add(Embedding(s_vocabsize, embedding_size, input_length=max_sequence))
    model2.add(SpatialDropout1D(0.2))
    model2.add(Bidirectional(LSTM(hidden_size, dropout=0.2, recurrent_dropout=0.2)))
    model2.add(RepeatVector(max_sequence))
    model2.add(Bidirectional(LSTM(hidden_size, return_sequences=True)))
    model2.add(TimeDistributed(Dense(t_vocabsize)))
    model2.add(Activation("softmax"))
    
    model2.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
    
    history3 = model2.fit(Xtrain, Ytrain, batch_size=batch_size, epochs=1, validation_data=[Xtest, Ytest])
    
    score, acc = model2.evaluate(Xtest, Ytest, batch_size=batch_size)
    print("Test score: %.3f, accuracy: %.3f" % (score, acc))
    

* * *

参考文献：

  * Jacob eisenstein， _Natural Language Processing_
  * Yoav Goldberg， _Neural Network Methods for Natural Language Processing_
  * Antonio Gulli，Sujit Pal， _Deep Learning with Keras_

